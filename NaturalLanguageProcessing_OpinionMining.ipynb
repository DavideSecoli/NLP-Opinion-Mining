{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c795a737",
   "metadata": {},
   "source": [
    "# Introduction to NLP: Opinion Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe21438",
   "metadata": {},
   "source": [
    "## Table of content <br>\n",
    "* ### Preface\n",
    "* ### How to run the program\n",
    "* ### Import libraries\n",
    "* ### Pipelines Hyperparameters\n",
    "* ### Pipeline 1 step 1: Analyse the data and the task\n",
    "* ### Pipeline 1 step 2: Data preprocessing\n",
    "* ### Pipeline 1 step 3: Product feature extraction\n",
    "* ### Pipeline 1 step 4: Sentiment analysis\n",
    "* ### Pipeline 1 step 5: Evaluation and discussion\n",
    "* ### Pipeline 2 step 3: Product feature extraction\n",
    "* ### Pipeline 2 step 4: Sentiment analysis\n",
    "* ### Pipeline 2 step 5: Evaluation and discussion\n",
    "* ### References\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8467908",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556a2ef",
   "metadata": {},
   "source": [
    "This notebook presents two opinion mining pipelines capable of ingesting raw unstructured text data, extract features and predict their sentiment polarity.\n",
    "<br>\n",
    "\n",
    "The pipeline outputs are a series of evaluation metrics that check the quality of extracted features, prediction accuracy of machine learning models and a summary table containing most important features and their sentiment count. \n",
    "<br> \n",
    "\n",
    "The user is invited to run them both to experience the seamless front to end process where all files are collected from thier directories, processed through the pipeline and statistical outputs for each product is presented. \n",
    "<br>\n",
    "\n",
    "The data analysed on this notebook is Amazon customer reviews across 17 different products. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b905f3",
   "metadata": {},
   "source": [
    "# How to run the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd9a94",
   "metadata": {},
   "source": [
    "To run the program the user is required to input below the path to the `Data` folder. Note the `Data` sub-directory is already present in the `load_data` function path in Step 1 therefore there's no need to include it.<br>\n",
    "Afterwards, all cells can be run and outputs for the two pipelines will be shown in sequntial order. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_path_to_data = # Please add your path to 'Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab474c4",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ffb03",
   "metadata": {},
   "source": [
    "Main libraries used on this assigment are NLTK and Spacy in the data pre-processing and feature extraction phase (pipeline steps 2 and 3). Sklearn is used for sentiment analysis and evaluation (pipeline steps 4 and 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb09b341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/davidesecoli/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "import nltk \n",
    "import string \n",
    "import warnings \n",
    "\n",
    "import Levenshtein \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import contractions \n",
    "from pprint import pprint\n",
    "\n",
    "import spacy \n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "import nltk \n",
    "from nltk import UnigramTagger \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import conll2000 \n",
    "from wordfreq import get_frequency_dict \n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from nltk.stem.lancaster import LancasterStemmer \n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents, ne_chunk\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import FunctionTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,make_scorer,classification_report\n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d632e",
   "metadata": {},
   "source": [
    "# Pipelines Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818b51a",
   "metadata": {},
   "source": [
    "The pipeline hyperparameters below are set to test model performance and output quality of different components across both pipelines. All tests have been set to be reproducable using a random seed. More details on each of them will be provided later on. Stay tuned! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2489bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "global RANDOM_SEED, chosen_feature, levenshtein_ratio\n",
    "\n",
    "RANDOM_SEED = 37\n",
    "\n",
    "# Word similarity pipe 1\n",
    "levenshtein_ratio_one = 0.7\n",
    "\n",
    "# Word similarity pipe 2\n",
    "levenshtein_ratio_two = 0.6\n",
    "\n",
    "# List of available features to train models on\n",
    "chosen_feature = ['Review_without_stopwords','Str_Stemmed_tokens','Extracted_feature',\n",
    "                  'Extracted_adjectives','Str_PoS_tokens']\n",
    "\n",
    "# Select index num of chosen_feature above\n",
    "feature_num = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174c607",
   "metadata": {},
   "source": [
    "# Pipeline 1 step 1: Analyse the data and the task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb11d0",
   "metadata": {},
   "source": [
    "As part of the assignment a `Data` folder was provided which contains three sub-folders:\n",
    "<br>\n",
    "* Customer_review_data\n",
    "* CustomerReviews-3_domains\n",
    "* Reviews-9-products.\n",
    "<br>\n",
    "\n",
    "The first two folders contain 5 and 3 txt files respectively of different product reviews and a readme.txt file detailing the format used on such files. <br>\n",
    "Last folder includes 9 txt files all about product review without any readme files.\n",
    "\n",
    "Function `load_data` parses the files using pandas `read_fwf` with `delimiter='##'` and `skiprows = 10` parameters for the first two folders and `delimiter='##'`,`skiprows = 1` and `encoding=\"ISO-8859-1\"` for the third folder to accomodate appropriate parsing. Then drops a few unnecessary rows and stores all parsed dataframes in a list. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "922f720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(your_path_to_data):\n",
    "    \n",
    "    # First two folders in Data \n",
    "    files_dir = ['/Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "                 '/Data/Customer_review_data/Canon G3.txt',\n",
    "                 '/Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "                 '/Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "                 '/Data/Customer_review_data/Nokia 6610.txt',\n",
    "                 '/Data/CustomerReviews-3_domains/Computer.txt',\n",
    "                 '/Data/CustomerReviews-3_domains/Router.txt',\n",
    "                 '/Data/CustomerReviews-3_domains/Speaker.txt']\n",
    "    \n",
    "    # Third folder which requires ISO-8859-1 encoding \n",
    "    files_dir_two = ['/Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "                     '/Data/Reviews-9-products/Canon S100.txt',\n",
    "                     '/Data/Reviews-9-products/Diaper Champ.txt',\n",
    "                     '/Data/Reviews-9-products/Hitachi router.txt',\n",
    "                     '/Data/Reviews-9-products/ipod.txt',\n",
    "                     '/Data/Reviews-9-products/Linksys Router.txt',\n",
    "                     '/Data/Reviews-9-products/MicroMP3.txt',\n",
    "                     '/Data/Reviews-9-products/Nokia 6600.txt',\n",
    "                     '/Data/Reviews-9-products/norton.txt']\n",
    "\n",
    "    items = ['apex_ad_2600', 'canon_g3', 'Creative Labs','Nikkon Coolpix 4300',\n",
    "             'Nokia 6610','Computer','Router','Speaker']\n",
    "    \n",
    "    # ISO-8859-1 encoding files\n",
    "    items_two = ['Canon Powershot SD500','Canon S100','Diaper Champ','Hitachi Router',\n",
    "                 'Ipod','Linksys Router','Micro MP3','Nokia 6600','Norton']\n",
    "    \n",
    "    global tot_items\n",
    "    tot_items = ['Apex AD 2600','Canon G3','Creative Labs','Nikkon Coolpix 4300',\n",
    "                 'Nokia 6610','Computer','Router','Speaker','Canon Powershot SD500',\n",
    "                 'Canon S100','Diaper Champ','Hitachi Router','Ipod','Linksys Router',\n",
    "                 'Micro MP3','Nokia 6600','Norton']\n",
    "    \n",
    "    df_dict = {}\n",
    "    for idx, file in enumerate(files_dir):      \n",
    "        df_dict[items[idx]] = pd.read_fwf(your_path_to_data+file,\n",
    "                                          skiprows = 10,header=None, delimiter='##',\n",
    "                                          skip_blank_lines=True)\n",
    "        # Drop meaningless rows\n",
    "        if items[idx] == 'Creative Labs':\n",
    "            df_dict[items[idx]] = df_dict[items[idx]].drop([1564,1568,1570,1574,1576,1596,1598,1602,1604,1606,\n",
    "                                                            1609,1613,1615,1617,1622,1624,1627,1632,1634,1636])\n",
    "        elif items[idx] == 'Computer':\n",
    "            df_dict[items[idx]] = df_dict[items[idx]].drop([306,309,313,316,393,391])\n",
    "        elif items[idx] == 'Router':\n",
    "            df_dict[items[idx]] = df_dict[items[idx]].drop([789,794,798,810,814,816,820,827,840,850])\n",
    "    \n",
    "    # ISO-8859-1 encoding files\n",
    "    for idx, file in enumerate(files_dir_two):\n",
    "\n",
    "        df_dict[items_two[idx]] = pd.read_fwf(your_path_to_data+file,\n",
    "                                          skiprows = 1,header=None, delimiter='##',\n",
    "                                          encoding=\"ISO-8859-1\", skip_blank_lines=True)\n",
    "        # Exclude [t] rows and dropna \n",
    "        df_dict[items_two[idx]] = df_dict[items_two[idx]][df_dict[items_two[idx]] != '[t]'].dropna()\n",
    "        \n",
    "        # Drop meaningless rows\n",
    "        if items_two[idx] == 'Linksys Router':\n",
    "            df_dict[items_two[idx]] = df_dict[items_two[idx]].drop([195,217])\n",
    "        elif items_two[idx] == 'Micro MP3':\n",
    "            df_dict[items_two[idx]] = df_dict[items_two[idx]].drop([19,37,196,218])\n",
    "        elif items_two[idx] == 'Nokia 6600':\n",
    "            df_dict[items_two[idx]] = df_dict[items_two[idx]].drop([19,37,196,218])\n",
    "    \n",
    "    return list(df_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3624ff",
   "metadata": {},
   "source": [
    "# Pipeline 1 step 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216533f1",
   "metadata": {},
   "source": [
    "Function `data_preprocessing_with_lables` gets passed in a list of dataframes from `load_data` and applies the following pre-processing methods:\n",
    "<br>\n",
    "\n",
    "* Removes capital letters\n",
    "* Removes digits\n",
    "* Removes extra spaces\n",
    "* Fixes word contractions\n",
    "* Removes punctuation\n",
    "* Splits concatenated words (Viterbi algorithm)\n",
    "* Applies tokenization\n",
    "* Removes stop words\n",
    "* Applies stemming\n",
    "<br>\n",
    "\n",
    "The end goal of this second step in the pipeline is to clean the data and have it ready for downstream consumption.\n",
    "<br>\n",
    "\n",
    "The first five points above should be self-explanatory and are achived by using conventional python methods except for the imported `contractions` library.\n",
    "<br>\n",
    "\n",
    "To split concatenated words the Viterbi algorithm is used. This function calculates the maximum posteriori probability estimate of the most likely sequence given an obervation sequence and selects the one with the highest probability [1][2]. \n",
    "<br>\n",
    "\n",
    "Tokenization is used for breaking the raw text into individual tokens. These tokens help in interpreting the meaning of the text by analysing the sequence of words and is an essential step for subsequent methods which will be touched upon in the next section.\n",
    "<br>\n",
    "\n",
    "Stop words are a set of commonly used words in a language. Examples of English stop words are “a”, “the”, “is\" and etc. In the context of Natural Language Processing (NLP) these commonly used words are eliminated as they carry high noise to signal ratio when training models.\n",
    "<br>\n",
    "\n",
    "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat. This is also the method used by search engines to index words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ad617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    \n",
    "    all_datasets_pre = []\n",
    "    for review_dataset in data:\n",
    "\n",
    "        # Orinal text to stip out reviews in feature_extraction func\n",
    "        review_data = review_dataset.copy()\n",
    "\n",
    "        # Remove capital letters\n",
    "        data = review_dataset.iloc[:,0].str.lower()\n",
    "\n",
    "        # Remove digits        \n",
    "        data = data.apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n",
    "\n",
    "        # Remove extra spaces\n",
    "        data = data.apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "        # Fix word contractions (i.e: I'd like -> I would like)\n",
    "        data = data.apply(lambda x: contractions.fix(x))\n",
    "\n",
    "        # Define punctuation string library\n",
    "        english_punctuations = string.punctuation\n",
    "\n",
    "        # Punctuation function \n",
    "        def remove_punctuations(text):\n",
    "            translator = str.maketrans('', '', english_punctuations)\n",
    "            return text.translate(translator)\n",
    "\n",
    "        # Remove punctuation \n",
    "        data = data.apply(remove_punctuations)\n",
    "\n",
    "        # Apply Viterbi algorithm to split concatenated words\n",
    "        def split_concat_words(text):\n",
    "            new_text = ' '.join(viterbi_algorithm(wordmash) for wordmash in text.split())\n",
    "            return new_text\n",
    "\n",
    "        data = data.apply(lambda x: split_concat_words(x))\n",
    "\n",
    "        # Use DataFrame format\n",
    "        dataframe = pd.DataFrame(data)\n",
    "\n",
    "        dataframe['Raw_text'] = review_data\n",
    "\n",
    "        # Apply tokenization on raw but cleaned data \n",
    "        dataframe[\"PoS_Tokens\"] = dataframe.iloc[:,0].apply(word_tokenize)\n",
    "        dataframe['Str_PoS_tokens'] = dataframe['PoS_Tokens'].apply(lambda x: ' '.join(x))\n",
    "        \n",
    "        # Remove stop words\n",
    "        stop = stopwords.words('english')\n",
    "        dataframe['Review_without_stopwords'] = dataframe.iloc[:,0].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "        # Apply word tokenize\n",
    "        dataframe[\"Tokens\"] = dataframe[\"Review_without_stopwords\"].apply(word_tokenize)\n",
    "\n",
    "        # Apply stemming\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        dataframe['Stemmed_tokens'] = dataframe['Tokens'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "        # Transform stemmed tokens back to string format \n",
    "        dataframe['Str_Stemmed_tokens'] = dataframe['Stemmed_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "        all_datasets_pre.append(dataframe)\n",
    "        \n",
    "    return all_datasets_pre\n",
    "\n",
    "\n",
    "word_prob = get_frequency_dict(lang='en', wordlist='large')\n",
    "max_word_len = max(map(len, word_prob)) \n",
    "\n",
    "\n",
    "def viterbi_algorithm(text):\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        new_probs = []\n",
    "        for j in range(max(0, i - max_word_len), i):\n",
    "            substring = text[j:i]\n",
    "            length_reward = np.exp(len(substring))\n",
    "            freq = word_prob.get(substring, 0) * length_reward\n",
    "            compounded_prob = probs[j] * freq\n",
    "            new_probs.append((compounded_prob, j))\n",
    "        \n",
    "        # max of a touple is the max across the first elements, which is the max of the compounded probabilities\n",
    "        prob_k, k = max(new_probs)\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "\n",
    "    # when text is a word that doesn't exist, the algorithm breaks it into individual letters.\n",
    "    # in that case, return the original word instead\n",
    "    if len(set(lasts)) == len(text):\n",
    "        return text\n",
    "    \n",
    "    words = []\n",
    "    k = len(text)\n",
    "    while 0 < k:\n",
    "        word = text[lasts[k]:k]\n",
    "        words.append(word)\n",
    "        k = lasts[k]\n",
    "    words.reverse()\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57aecd",
   "metadata": {},
   "source": [
    "# Pipeline 1 step 3: Product feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13596c5",
   "metadata": {},
   "source": [
    "In the third step of the pipeline features are extracted from the data cleaned in step two.<br>\n",
    "Function `feature_extraction` gets a list of dataframes from `data_preprocessing` and performs the following operations:<br>\n",
    "* Extracts Part of Speech (PoS) tags from tokenised words using NLTK\n",
    "* Applies NLTK Chunker on PoS tags and extracts noun chunks\n",
    "* Cleans chunks to extract product features\n",
    "* Extracts adjectives from PoS tags and cleans them\n",
    "* Extracts product reviews from original text.\n",
    "\n",
    "PoS tags are label assigned to each token (word) in a text corpus to assign grammatical information of each word of the sentence (adjectives, nouns ect..). PoS tagging is applied on tokenized words using a supervised learning algorithm that uses features like previous word and next word to determine the statistical most likely tag.<br>\n",
    "On top of PoS a Chunker is applied. Chunking is the process of extracting phrases from PoS tags. This pipeline explores through the `grammar` variable the use of regex parsing to filter out noun phrases and adjectives. The following pipeline, among other things, explores the quality of noun phrases extracted using the `Spacy` library.<br>\n",
    "Regex is also used to extract customer product reviews denoted with a `+`, `-` sign and a `1,2,3` magnidute inside `[]` brakets. Note the magnite was not taken into account as more likely to be user subjective.\n",
    "<br>\n",
    "\n",
    "As a last step, rows containing `NaN` values are dropped to prepare datasets for vectorisation and ultimatly be fed to machine learning models. This is a pivotal point where crucial design and engineering decisions had to be made since the number of NaN rows (majority attributed to missing reviews) is considerably large. Because of this, pipeline 2 will expand on this approach by exploring a different engineering solution in the attempt to minimise data wastage. \n",
    "<br>\n",
    "\n",
    "Back to the current workflow, the list of datrames is passed to `split_data_apply_tfidf` which splits the data into an 80/20 training and testing set and applies `TfidfVectorizer fit_transofrm` method on the traing using `chosen_feature[feature_num]` pipeline settable parameters and `transform` on the testing set using same parameters settings. These parameters are the product features and sentiment-bearing sentences. Worth noting that several combinations have been tried and in our case `Review_without_stopwords` yielded the highest overall accururacy as well as recall, precision and F1 score. The pipeline is currently set to use this feature facilitated via variable `feature_num = 0`.<br>\n",
    "Term frequency-inverse document frequency (TD-IDF) combines 2 concepts, Term Frequency (TF) and Document Frequency (DF). Term frequency is the number of occurrences of a specific term in a document. Inverse document frequency (IDF) calculates the weight of a term by reducing it's weight if the term’s occurrences are very present in the document. <br>The role of TD-IDF is to transform text into vectors that can be consumed by machine learning models.\n",
    "To conclude, once both training and testing datasets across all product reviews are vectorised, they get appended to `all_datasets_split` dictionary concluding the feature extraction step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3605d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(all_datasets_pre):\n",
    "    \n",
    "    \n",
    "    global pos_eval, chunks_eval, adj_eval, df_global_features\n",
    "    pos_eval = []\n",
    "    chunks_eval = []\n",
    "    adj_eval = []\n",
    "    df_global_features = []\n",
    "    \n",
    "    all_datasets_extract = {}\n",
    "    \n",
    "    for idx, review_dataset in enumerate(all_datasets_pre):\n",
    "        \n",
    "        dataframe = review_dataset.copy()\n",
    "\n",
    "        # extract Part of Speech tags from tokenised words\n",
    "        dataframe['PoS_Full'] = dataframe['PoS_Tokens'].apply(lambda x: pos_tag(x))\n",
    "\n",
    "        # Define chunk grammar \n",
    "        grammar = [\"NP: {<DT>?<JJ.*>*<NN.*>+}\",\"JJ: {<VB>*<JJ.*>+}\",\"NP: {<[CDJNP].*>+}\",\n",
    "                   \"NP: {<NN.?>+<NN.?>}\"]\n",
    "\n",
    "\n",
    "        # Create chunk parser\n",
    "        chunkParser = nltk.RegexpParser(grammar[0])\n",
    "\n",
    "        # Apply NLTK chunk parser\n",
    "        dataframe['Chunk'] = dataframe['PoS_Full'].apply(lambda x: chunkParser.parse(x))\n",
    "\n",
    "        def extract_chunks(chunks):\n",
    "            for a in chunks:\n",
    "                if isinstance(a, nltk.tree.Tree):\n",
    "                    if a.label() == \"NP\":\n",
    "                        return a\n",
    "\n",
    "        dataframe['Extracted_Chunks'] = dataframe['Chunk'].apply(lambda x: extract_chunks(x))\n",
    "\n",
    "        def clean_extracted_chunks(chunks):\n",
    "\n",
    "            if chunks is None:\n",
    "                return None\n",
    "\n",
    "            feature = \"\"\n",
    "            extracted_feature = \"\"\n",
    "            for num, idx in enumerate(list(chunks)):\n",
    "\n",
    "                # Include only words longer than 2 latters\n",
    "                if len(idx[0]) >= 3:            \n",
    "                    feature += idx[0]\n",
    "                    feature += \" \"\n",
    "            feature = feature[:-1] # Get rid of last space \n",
    "            extracted_feature = feature\n",
    "            return extracted_feature\n",
    "\n",
    "        dataframe['Extracted_feature'] = dataframe['Extracted_Chunks'].apply(lambda x: clean_extracted_chunks(x))\n",
    "\n",
    "        # Create adjective chunk parser\n",
    "        chunkParser = nltk.RegexpParser(grammar[1])\n",
    "\n",
    "        def extract_chunks_jj(chunks):\n",
    "            for a in chunks:\n",
    "                if isinstance(a, nltk.tree.Tree):\n",
    "                    if a.label() == \"JJ\":\n",
    "                        return a\n",
    "\n",
    "        dataframe['Adjectives'] = dataframe['PoS_Full'].apply(lambda x: (extract_chunks_jj(chunkParser.parse(x))))\n",
    "        dataframe['Extracted_adjectives'] = dataframe['PoS_Full'].apply(lambda x: clean_extracted_chunks(extract_chunks_jj(chunkParser.parse(x))))\n",
    "        \n",
    "        # Store values for each dataset for evaluation metrics\n",
    "        pos_eval.append(dataframe['PoS_Full'])\n",
    "        chunks_eval.append(dataframe['Chunk'])\n",
    "        adj_eval.append(dataframe['Adjectives'])\n",
    "\n",
    "        # Store negative reviews\n",
    "        negative_reviews = [\"-1\",\"-2\",\"-3\"]\n",
    "\n",
    "        # Extract reviews\n",
    "        def split_review(raw_review):\n",
    "\n",
    "            # Regex to get [x] from text\n",
    "            review = re.findall(r\"\\[\\s*\\+?(-?\\d+)\\s*\\]\", raw_review)\n",
    "\n",
    "            if len(review) == 0: # Missing review\n",
    "                return None\n",
    "            elif review[0] in negative_reviews:\n",
    "                return 0\n",
    "            return 1\n",
    "\n",
    "        # Apply review extraction\n",
    "        dataframe['Review'] = dataframe['Raw_text'].apply(lambda x: split_review(x))\n",
    "\n",
    "        # those with none values\n",
    "        df_global = dataframe.dropna() \n",
    "        \n",
    "        # Append dataframes to dict of lists\n",
    "        all_datasets_extract[idx] = []\n",
    "        all_datasets_extract[idx].append(df_global)\n",
    "    \n",
    "    return all_datasets_extract\n",
    "\n",
    "\n",
    "def split_data_apply_tfidf(all_datasets_extract):\n",
    "\n",
    "    # Use also to vectorize nan reviews\n",
    "    global vectorizer_list\n",
    "    vectorizer_list = []\n",
    "    all_datasets_split = {}\n",
    "\n",
    "    for key, review_dataset in all_datasets_extract.items():\n",
    "\n",
    "        df_global = pd.DataFrame(review_dataset[0])       \n",
    "            \n",
    "        # Dataset\n",
    "        X = df_global.iloc[:,:-1]\n",
    "\n",
    "        # Labels\n",
    "        y = df_global.iloc[:,-1:]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "        \n",
    "        # Apply tfidf vectorizer\n",
    "        vectorizer = TfidfVectorizer(stop_words='english',ngram_range=(1, 2))\n",
    "\n",
    "        def vectorized_reviews(vectorizer, X_train, X_test):\n",
    "            X_train = vectorizer.fit_transform(X_train[chosen_feature[feature_num]])\n",
    "            \n",
    "            # Append fit transform state for nan review\n",
    "            vectorizer_list.append(vectorizer)\n",
    "            \n",
    "            X_test = vectorizer.transform(X_test[chosen_feature[feature_num]])\n",
    "\n",
    "            return X_train, X_test\n",
    "\n",
    "        X_train, X_test = vectorized_reviews(vectorizer, X_train, X_test)\n",
    "    \n",
    "        all_datasets_split[key] = []\n",
    "        all_datasets_split[key].extend([X_train, X_test, y_train, y_test, df_global])\n",
    "    \n",
    "    return all_datasets_split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b173e",
   "metadata": {},
   "source": [
    "# Pipeline 1 step 4: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787c12a",
   "metadata": {},
   "source": [
    "Step four is the modelling phase. This is where all the hard work performed in step one through three (data parsing, data pre-processing and features extraction) come to fruition. \n",
    "<br>\n",
    "\n",
    "This section of the pipeline explores the performance of a baseline Random Forest Classifier (RFC) and compares its performance against an hyperparameter optimised version of itself. Random forest was chosen because of its good performance with high dimentional datasets thanks to the splitting of data into subsets. Since `TfidfVectorizer` was used, which largly increases the curse of dimensionality, using Random Forest seemed to be the right choice to start with. Pipeline two will expand on this by applying several different machine learing models and compare their performance.\n",
    "<br>\n",
    "\n",
    "Back to RFC, this classification algorithm is formed by many decisions trees. Uses bagging and feature randomness when building each individual tree in the attempt to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. Bagging is what makes Random Forests performing classifiers. At its core is an ensemble algorithm that fits multiple models on different subsets of a training dataset to then combine all the predictions by coming up with a more accurate and stable forecast.\n",
    "<br>\n",
    "\n",
    "In terms of implementation, function `models_training` reads in a dictionary of lists from `split_data_apply_tfidf` each containing product reviews vectorised training and test datasets. \n",
    "An RFC baseline model is fitted on each product train set and predicted results for each product are stored for downstream comparisons. \n",
    "Next a new RFC model is trained using GridSearchCV which is a technique that searches through a predefined parameter space to find the optimal values. Could be thought of as a cross-validation method where the model and the parameters are fed in on different folders and the best parameter set are extracted and predictions using this set are made.\n",
    "<br>\n",
    "\n",
    "Below are the parameters used [3]: \n",
    "\n",
    "* `bootstrap`: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree\n",
    "* `n_estimators`: Number of trees in the forest\n",
    "* `criterion`: Function to measure the quality of a split.<br>\n",
    "Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.<br>\n",
    "Note: this parameter is tree-specific.\n",
    "* `min_samples_leaf`: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.<br>\n",
    "If int, then consider min_samples_leaf as the minimum number.<br>\n",
    "If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "* `max_features`: Number of features to consider when looking for the best split:<br>\n",
    "If “sqrt”, then max_features=sqrt(n_features).<br>\n",
    "If “log2”, then max_features=log2(n_features).\n",
    "\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c01a5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def models_training(all_datasets_split):\n",
    "    \n",
    "    \n",
    "    global model, grid_search, grid_params\n",
    "    model = []\n",
    "    grid_search = []\n",
    "    \n",
    "    all_datasets_training = {}\n",
    "    for key, review_dataset in all_datasets_split.items():\n",
    "        \n",
    "        X_train = review_dataset[0]\n",
    "        X_test = review_dataset[1]\n",
    "        y_train = review_dataset[2]\n",
    "        y_test = review_dataset[3]\n",
    "        df_global = review_dataset[4]\n",
    "\n",
    "        model.append(RandomForestClassifier())\n",
    "                \n",
    "        # Train baseline model for comparison\n",
    "        baseline_model = model[key].fit(X_train, y_train)\n",
    "        baseline_pred = baseline_model.predict(X_test)\n",
    "        \n",
    "        # Set parameter grid\n",
    "        bootstrap_v = [True, False]\n",
    "        n_estimators_v = list(range(100,600,100)) \n",
    "        # n_estimators_v = list(range(100,2000,200))\n",
    "        criterion = ['gini', 'entropy']\n",
    "        min_sample_leaf_v = list(range(1,5,2))\n",
    "        max_features_v = ['sqrt', 'log2']\n",
    "\n",
    "        grid_params = {\n",
    "            'bootstrap' : bootstrap_v,\n",
    "            'n_estimators' : n_estimators_v,\n",
    "            'criterion' : criterion,\n",
    "            'min_samples_leaf' : min_sample_leaf_v,\n",
    "            'max_features' : max_features_v\n",
    "        }\n",
    "\n",
    "        # Search best set of params\n",
    "        grid_search.append(GridSearchCV(estimator=model[key], param_grid=grid_params, cv=3, verbose=1))\n",
    "\n",
    "        # Fit model using best params\n",
    "        grid_search[key].fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = grid_search[key].predict(X_test)\n",
    "        \n",
    "        all_datasets_training[key] = []\n",
    "        all_datasets_training[key].extend([y_test, baseline_pred, predictions, df_global])\n",
    "\n",
    "    return all_datasets_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301bde6",
   "metadata": {},
   "source": [
    "# Pipeline 1 step 5: Evaluation and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e2ed5c",
   "metadata": {},
   "source": [
    "In this final step of the pipeline, preformance metrics are outputted on the following: \n",
    "<br>\n",
    "* PoS tags: Breakdown of Precicion, Recall, F-1 and Accuracy score across all tags (i.e. DT, IN, JJ, NN)\n",
    "* Adjective phrases: Breakdown of Precicion, Recall, F-1 score and Accuracy across adjectives tags\n",
    "* Chunked noun sentences: IOB, Precision, Recall, F-measure and Accuracy\n",
    "* Baseline RFC model: Precision, Recall, F-measure and Accuracy\n",
    "* Hyperparameter optimised RFC: Precision, Recall, F-measure and Accuracy\n",
    "<br>\n",
    "\n",
    "`Precision` quantifies the number of positive class predictions that actually belong to the positive class. Is caclulated as `TruePositives / (TruePositives + FalsePositives)`\n",
    "<br>\n",
    "\n",
    "`Recall` quantifies the number of positive class predictions made out of all positive examples in the dataset. Is calculated as `TruePositives / (TruePositives + FalseNegatives)` \n",
    "<br>\n",
    "\n",
    "`F-Measure` provides a single score that balances both the concerns of precision and recall in one number. Is calculated as `(2 * Precision * Recall) / (Precision + Recall)`\n",
    "<br>\n",
    "\n",
    "To calculate these stats a `UnigramTagger`, which is a tagger that uses a single word as its context for determining PoS tags, is trained on the `conll2000.tagged_sents` train.txt corpus which contains 270k words of Wall Street Journal text.<br>\n",
    "These trained tags are the 'gold standard' with which the results of the PoS tags, adjectives and chunked noun sentences from step three are compared against.\n",
    "<br>\n",
    "The results across the spectrum are quite encouraging with Pecision showing slightly higher figures (about 85%) than Recall on PoS tags and Adjective phrases indicating that the trained model returns most of the relevant results. <br>\n",
    "Recall is higher on Chunked sentenses (about 85%) than Precision which averages to about 60%. This is a considerable difference driven by many false positives and because of this, unlike for the PoS tags and adjectives, F-measure also takes a performance hit settling around 70% on average.\n",
    "<br>\n",
    "<br>\n",
    "Moving on to the models performance, its worth noting the relatively high score of the baseline Random Forest Classifier. This is the model trained without any hyperparameter optimisation. Although, comes with quite some precision variance on some datasets where scores are in the 60 percentile vs 80+ on the others. <br>\n",
    "The hyperparameter optimised models do perform better but require computational resorces to achieve full potential. In the interest of keeping running time managable `n_estimators_v = list(range(100,2000,200))` was left commented out. This increases considerably the search space and allows the cost function to get closer to global minima. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5627e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code borrowed from NLTK [4]\n",
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "        def __init__(self, train_sents):\n",
    "            train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                          for sent in train_sents]\n",
    "            self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "        def parse(self, sentence):\n",
    "            pos_tags = [pos for (word,pos) in sentence]\n",
    "            tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "            chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "            conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                         in zip(sentence, chunktags)]\n",
    "            return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb1e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(all_datasets_training):\n",
    "    \n",
    "    all_datasets_eval = all_datasets_training.copy()\n",
    "\n",
    "    for idx, (key, review_dataset) in enumerate(all_datasets_training.items()):\n",
    "        \n",
    "        # Unpack dict\n",
    "        y_test = review_dataset[0]\n",
    "        baseline_pred = review_dataset[1]\n",
    "        predictions = review_dataset[2]\n",
    "        df_global = review_dataset[3]\n",
    "        \n",
    "        train_sentences = conll2000.tagged_sents('train.txt')\n",
    "\n",
    "        test_sentences = {'Part of Speech tags': pos_eval[key].dropna().to_list(),\n",
    "                          'Adjectives phrases': adj_eval[key].dropna().to_list()}\n",
    "\n",
    "        # Train the tagger\n",
    "        unigram_tagger = UnigramTagger(train_sentences)\n",
    "        \n",
    "        print('===='*27)\n",
    "        print(' '*33,f'Evaluation metrics for {tot_items[idx]} dataset')\n",
    "        print('===='*27)\n",
    "\n",
    "        for key, test_sentence in test_sentences.items():\n",
    "\n",
    "            print(f'\\nEvaluation metrics for {tot_items[idx]}: {key}')\n",
    "            print('='*52)\n",
    "            tagged_test_sentences = unigram_tagger.tag_sents([[token for token,tag in sent] for sent in test_sentence])\n",
    "            gold = [str(tag) for sentence in test_sentence for token,tag in sentence]\n",
    "            pred = [str(tag) for sentence in tagged_test_sentences for token,tag in sentence]\n",
    "\n",
    "            print(metrics.classification_report(gold, pred),'\\n\\n')\n",
    "\n",
    "        print(f'Evaluation metrics for {tot_items[idx]}: Chunked sentences')\n",
    "        print('='*53)\n",
    "        train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "        test_sentences = chunks_eval[idx].dropna()\n",
    "\n",
    "        train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "        unigram_chunker = UnigramChunker(train_sents)\n",
    "        print(unigram_chunker.evaluate(test_sentences))\n",
    "        print('='*53)\n",
    "        print(f'\\n\\nBest Parameters for {tot_items[idx]} classsifier')\n",
    "        print('='*53)\n",
    "        print(grid_search[idx].best_params_)\n",
    "        print('='*53)\n",
    "\n",
    "        #pprint(grid_search[idx].get_params())\n",
    "        report = classification_report(y_test, predictions)\n",
    "        baseline_report = classification_report(y_test, baseline_pred)\n",
    "\n",
    "        score = accuracy_score(y_true=y_test, y_pred=predictions)\n",
    "        baseline_score = accuracy_score(y_true=y_test, y_pred=baseline_pred)\n",
    "        \n",
    "        print(f'\\n\\nReport of the baseline Random Forest Classifier model')\n",
    "        print('=='*27)\n",
    "        print(baseline_report)\n",
    "        print('=='*27)\n",
    "        print(\"{} {:0.2f}%\".format(\"Accuracy Score: \", baseline_score*100))\n",
    "        print('=='*27)\n",
    "        print(f'\\n\\nReport of the Optimised Random Forest Classifier model')\n",
    "        print('=='*27)\n",
    "        print(report)\n",
    "        print('=='*27)\n",
    "        print(\"{} {:0.2f}%\".format(\"Accuracy Score: \", score*100))\n",
    "        print('=='*27)\n",
    "        print('\\n\\n\\n')\n",
    "    \n",
    "    return all_datasets_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f8886",
   "metadata": {},
   "source": [
    "To complete the evaluation metrics step, for each dataset, function `review_similarity_count` outpus a summary table of top features and their review polarity count. <br> \n",
    "\n",
    "As a first step the function reads in the reviews and transforms them from binary `1`, `0` (used in training) to `Positive` and `Negative` strings. <br>\n",
    "Then all extracted features, which are derived from noun phrases, are compared against each other using Levenshtein distance. Levenshtein is a metric that measures the difference between strings. Can be thought as the euclidean distance between two words is the minimum number of edits required to change one string into the other [5]. Various testings have been carried on to find the right balance between merging to words together. On this pipeline the optimal point was found at 0.7 and this ratio is also set as a tunable pipeline hyperparameter under the `levenshtein_ratio_one` variable. \n",
    "<br>\n",
    "\n",
    "The main constrained encountered was the lack of features aboundance mainly because lerge percentage of data on each dataset was dropped due to missing reviews. The engeneering solution of pipeline two was designed to overcome such shortage.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "This brings us to the end of this pipeline! In the notebook cell below `review_similarity_count` function, evaluation summaries split by product review can be found, followed by summary tables of top features for each product review. Enjoy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a2573ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def review_similarity_count(all_datasets_eval):\n",
    "    \n",
    "    for idx, (key, review_dataset) in enumerate(all_datasets_eval.items()):\n",
    "                \n",
    "        # Unpack df_global\n",
    "        review_count = review_dataset[3].copy().dropna()\n",
    "        \n",
    "        # Convert bool review to string \n",
    "        review_count['Review'] = review_count['Review'].apply(lambda x: 'Positive' if x == 1 else 'Negative')\n",
    "\n",
    "        print('===='*27)\n",
    "        print(' '*33,f'Top Features extracted from {tot_items[idx]} dataset')\n",
    "        print('===='*27,'\\n')\n",
    "        \n",
    "        # Loop through features \n",
    "        for idx, word1 in enumerate(review_count['Extracted_feature']):\n",
    "            for idy, word2 in enumerate(review_count['Extracted_feature']):\n",
    "                # Assess word similarity\n",
    "                ratio = Levenshtein.ratio(word1, word2)\n",
    "                if ratio > levenshtein_ratio_one:\n",
    "                    # Get smallest name\n",
    "                    if len(word1) < len(word2):\n",
    "                        review_count['Extracted_feature'].iloc[idy] = word1.capitalize()\n",
    "                    else:\n",
    "                        review_count['Extracted_feature'].iloc[idx] = word2.capitalize()\n",
    "\n",
    "        # Gruop product feature review and count \n",
    "        review_count = review_count.groupby(['Extracted_feature', 'Review']).agg({'Review': ['count']})\n",
    "        review_count = review_count.sort_values(('Review', 'count'), \n",
    "                                                ascending=False).groupby(level=1).head(5).sort_index().reset_index()\n",
    "        cols =[('Extracted_feature',''),\n",
    "               ('Review',''),\n",
    "               ('Review', 'count')]\n",
    "\n",
    "        review_count = review_count.groupby(['Extracted_feature',('Review','')])[cols].sum()\n",
    "        review_count.index.names = ['Extracted Features','Reviews']\n",
    "        review_count.columns = review_count.columns.set_levels(['Count'], level=1)\n",
    "\n",
    "        print(review_count,'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09c142dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   41.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   35.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   29.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   36.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   35.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   36.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   37.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   28.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   30.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   30.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   31.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   31.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   31.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   59.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   45.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   32.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================\n",
      "                                  Evaluation metrics for Apex AD 2600 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Apex AD 2600: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      1.00      1.00       423\n",
      "          CD       0.88      1.00      0.93        91\n",
      "          DT       0.99      0.98      0.99      1172\n",
      "          EX       0.69      1.00      0.82        20\n",
      "          FW       0.20      0.50      0.29         2\n",
      "          IN       0.91      0.97      0.94      1133\n",
      "          JJ       0.89      0.58      0.71       986\n",
      "         JJR       0.70      0.95      0.81        44\n",
      "         JJS       1.00      0.58      0.73        31\n",
      "          MD       1.00      1.00      1.00       212\n",
      "          NN       0.90      0.53      0.67      2938\n",
      "         NNP       0.00      0.00      0.00         7\n",
      "         NNS       0.94      0.63      0.76       502\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00         8\n",
      "         PRP       0.85      0.99      0.92       590\n",
      "        PRP$       1.00      1.00      1.00       152\n",
      "          RB       0.95      0.86      0.90       758\n",
      "         RBR       0.00      0.00      0.00        21\n",
      "         RBS       0.35      1.00      0.52         6\n",
      "          RP       0.00      0.00      0.00        62\n",
      "          TO       1.00      1.00      1.00       258\n",
      "          UH       0.00      0.00      0.00         1\n",
      "          VB       0.62      0.58      0.60       636\n",
      "         VBD       0.83      0.72      0.77       472\n",
      "         VBG       0.90      0.77      0.83       215\n",
      "         VBN       0.54      0.58      0.56       230\n",
      "         VBP       0.68      0.51      0.58       380\n",
      "         VBZ       0.94      0.79      0.86       367\n",
      "         WDT       1.00      0.48      0.65        46\n",
      "          WP       1.00      1.00      1.00        41\n",
      "         WRB       1.00      1.00      1.00        44\n",
      "\n",
      "    accuracy                           0.74     11848\n",
      "   macro avg       0.68      0.66      0.65     11848\n",
      "weighted avg       0.88      0.74      0.79     11848\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Apex AD 2600: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.58      0.74       563\n",
      "         JJR       1.00      0.95      0.98        22\n",
      "         JJS       1.00      0.61      0.76        18\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PRP       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.70      0.70      0.70        30\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.60       633\n",
      "   macro avg       0.28      0.22      0.24       633\n",
      "weighted avg       0.98      0.60      0.74       633\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Apex AD 2600: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  84.4%%\n",
      "    Precision:     59.5%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     70.6%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Apex AD 2600 classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1, 'n_estimators': 400}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.60      0.63        25\n",
      "         1.0       0.68      0.72      0.70        29\n",
      "\n",
      "    accuracy                           0.67        54\n",
      "   macro avg       0.66      0.66      0.66        54\n",
      "weighted avg       0.67      0.67      0.67        54\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  66.67%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.68      0.68        25\n",
      "         1.0       0.72      0.72      0.72        29\n",
      "\n",
      "    accuracy                           0.70        54\n",
      "   macro avg       0.70      0.70      0.70        54\n",
      "weighted avg       0.70      0.70      0.70        54\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  70.37%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Canon G3 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Canon G3: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.99      0.99      0.99       426\n",
      "          CD       0.94      1.00      0.97        46\n",
      "          DT       0.99      0.98      0.99      1244\n",
      "          EX       0.62      1.00      0.76        16\n",
      "          FW       0.29      0.67      0.40         3\n",
      "          IN       0.90      0.97      0.94      1168\n",
      "          JJ       0.90      0.60      0.72       979\n",
      "         JJR       0.62      0.93      0.74        55\n",
      "         JJS       1.00      0.74      0.85        58\n",
      "          MD       1.00      1.00      1.00       231\n",
      "          NN       0.85      0.57      0.69      2422\n",
      "         NNP       0.00      0.00      0.00         5\n",
      "         NNS       0.92      0.65      0.76       551\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00        17\n",
      "         PRP       0.89      1.00      0.94       458\n",
      "        PRP$       1.00      1.00      1.00       121\n",
      "          RB       0.94      0.83      0.88       659\n",
      "         RBR       0.00      0.00      0.00        30\n",
      "         RBS       0.25      1.00      0.40         4\n",
      "          RP       0.00      0.00      0.00        51\n",
      "          TO       1.00      1.00      1.00       252\n",
      "          UH       0.12      1.00      0.22         1\n",
      "          VB       0.59      0.51      0.55       547\n",
      "         VBD       0.82      0.69      0.75       278\n",
      "         VBG       0.90      0.76      0.83       191\n",
      "         VBN       0.65      0.68      0.67       206\n",
      "         VBP       0.67      0.54      0.60       378\n",
      "         VBZ       0.94      0.83      0.88       405\n",
      "         WDT       1.00      0.35      0.52        54\n",
      "          WP       1.00      1.00      1.00        50\n",
      "         WRB       1.00      1.00      1.00        32\n",
      "\n",
      "    accuracy                           0.76     10938\n",
      "   macro avg       0.68      0.70      0.66     10938\n",
      "weighted avg       0.87      0.76      0.80     10938\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Canon G3: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.57      0.72       501\n",
      "         JJR       1.00      0.91      0.95        22\n",
      "         JJS       1.00      0.72      0.84        32\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PRP       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          UH       0.00      0.00      0.00         0\n",
      "          VB       0.65      0.68      0.67        22\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBG       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.59       577\n",
      "   macro avg       0.24      0.19      0.21       577\n",
      "weighted avg       0.99      0.59      0.74       577\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Canon G3: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  85.8%%\n",
      "    Precision:     60.5%%\n",
      "    Recall:        88.4%%\n",
      "    F-Measure:     71.8%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Canon G3 classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.10      0.17        10\n",
      "         1.0       0.78      0.97      0.86        33\n",
      "\n",
      "    accuracy                           0.77        43\n",
      "   macro avg       0.64      0.53      0.52        43\n",
      "weighted avg       0.72      0.77      0.70        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  76.74%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.30      0.35        10\n",
      "         1.0       0.81      0.88      0.84        33\n",
      "\n",
      "    accuracy                           0.74        43\n",
      "   macro avg       0.62      0.59      0.60        43\n",
      "weighted avg       0.72      0.74      0.73        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  74.42%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Creative Labs dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Creative Labs: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.99      0.99      0.99      1005\n",
      "          CD       0.95      0.98      0.96       127\n",
      "          DT       0.99      0.97      0.98      3219\n",
      "          EX       0.72      1.00      0.84        58\n",
      "          FW       0.22      0.67      0.33         6\n",
      "          IN       0.88      0.97      0.92      2958\n",
      "          JJ       0.87      0.61      0.72      2495\n",
      "         JJR       0.73      0.91      0.81       169\n",
      "         JJS       0.90      0.65      0.76        72\n",
      "          MD       1.00      0.99      1.00       492\n",
      "          NN       0.87      0.56      0.68      6626\n",
      "         NNP       0.00      0.00      0.00        56\n",
      "         NNS       0.93      0.56      0.70      1417\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00        37\n",
      "         PRP       0.87      0.99      0.93      1363\n",
      "        PRP$       1.00      1.00      1.00       386\n",
      "          RB       0.92      0.81      0.86      2001\n",
      "         RBR       0.80      0.07      0.13        56\n",
      "         RBS       0.20      0.83      0.32         6\n",
      "          RP       0.00      0.00      0.00       185\n",
      "          TO       1.00      1.00      1.00       808\n",
      "          UH       0.00      0.00      0.00         1\n",
      "          VB       0.63      0.55      0.59      1595\n",
      "         VBD       0.84      0.74      0.79       834\n",
      "         VBG       0.92      0.75      0.83       497\n",
      "         VBN       0.64      0.56      0.60       528\n",
      "         VBP       0.73      0.60      0.66       965\n",
      "         VBZ       0.95      0.79      0.86      1130\n",
      "         WDT       1.00      0.49      0.66       141\n",
      "          WP       0.99      1.00      0.99        78\n",
      "         WRB       0.99      1.00      1.00       141\n",
      "\n",
      "    accuracy                           0.75     29452\n",
      "   macro avg       0.70      0.66      0.65     29452\n",
      "weighted avg       0.88      0.75      0.80     29452\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Creative Labs: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.00      0.00      0.00         0\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          IN       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.58      0.74      1320\n",
      "         JJR       1.00      0.93      0.96        68\n",
      "         JJS       1.00      0.68      0.81        41\n",
      "          NN       0.00      0.00      0.00         0\n",
      "         NNS       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PRP       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.57      0.80      0.67        50\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.61      1479\n",
      "   macro avg       0.22      0.19      0.20      1479\n",
      "weighted avg       0.99      0.61      0.75      1479\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Creative Labs: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  84.9%%\n",
      "    Precision:     59.5%%\n",
      "    Recall:        87.6%%\n",
      "    F-Measure:     70.8%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Creative Labs classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.29      0.42        51\n",
      "         1.0       0.65      0.92      0.76        72\n",
      "\n",
      "    accuracy                           0.66       123\n",
      "   macro avg       0.68      0.61      0.59       123\n",
      "weighted avg       0.67      0.66      0.62       123\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  65.85%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.37      0.48        51\n",
      "         1.0       0.66      0.86      0.75        72\n",
      "\n",
      "    accuracy                           0.66       123\n",
      "   macro avg       0.66      0.62      0.61       123\n",
      "weighted avg       0.66      0.66      0.63       123\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  65.85%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Nikkon Coolpix 4300 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Nikkon Coolpix 4300: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      0.99      0.99       226\n",
      "          CD       0.88      1.00      0.94        30\n",
      "          DT       0.97      0.98      0.98       637\n",
      "          EX       0.70      1.00      0.82         7\n",
      "          FW       0.00      0.00      0.00         1\n",
      "          IN       0.91      0.97      0.94       644\n",
      "          JJ       0.91      0.64      0.75       671\n",
      "         JJR       0.81      0.93      0.87        28\n",
      "         JJS       1.00      0.53      0.69        17\n",
      "          MD       1.00      1.00      1.00        80\n",
      "          NN       0.89      0.63      0.74      1428\n",
      "         NNP       0.00      0.00      0.00         4\n",
      "         NNS       0.91      0.67      0.78       329\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00        19\n",
      "         PRP       0.89      1.00      0.94       262\n",
      "        PRP$       1.00      1.00      1.00        71\n",
      "          RB       0.92      0.84      0.88       353\n",
      "         RBR       1.00      0.14      0.25         7\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          RP       0.00      0.00      0.00        32\n",
      "          TO       1.00      1.00      1.00       154\n",
      "          UH       0.00      0.00      0.00         0\n",
      "          VB       0.58      0.59      0.58       275\n",
      "         VBD       0.81      0.79      0.80       136\n",
      "         VBG       0.87      0.75      0.81       100\n",
      "         VBN       0.69      0.61      0.65        97\n",
      "         VBP       0.77      0.56      0.65       232\n",
      "         VBZ       0.98      0.82      0.89       233\n",
      "         WDT       1.00      0.60      0.75        40\n",
      "          WP       1.00      1.00      1.00         8\n",
      "         WRB       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.77      6138\n",
      "   macro avg       0.70      0.63      0.65      6138\n",
      "weighted avg       0.89      0.77      0.82      6138\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Nikkon Coolpix 4300: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          IN       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.59      0.74       314\n",
      "         JJR       1.00      0.92      0.96        12\n",
      "         JJS       1.00      0.67      0.80         9\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.50      0.75      0.60         4\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.60       339\n",
      "   macro avg       0.29      0.24      0.26       339\n",
      "weighted avg       0.99      0.60      0.75       339\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Nikkon Coolpix 4300: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  84.4%%\n",
      "    Precision:     59.5%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     70.6%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Nikkon Coolpix 4300 classsifier\n",
      "=====================================================\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         3\n",
      "         1.0       0.89      1.00      0.94        25\n",
      "\n",
      "    accuracy                           0.89        28\n",
      "   macro avg       0.45      0.50      0.47        28\n",
      "weighted avg       0.80      0.89      0.84        28\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  89.29%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         3\n",
      "         1.0       0.89      1.00      0.94        25\n",
      "\n",
      "    accuracy                           0.89        28\n",
      "   macro avg       0.45      0.50      0.47        28\n",
      "weighted avg       0.80      0.89      0.84        28\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  89.29%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Nokia 6610 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Nokia 6610: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.99      0.99      0.99       291\n",
      "          CD       0.98      0.97      0.98        62\n",
      "          DT       0.99      0.97      0.98       928\n",
      "          EX       0.81      1.00      0.90        22\n",
      "          FW       0.12      0.25      0.17         4\n",
      "          IN       0.88      0.96      0.92       848\n",
      "          JJ       0.86      0.60      0.71       828\n",
      "         JJR       0.62      0.94      0.74        31\n",
      "         JJS       1.00      0.73      0.84        33\n",
      "          MD       1.00      0.99      0.99       156\n",
      "          NN       0.88      0.60      0.71      2081\n",
      "         NNP       0.00      0.00      0.00         1\n",
      "         NNS       0.96      0.69      0.80       469\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00        13\n",
      "         PRP       0.93      1.00      0.96       389\n",
      "        PRP$       0.99      1.00      1.00       143\n",
      "          RB       0.93      0.79      0.85       592\n",
      "         RBR       1.00      0.05      0.09        21\n",
      "         RBS       0.50      1.00      0.67         6\n",
      "          RP       0.00      0.00      0.00        33\n",
      "          TO       1.00      1.00      1.00       214\n",
      "          UH       0.00      0.00      0.00         1\n",
      "          VB       0.65      0.51      0.58       399\n",
      "         VBD       0.79      0.76      0.77       191\n",
      "         VBG       0.95      0.66      0.78       152\n",
      "         VBN       0.69      0.58      0.63       157\n",
      "         VBP       0.71      0.58      0.64       337\n",
      "         VBZ       0.97      0.90      0.93       354\n",
      "         WDT       1.00      0.56      0.72        57\n",
      "          WP       1.00      1.00      1.00        18\n",
      "         WRB       1.00      1.00      1.00        49\n",
      "\n",
      "    accuracy                           0.76      8880\n",
      "   macro avg       0.72      0.66      0.67      8880\n",
      "weighted avg       0.89      0.76      0.81      8880\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Nokia 6610: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.00      0.00      0.00         0\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          IN       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.60      0.75       426\n",
      "         JJR       1.00      0.95      0.97        19\n",
      "         JJS       1.00      0.76      0.86        21\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          UH       0.00      0.00      0.00         0\n",
      "          VB       0.71      0.89      0.79        19\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.63       485\n",
      "   macro avg       0.25      0.21      0.23       485\n",
      "weighted avg       0.99      0.63      0.76       485\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Nokia 6610: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  83.6%%\n",
      "    Precision:     58.4%%\n",
      "    Recall:        85.1%%\n",
      "    F-Measure:     69.3%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Nokia 6610 classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        13\n",
      "         1.0       0.70      0.97      0.82        32\n",
      "\n",
      "    accuracy                           0.69        45\n",
      "   macro avg       0.35      0.48      0.41        45\n",
      "weighted avg       0.50      0.69      0.58        45\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  68.89%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.08      0.13        13\n",
      "         1.0       0.72      0.97      0.83        32\n",
      "\n",
      "    accuracy                           0.71        45\n",
      "   macro avg       0.61      0.52      0.48        45\n",
      "weighted avg       0.66      0.71      0.63        45\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.11%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Computer dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Computer: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.99      1.00      0.99       282\n",
      "          CD       0.92      1.00      0.96        34\n",
      "          DT       0.99      0.98      0.98       845\n",
      "          EX       0.93      1.00      0.96        13\n",
      "          FW       0.44      0.67      0.53         6\n",
      "          IN       0.90      0.97      0.93       814\n",
      "          JJ       0.89      0.64      0.75       795\n",
      "         JJR       0.72      0.77      0.74        43\n",
      "         JJS       1.00      0.67      0.80        12\n",
      "          MD       1.00      1.00      1.00       116\n",
      "          NN       0.87      0.51      0.64      1921\n",
      "         NNP       0.00      0.00      0.00         3\n",
      "         NNS       0.92      0.62      0.74       385\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00         7\n",
      "         PRP       0.90      1.00      0.95       371\n",
      "        PRP$       1.00      1.00      1.00       125\n",
      "          RB       0.93      0.83      0.88       552\n",
      "         RBR       0.25      0.06      0.09        18\n",
      "         RBS       0.40      1.00      0.57         2\n",
      "          RP       0.00      0.00      0.00        49\n",
      "          TO       1.00      1.00      1.00       165\n",
      "          VB       0.45      0.57      0.50       370\n",
      "         VBD       0.89      0.74      0.81       281\n",
      "         VBG       0.90      0.73      0.81       142\n",
      "         VBN       0.54      0.61      0.57       129\n",
      "         VBP       0.76      0.53      0.62       297\n",
      "         VBZ       0.94      0.77      0.84       273\n",
      "         WDT       1.00      0.38      0.55        29\n",
      "          WP       1.00      0.95      0.97        19\n",
      "         WRB       0.97      0.97      0.97        39\n",
      "\n",
      "    accuracy                           0.74      8137\n",
      "   macro avg       0.73      0.68      0.68      8137\n",
      "weighted avg       0.87      0.74      0.79      8137\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Computer: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.64      0.78       399\n",
      "         JJR       1.00      0.73      0.84        22\n",
      "         JJS       1.00      0.67      0.80         6\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PRP       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBR       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.59      0.67      0.62        15\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBG       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.65       442\n",
      "   macro avg       0.24      0.18      0.20       442\n",
      "weighted avg       0.98      0.65      0.78       442\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Computer: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  84.1%%\n",
      "    Precision:     58.4%%\n",
      "    Recall:        84.6%%\n",
      "    F-Measure:     69.1%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Computer classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         7\n",
      "         1.0       0.83      1.00      0.91        34\n",
      "\n",
      "    accuracy                           0.83        41\n",
      "   macro avg       0.41      0.50      0.45        41\n",
      "weighted avg       0.69      0.83      0.75        41\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  82.93%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         7\n",
      "         1.0       0.83      1.00      0.91        34\n",
      "\n",
      "    accuracy                           0.83        41\n",
      "   macro avg       0.41      0.50      0.45        41\n",
      "weighted avg       0.69      0.83      0.75        41\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  82.93%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Router dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Router: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      0.99      0.99       496\n",
      "          CD       0.92      0.98      0.95        94\n",
      "          DT       0.99      0.98      0.98      1469\n",
      "          EX       0.70      1.00      0.82        14\n",
      "          FW       0.12      0.20      0.15         5\n",
      "          IN       0.86      0.96      0.91      1319\n",
      "          JJ       0.91      0.54      0.68      1259\n",
      "         JJR       0.71      0.81      0.76        43\n",
      "         JJS       1.00      0.76      0.86        42\n",
      "          MD       1.00      1.00      1.00       224\n",
      "          NN       0.87      0.47      0.61      3454\n",
      "         NNP       0.00      0.00      0.00         5\n",
      "         NNS       0.95      0.66      0.78       560\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00        18\n",
      "         PRP       0.91      0.99      0.95       548\n",
      "        PRP$       0.99      1.00      1.00       255\n",
      "          RB       0.94      0.81      0.87       879\n",
      "         RBR       0.33      0.10      0.15        21\n",
      "         RBS       0.50      1.00      0.67         6\n",
      "          RP       0.00      0.00      0.00       109\n",
      "          TO       0.98      1.00      0.99       413\n",
      "          UH       0.00      0.00      0.00         0\n",
      "          VB       0.61      0.52      0.56       715\n",
      "         VBD       0.88      0.76      0.82       678\n",
      "         VBG       0.96      0.82      0.88       258\n",
      "         VBN       0.54      0.64      0.58       274\n",
      "         VBP       0.68      0.50      0.58       371\n",
      "         VBZ       0.98      0.80      0.88       353\n",
      "         WDT       1.00      0.48      0.65        75\n",
      "          WP       1.00      1.00      1.00        33\n",
      "         WRB       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           0.72     14043\n",
      "   macro avg       0.70      0.65      0.66     14043\n",
      "weighted avg       0.88      0.72      0.77     14043\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Router: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.57      0.72       640\n",
      "         JJR       1.00      1.00      1.00        16\n",
      "         JJS       1.00      0.75      0.86        20\n",
      "          NN       0.00      0.00      0.00         0\n",
      "         NNS       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PRP       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.45      0.68      0.55        22\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.59       698\n",
      "   macro avg       0.25      0.21      0.22       698\n",
      "weighted avg       0.98      0.59      0.73       698\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Router: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  85.0%%\n",
      "    Precision:     59.7%%\n",
      "    Recall:        84.0%%\n",
      "    F-Measure:     69.8%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Router classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.47      0.52        15\n",
      "         1.0       0.74      0.82      0.78        28\n",
      "\n",
      "    accuracy                           0.70        43\n",
      "   macro avg       0.66      0.64      0.65        43\n",
      "weighted avg       0.69      0.70      0.69        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  69.77%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.60      0.55        15\n",
      "         1.0       0.76      0.68      0.72        28\n",
      "\n",
      "    accuracy                           0.65        43\n",
      "   macro avg       0.63      0.64      0.63        43\n",
      "weighted avg       0.67      0.65      0.66        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  65.12%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Speaker dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Speaker: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      1.00      1.00       408\n",
      "          CD       0.91      1.00      0.95        40\n",
      "          DT       0.99      0.98      0.98      1308\n",
      "          EX       0.84      1.00      0.91        21\n",
      "          FW       0.11      0.33      0.17         3\n",
      "          IN       0.90      0.97      0.93      1262\n",
      "          JJ       0.87      0.62      0.73      1122\n",
      "         JJR       0.66      0.83      0.74        59\n",
      "         JJS       1.00      0.76      0.86        29\n",
      "          MD       1.00      0.99      0.99       177\n",
      "          NN       0.86      0.57      0.69      2732\n",
      "         NNP       0.00      0.00      0.00         7\n",
      "         NNS       0.93      0.66      0.77       638\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00        11\n",
      "         PRP       0.89      1.00      0.94       461\n",
      "        PRP$       0.99      1.00      1.00       193\n",
      "          RB       0.92      0.80      0.86       743\n",
      "         RBR       0.33      0.04      0.06        28\n",
      "         RBS       0.25      1.00      0.40         2\n",
      "          RP       0.00      0.00      0.00        68\n",
      "          TO       1.00      1.00      1.00       269\n",
      "          UH       0.00      0.00      0.00         0\n",
      "          VB       0.60      0.49      0.54       507\n",
      "         VBD       0.84      0.68      0.75       407\n",
      "         VBG       0.85      0.73      0.79       176\n",
      "         VBN       0.59      0.58      0.59       210\n",
      "         VBP       0.66      0.50      0.57       466\n",
      "         VBZ       0.89      0.79      0.84       409\n",
      "         WDT       0.94      0.41      0.57        39\n",
      "          WP       1.00      1.00      1.00        33\n",
      "         WRB       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           0.75     11896\n",
      "   macro avg       0.68      0.65      0.64     11896\n",
      "weighted avg       0.87      0.75      0.80     11896\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Speaker: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          IN       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.60      0.75       543\n",
      "         JJR       1.00      0.73      0.84        22\n",
      "         JJS       1.00      0.85      0.92        20\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PRP       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBR       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.78      0.64      0.70        22\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.61       607\n",
      "   macro avg       0.25      0.19      0.21       607\n",
      "weighted avg       0.99      0.61      0.76       607\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Speaker: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  84.5%%\n",
      "    Precision:     58.9%%\n",
      "    Recall:        86.7%%\n",
      "    F-Measure:     70.2%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Speaker classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         9\n",
      "         1.0       0.83      1.00      0.91        43\n",
      "\n",
      "    accuracy                           0.83        52\n",
      "   macro avg       0.41      0.50      0.45        52\n",
      "weighted avg       0.68      0.83      0.75        52\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  82.69%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         9\n",
      "         1.0       0.82      0.95      0.88        43\n",
      "\n",
      "    accuracy                           0.79        52\n",
      "   macro avg       0.41      0.48      0.44        52\n",
      "weighted avg       0.68      0.79      0.73        52\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.85%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Canon Powershot SD500 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Canon Powershot SD500: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      0.99      1.00       172\n",
      "          CD       0.90      1.00      0.95        19\n",
      "          DT       0.98      0.99      0.99       461\n",
      "          EX       0.83      1.00      0.91         5\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          IN       0.90      0.98      0.94       461\n",
      "          JJ       0.89      0.67      0.76       461\n",
      "         JJR       0.65      1.00      0.79        24\n",
      "         JJS       1.00      0.75      0.86        12\n",
      "          MD       1.00      1.00      1.00        83\n",
      "          NN       0.89      0.63      0.74       915\n",
      "         NNP       0.00      0.00      0.00         4\n",
      "         NNS       0.92      0.65      0.76       255\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00         8\n",
      "         PRP       0.98      0.99      0.99       178\n",
      "        PRP$       1.00      1.00      1.00        68\n",
      "          RB       0.93      0.88      0.90       297\n",
      "         RBR       1.00      0.08      0.14        13\n",
      "         RBS       0.25      1.00      0.40         1\n",
      "          RP       0.00      0.00      0.00        24\n",
      "          TO       1.00      1.00      1.00        81\n",
      "          VB       0.59      0.61      0.60       188\n",
      "         VBD       0.79      0.78      0.78        90\n",
      "         VBG       0.88      0.78      0.83        67\n",
      "         VBN       0.68      0.58      0.63        79\n",
      "         VBP       0.80      0.59      0.68       171\n",
      "         VBZ       0.94      0.85      0.89       170\n",
      "         WDT       1.00      0.20      0.33        10\n",
      "          WP       1.00      1.00      1.00        11\n",
      "         WRB       1.00      0.96      0.98        26\n",
      "\n",
      "    accuracy                           0.79      4354\n",
      "   macro avg       0.74      0.68      0.67      4354\n",
      "weighted avg       0.89      0.79      0.83      4354\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Canon Powershot SD500: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00         0\n",
      "          JJ       0.99      0.66      0.79       201\n",
      "         JJR       1.00      1.00      1.00         8\n",
      "         JJS       1.00      0.80      0.89         5\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.71      0.62      0.67         8\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.67       222\n",
      "   macro avg       0.31      0.26      0.28       222\n",
      "weighted avg       0.98      0.67      0.80       222\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Canon Powershot SD500: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  84.7%%\n",
      "    Precision:     59.6%%\n",
      "    Recall:        86.0%%\n",
      "    F-Measure:     70.4%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Canon Powershot SD500 classsifier\n",
      "=====================================================\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.78      1.00      0.88        18\n",
      "\n",
      "    accuracy                           0.78        23\n",
      "   macro avg       0.39      0.50      0.44        23\n",
      "weighted avg       0.61      0.78      0.69        23\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.26%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.78      1.00      0.88        18\n",
      "\n",
      "    accuracy                           0.78        23\n",
      "   macro avg       0.39      0.50      0.44        23\n",
      "weighted avg       0.61      0.78      0.69        23\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.26%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Canon S100 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Canon S100: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      1.00      1.00       180\n",
      "          CD       1.00      1.00      1.00        30\n",
      "          DT       0.99      0.98      0.99       515\n",
      "          EX       1.00      1.00      1.00         9\n",
      "          FW       0.00      0.00      0.00         1\n",
      "          IN       0.91      0.98      0.94       508\n",
      "          JJ       0.92      0.63      0.75       498\n",
      "         JJR       0.73      0.92      0.81        26\n",
      "         JJS       0.83      0.67      0.74        15\n",
      "          MD       1.00      0.99      0.99        98\n",
      "          NN       0.87      0.63      0.73      1095\n",
      "         NNP       0.00      0.00      0.00         6\n",
      "         NNS       0.94      0.66      0.77       292\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00         8\n",
      "         PRP       0.92      0.99      0.96       247\n",
      "        PRP$       1.00      1.00      1.00        75\n",
      "          RB       0.93      0.88      0.90       303\n",
      "         RBR       0.00      0.00      0.00         8\n",
      "         RBS       0.33      0.33      0.33         3\n",
      "          RP       0.00      0.00      0.00        18\n",
      "          TO       1.00      1.00      1.00       104\n",
      "          VB       0.55      0.45      0.49       226\n",
      "         VBD       0.77      0.76      0.77       130\n",
      "         VBG       0.93      0.78      0.85        81\n",
      "         VBN       0.73      0.59      0.65        83\n",
      "         VBP       0.73      0.66      0.70       201\n",
      "         VBZ       0.95      0.86      0.90       188\n",
      "         WDT       1.00      0.45      0.62        22\n",
      "          WP       0.91      1.00      0.95        10\n",
      "         WRB       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           0.78      5005\n",
      "   macro avg       0.71      0.65      0.67      5005\n",
      "weighted avg       0.89      0.78      0.82      5005\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Canon S100: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          IN       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.65      0.79       242\n",
      "         JJR       1.00      0.89      0.94         9\n",
      "         JJS       1.00      0.71      0.83         7\n",
      "          NN       0.00      0.00      0.00         0\n",
      "         NNS       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "          VB       0.00      0.00      0.00         4\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.65       262\n",
      "   macro avg       0.25      0.19      0.21       262\n",
      "weighted avg       0.98      0.65      0.78       262\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Canon S100: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  83.6%%\n",
      "    Precision:     57.7%%\n",
      "    Recall:        86.5%%\n",
      "    F-Measure:     69.2%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Canon S100 classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         9\n",
      "         1.0       0.69      1.00      0.82        20\n",
      "\n",
      "    accuracy                           0.69        29\n",
      "   macro avg       0.34      0.50      0.41        29\n",
      "weighted avg       0.48      0.69      0.56        29\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  68.97%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         9\n",
      "         1.0       0.69      1.00      0.82        20\n",
      "\n",
      "    accuracy                           0.69        29\n",
      "   macro avg       0.34      0.50      0.41        29\n",
      "weighted avg       0.48      0.69      0.56        29\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  68.97%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Diaper Champ dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Diaper Champ: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      1.00      1.00       247\n",
      "          CD       0.90      1.00      0.95        36\n",
      "          DT       0.99      0.98      0.99       686\n",
      "          EX       1.00      1.00      1.00        16\n",
      "          FW       1.00      0.33      0.50         3\n",
      "          IN       0.87      0.96      0.91       625\n",
      "          JJ       0.81      0.68      0.74       435\n",
      "         JJR       0.77      1.00      0.87        37\n",
      "         JJS       1.00      0.88      0.93        16\n",
      "          MD       1.00      1.00      1.00        90\n",
      "          NN       0.80      0.48      0.60      1286\n",
      "         NNS       0.88      0.52      0.65       331\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00         3\n",
      "         PRP       0.99      1.00      0.99       402\n",
      "        PRP$       1.00      1.00      1.00        98\n",
      "          RB       0.94      0.89      0.91       515\n",
      "         RBR       0.00      0.00      0.00         9\n",
      "         RBS       0.50      1.00      0.67         1\n",
      "          RP       0.00      0.00      0.00        46\n",
      "          TO       1.00      1.00      1.00       175\n",
      "          UH       0.00      0.00      0.00         0\n",
      "          VB       0.65      0.49      0.56       358\n",
      "         VBD       0.86      0.78      0.82       199\n",
      "         VBG       0.94      0.77      0.85       115\n",
      "         VBN       0.60      0.59      0.59        92\n",
      "         VBP       0.71      0.53      0.61       246\n",
      "         VBZ       0.97      0.86      0.91       256\n",
      "         WDT       1.00      0.52      0.68        29\n",
      "          WP       1.00      1.00      1.00        17\n",
      "         WRB       1.00      1.00      1.00        36\n",
      "\n",
      "    accuracy                           0.75      6405\n",
      "   macro avg       0.75      0.69      0.70      6405\n",
      "weighted avg       0.87      0.75      0.80      6405\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Diaper Champ: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          JJ       1.00      0.70      0.82       239\n",
      "         JJR       1.00      1.00      1.00        20\n",
      "         JJS       1.00      0.91      0.95        11\n",
      "          NN       0.00      0.00      0.00         0\n",
      "         NNS       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PRP       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.67      0.50      0.57        12\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.72       282\n",
      "   macro avg       0.28      0.24      0.26       282\n",
      "weighted avg       0.99      0.72      0.83       282\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Diaper Champ: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  82.9%%\n",
      "    Precision:     53.7%%\n",
      "    Recall:        86.6%%\n",
      "    F-Measure:     66.3%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Diaper Champ classsifier\n",
      "=====================================================\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         9\n",
      "         1.0       0.72      1.00      0.84        23\n",
      "\n",
      "    accuracy                           0.72        32\n",
      "   macro avg       0.36      0.50      0.42        32\n",
      "weighted avg       0.52      0.72      0.60        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.88%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         9\n",
      "         1.0       0.72      1.00      0.84        23\n",
      "\n",
      "    accuracy                           0.72        32\n",
      "   macro avg       0.36      0.50      0.42        32\n",
      "weighted avg       0.52      0.72      0.60        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.88%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Hitachi Router dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Hitachi Router: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      0.98      0.99       203\n",
      "          CD       0.76      1.00      0.86        25\n",
      "          DT       0.99      0.98      0.98       627\n",
      "          EX       0.67      1.00      0.80         2\n",
      "          FW       0.00      0.00      0.00         1\n",
      "          IN       0.88      0.98      0.93       501\n",
      "          JJ       0.84      0.70      0.76       425\n",
      "         JJR       0.79      0.88      0.84        26\n",
      "         JJS       0.92      0.73      0.81        15\n",
      "          MD       1.00      1.00      1.00        64\n",
      "          NN       0.85      0.48      0.61      1289\n",
      "         NNP       0.00      0.00      0.00         1\n",
      "         NNS       0.87      0.51      0.64       237\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00         6\n",
      "         PRP       0.98      1.00      0.99       281\n",
      "        PRP$       0.99      1.00      0.99        70\n",
      "          RB       0.92      0.90      0.91       330\n",
      "         RBR       0.00      0.00      0.00         6\n",
      "         RBS       0.33      0.67      0.44         3\n",
      "          RP       0.00      0.00      0.00        38\n",
      "          TO       1.00      1.00      1.00       124\n",
      "          VB       0.49      0.54      0.52       246\n",
      "         VBD       0.83      0.69      0.75       151\n",
      "         VBG       0.87      0.87      0.87       101\n",
      "         VBN       0.64      0.68      0.66       112\n",
      "         VBP       0.85      0.50      0.63       248\n",
      "         VBZ       0.94      0.82      0.88       204\n",
      "         WDT       1.00      0.48      0.65        25\n",
      "          WP       1.00      1.00      1.00         8\n",
      "         WRB       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           0.74      5391\n",
      "   macro avg       0.69      0.66      0.66      5391\n",
      "weighted avg       0.87      0.74      0.78      5391\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Hitachi Router: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          IN       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.69      0.82       229\n",
      "         JJR       1.00      1.00      1.00        13\n",
      "         JJS       1.00      0.67      0.80         9\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PRP       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.67      1.00      0.80         4\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.71       255\n",
      "   macro avg       0.31      0.28      0.28       255\n",
      "weighted avg       0.99      0.71      0.83       255\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Hitachi Router: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  85.1%%\n",
      "    Precision:     60.5%%\n",
      "    Recall:        87.2%%\n",
      "    F-Measure:     71.4%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Hitachi Router classsifier\n",
      "=====================================================\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 200}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        12\n",
      "         1.0       0.61      1.00      0.76        19\n",
      "\n",
      "    accuracy                           0.61        31\n",
      "   macro avg       0.31      0.50      0.38        31\n",
      "weighted avg       0.38      0.61      0.47        31\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  61.29%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        12\n",
      "         1.0       0.61      1.00      0.76        19\n",
      "\n",
      "    accuracy                           0.61        31\n",
      "   macro avg       0.31      0.50      0.38        31\n",
      "weighted avg       0.38      0.61      0.47        31\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  61.29%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Ipod dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Ipod: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      0.99      0.99       380\n",
      "          CD       0.89      0.97      0.93        76\n",
      "          DT       0.99      0.98      0.99      1115\n",
      "          EX       0.87      1.00      0.93        20\n",
      "          FW       0.17      0.33      0.22         3\n",
      "          IN       0.89      0.97      0.93      1096\n",
      "          JJ       0.88      0.63      0.74       829\n",
      "         JJR       0.65      0.85      0.74        46\n",
      "         JJS       0.96      0.64      0.77        36\n",
      "          MD       1.00      1.00      1.00       150\n",
      "          NN       0.87      0.51      0.64      2040\n",
      "         NNS       0.94      0.51      0.66       565\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00        10\n",
      "         PRP       0.99      0.99      0.99       521\n",
      "        PRP$       1.00      1.00      1.00       153\n",
      "          RB       0.93      0.87      0.90       749\n",
      "         RBR       0.40      0.07      0.12        27\n",
      "         RBS       0.45      0.90      0.60        10\n",
      "          RP       0.00      0.00      0.00        73\n",
      "          TO       1.00      1.00      1.00       276\n",
      "          VB       0.63      0.56      0.59       495\n",
      "         VBD       0.78      0.74      0.76       212\n",
      "         VBG       0.95      0.74      0.84       141\n",
      "         VBN       0.70      0.59      0.64       175\n",
      "         VBP       0.71      0.58      0.64       366\n",
      "         VBZ       0.96      0.91      0.93       425\n",
      "         WDT       1.00      0.54      0.70        57\n",
      "          WP       0.88      1.00      0.94        15\n",
      "         WRB       1.00      1.00      1.00        48\n",
      "\n",
      "    accuracy                           0.76     10109\n",
      "   macro avg       0.75      0.70      0.71     10109\n",
      "weighted avg       0.89      0.76      0.80     10109\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Ipod: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          IN       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.63      0.77       408\n",
      "         JJR       1.00      0.83      0.90        23\n",
      "         JJS       1.00      0.63      0.77        19\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBR       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.57      0.67      0.62        12\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.64       462\n",
      "   macro avg       0.27      0.21      0.24       462\n",
      "weighted avg       0.99      0.64      0.78       462\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Ipod: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  83.6%%\n",
      "    Precision:     56.6%%\n",
      "    Recall:        86.3%%\n",
      "    F-Measure:     68.4%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Ipod classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 400}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.11      0.17         9\n",
      "         1.0       0.68      0.89      0.77        19\n",
      "\n",
      "    accuracy                           0.64        28\n",
      "   macro avg       0.51      0.50      0.47        28\n",
      "weighted avg       0.57      0.64      0.58        28\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  64.29%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.11      0.17         9\n",
      "         1.0       0.68      0.89      0.77        19\n",
      "\n",
      "    accuracy                           0.64        28\n",
      "   macro avg       0.51      0.50      0.47        28\n",
      "weighted avg       0.57      0.64      0.58        28\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  64.29%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Linksys Router dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Linksys Router: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      1.00      1.00       328\n",
      "          CD       0.89      1.00      0.94        39\n",
      "          DT       0.99      0.98      0.98      1071\n",
      "          EX       0.74      1.00      0.85        17\n",
      "          FW       0.10      0.33      0.15         3\n",
      "          IN       0.86      0.98      0.91       962\n",
      "          JJ       0.87      0.60      0.71       738\n",
      "         JJR       0.83      0.91      0.86        53\n",
      "         JJS       0.96      0.63      0.76        35\n",
      "          MD       1.00      0.99      1.00       134\n",
      "          NN       0.87      0.47      0.61      2252\n",
      "         NNP       0.00      0.00      0.00         3\n",
      "         NNS       0.89      0.68      0.77       416\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00         7\n",
      "         PRP       0.99      0.99      0.99       486\n",
      "        PRP$       1.00      1.00      1.00       173\n",
      "          RB       0.94      0.84      0.89       666\n",
      "         RBR       0.67      0.20      0.31        10\n",
      "         RBS       0.36      0.71      0.48         7\n",
      "          RP       0.00      0.00      0.00        70\n",
      "          TO       1.00      1.00      1.00       321\n",
      "          UH       0.00      0.00      0.00         0\n",
      "          VB       0.66      0.56      0.61       555\n",
      "         VBD       0.87      0.77      0.82       336\n",
      "         VBG       0.95      0.79      0.86       195\n",
      "         VBN       0.53      0.68      0.59       149\n",
      "         VBP       0.72      0.59      0.65       334\n",
      "         VBZ       0.97      0.83      0.90       315\n",
      "         WDT       1.00      0.37      0.54        57\n",
      "          WP       0.98      1.00      0.99        47\n",
      "         WRB       1.00      0.98      0.99        42\n",
      "\n",
      "    accuracy                           0.74      9821\n",
      "   macro avg       0.71      0.65      0.66      9821\n",
      "weighted avg       0.88      0.74      0.79      9821\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Linksys Router: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00         0\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          IN       0.00      0.00      0.00         0\n",
      "          JJ       0.99      0.59      0.74       387\n",
      "         JJR       1.00      0.88      0.94        25\n",
      "         JJS       1.00      0.70      0.82        23\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.62      0.90      0.73        20\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBZ       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.62       455\n",
      "   macro avg       0.26      0.22      0.23       455\n",
      "weighted avg       0.98      0.62      0.75       455\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Linksys Router: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  84.2%%\n",
      "    Precision:     57.2%%\n",
      "    Recall:        85.7%%\n",
      "    F-Measure:     68.6%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Linksys Router classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 200}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         4\n",
      "         1.0       0.87      1.00      0.93        27\n",
      "\n",
      "    accuracy                           0.87        31\n",
      "   macro avg       0.44      0.50      0.47        31\n",
      "weighted avg       0.76      0.87      0.81        31\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  87.10%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.25      0.40         4\n",
      "         1.0       0.90      1.00      0.95        27\n",
      "\n",
      "    accuracy                           0.90        31\n",
      "   macro avg       0.95      0.62      0.67        31\n",
      "weighted avg       0.91      0.90      0.88        31\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  90.32%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Micro MP3 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Micro MP3: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.99      0.99      0.99       653\n",
      "          CD       0.93      0.96      0.94       114\n",
      "          DT       0.98      0.98      0.98      1914\n",
      "          EX       0.88      0.97      0.92        29\n",
      "          FW       0.25      1.00      0.40         3\n",
      "          IN       0.88      0.97      0.93      1847\n",
      "          JJ       0.87      0.64      0.74      1578\n",
      "         JJR       0.71      0.94      0.81       102\n",
      "         JJS       0.97      0.65      0.78        60\n",
      "          MD       1.00      1.00      1.00       241\n",
      "          NN       0.85      0.52      0.65      4097\n",
      "         NNP       0.00      0.00      0.00        17\n",
      "         NNS       0.91      0.55      0.69      1006\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00        30\n",
      "         PRP       0.99      1.00      0.99       893\n",
      "        PRP$       1.00      1.00      1.00       258\n",
      "          RB       0.94      0.86      0.90      1222\n",
      "         RBR       1.00      0.02      0.05        42\n",
      "         RBS       0.29      1.00      0.44         8\n",
      "          RP       0.00      0.00      0.00        99\n",
      "          TO       1.00      1.00      1.00       530\n",
      "          UH       0.50      1.00      0.67         2\n",
      "          VB       0.57      0.51      0.54       886\n",
      "         VBD       0.82      0.73      0.77       452\n",
      "         VBG       0.94      0.67      0.78       362\n",
      "         VBN       0.70      0.65      0.67       337\n",
      "         VBP       0.75      0.60      0.66       721\n",
      "         VBZ       0.95      0.85      0.90       695\n",
      "         WDT       0.98      0.38      0.55       104\n",
      "          WP       0.96      1.00      0.98        45\n",
      "         WRB       0.99      0.99      0.99        80\n",
      "\n",
      "    accuracy                           0.74     18427\n",
      "   macro avg       0.74      0.70      0.68     18427\n",
      "weighted avg       0.88      0.74      0.79     18427\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Micro MP3: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          JJ       1.00      0.64      0.78       733\n",
      "         JJR       1.00      0.98      0.99        49\n",
      "         JJS       1.00      0.74      0.85        27\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PRP       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.59      0.68      0.63        25\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.67       834\n",
      "   macro avg       0.30      0.25      0.27       834\n",
      "weighted avg       0.99      0.67      0.79       834\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Micro MP3: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  84.6%%\n",
      "    Precision:     58.8%%\n",
      "    Recall:        86.9%%\n",
      "    F-Measure:     70.2%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Micro MP3 classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.12      0.22        24\n",
      "         1.0       0.73      1.00      0.84        57\n",
      "\n",
      "    accuracy                           0.74        81\n",
      "   macro avg       0.87      0.56      0.53        81\n",
      "weighted avg       0.81      0.74      0.66        81\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  74.07%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.29      0.42        24\n",
      "         1.0       0.76      0.96      0.85        57\n",
      "\n",
      "    accuracy                           0.77        81\n",
      "   macro avg       0.77      0.63      0.64        81\n",
      "weighted avg       0.77      0.77      0.73        81\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  76.54%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Nokia 6600 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Nokia 6600: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.99      0.99      0.99       345\n",
      "          CD       0.87      0.98      0.92        53\n",
      "          DT       0.99      0.96      0.97      1062\n",
      "          EX       0.82      1.00      0.90        14\n",
      "          FW       0.12      0.10      0.11        10\n",
      "          IN       0.91      0.97      0.94       992\n",
      "          JJ       0.87      0.64      0.74       812\n",
      "         JJR       0.77      0.82      0.80        45\n",
      "         JJS       1.00      0.60      0.75        43\n",
      "          MD       1.00      1.00      1.00       153\n",
      "          NN       0.87      0.58      0.69      2218\n",
      "         NNP       0.00      0.00      0.00         2\n",
      "         NNS       0.91      0.64      0.75       555\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00        13\n",
      "         PRP       0.96      1.00      0.98       460\n",
      "        PRP$       1.00      1.00      1.00       119\n",
      "          RB       0.93      0.86      0.90       595\n",
      "         RBR       0.00      0.00      0.00        12\n",
      "         RBS       0.44      1.00      0.61        11\n",
      "          RP       0.00      0.00      0.00        37\n",
      "          TO       1.00      1.00      1.00       215\n",
      "          UH       0.50      0.67      0.57         3\n",
      "          VB       0.59      0.50      0.54       442\n",
      "         VBD       0.78      0.73      0.75       168\n",
      "         VBG       0.87      0.66      0.75       161\n",
      "         VBN       0.78      0.66      0.71       165\n",
      "         VBP       0.74      0.57      0.64       393\n",
      "         VBZ       0.92      0.87      0.90       390\n",
      "         WDT       1.00      0.51      0.68        39\n",
      "          WP       1.00      1.00      1.00        16\n",
      "         WRB       0.94      1.00      0.97        33\n",
      "\n",
      "    accuracy                           0.76      9576\n",
      "   macro avg       0.71      0.67      0.67      9576\n",
      "weighted avg       0.88      0.76      0.81      9576\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Nokia 6600: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.00      0.00      0.00         0\n",
      "          CD       0.00      0.00      0.00         0\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          IN       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.62      0.77       417\n",
      "         JJR       1.00      0.74      0.85        19\n",
      "         JJS       1.00      0.67      0.80        24\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PRP       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.52      0.79      0.63        14\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBG       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.64       474\n",
      "   macro avg       0.21      0.17      0.18       474\n",
      "weighted avg       0.99      0.64      0.77       474\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Nokia 6600: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  83.9%%\n",
      "    Precision:     58.2%%\n",
      "    Recall:        85.9%%\n",
      "    F-Measure:     69.4%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Nokia 6600 classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.17      0.29        12\n",
      "         1.0       0.83      1.00      0.91        48\n",
      "\n",
      "    accuracy                           0.83        60\n",
      "   macro avg       0.91      0.58      0.60        60\n",
      "weighted avg       0.86      0.83      0.78        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  83.33%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.25      0.38        12\n",
      "         1.0       0.84      0.98      0.90        48\n",
      "\n",
      "    accuracy                           0.83        60\n",
      "   macro avg       0.79      0.61      0.64        60\n",
      "weighted avg       0.82      0.83      0.80        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  83.33%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Norton dataset\n",
      "============================================================================================================\n",
      "\n",
      "Evaluation metrics for Norton: Part of Speech tags\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       1.00      0.99      0.99       257\n",
      "          CD       0.92      0.94      0.93        36\n",
      "          DT       0.97      0.97      0.97       614\n",
      "          EX       0.83      1.00      0.91        10\n",
      "          FW       0.00      0.00      0.00         5\n",
      "          IN       0.87      0.96      0.92       612\n",
      "          JJ       0.86      0.49      0.63       565\n",
      "         JJR       0.54      0.93      0.68        15\n",
      "         JJS       1.00      0.73      0.85        15\n",
      "          MD       1.00      1.00      1.00        96\n",
      "          NN       0.91      0.54      0.67      1453\n",
      "         NNP       0.00      0.00      0.00         2\n",
      "         NNS       0.89      0.68      0.77       338\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00        15\n",
      "         PRP       0.97      1.00      0.99       273\n",
      "        PRP$       1.00      1.00      1.00       100\n",
      "          RB       0.92      0.86      0.89       471\n",
      "         RBR       1.00      0.14      0.25        14\n",
      "         RBS       0.43      1.00      0.60         3\n",
      "          RP       0.00      0.00      0.00        44\n",
      "          TO       1.00      1.00      1.00       162\n",
      "          VB       0.63      0.57      0.60       324\n",
      "         VBD       0.84      0.75      0.79       260\n",
      "         VBG       0.95      0.72      0.82       138\n",
      "         VBN       0.61      0.61      0.61       147\n",
      "         VBP       0.74      0.64      0.69       228\n",
      "         VBZ       0.98      0.81      0.89       233\n",
      "         WDT       1.00      0.40      0.57        35\n",
      "          WP       1.00      0.95      0.98        21\n",
      "         WRB       0.97      1.00      0.99        33\n",
      "\n",
      "    accuracy                           0.74      6519\n",
      "   macro avg       0.74      0.67      0.68      6519\n",
      "weighted avg       0.88      0.74      0.79      6519\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Evaluation metrics for Norton: Adjectives phrases\n",
      "====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          JJ       1.00      0.47      0.64       291\n",
      "         JJR       1.00      1.00      1.00        10\n",
      "         JJS       1.00      0.78      0.88         9\n",
      "          NN       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "         PRP       0.00      0.00      0.00         0\n",
      "          RB       0.00      0.00      0.00         0\n",
      "         RBS       0.00      0.00      0.00         0\n",
      "          VB       0.44      0.67      0.53        12\n",
      "         VBD       0.00      0.00      0.00         0\n",
      "         VBG       0.00      0.00      0.00         0\n",
      "         VBN       0.00      0.00      0.00         0\n",
      "         VBP       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.50       322\n",
      "   macro avg       0.25      0.21      0.22       322\n",
      "weighted avg       0.98      0.50      0.65       322\n",
      " \n",
      "\n",
      "\n",
      "Evaluation metrics for Norton: Chunked sentences\n",
      "=====================================================\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  85.2%%\n",
      "    Precision:     61.4%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     71.9%%\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Best Parameters for Norton classsifier\n",
      "=====================================================\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Report of the baseline Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      1.00      0.77        22\n",
      "         1.0       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.63        35\n",
      "   macro avg       0.31      0.50      0.39        35\n",
      "weighted avg       0.40      0.63      0.49        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  62.86%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Report of the Optimised Random Forest Classifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      1.00      0.77        22\n",
      "         1.0       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.63        35\n",
      "   macro avg       0.31      0.50      0.39        35\n",
      "weighted avg       0.40      0.63      0.49        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  62.86%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Apex AD 2600 dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "Customer service   Negative     12\n",
      "Dvd                Negative      7\n",
      "                   Positive      6\n",
      "Feature            Positive      6\n",
      "Format             Positive      6\n",
      "Play               Negative     39\n",
      "                   Positive     40\n",
      "Price              Positive      5\n",
      "Remote             Negative      8\n",
      "Support            Negative      5 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Canon G3 dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "Camera             Positive     44\n",
      "Control            Positive      7\n",
      "Design             Negative      2\n",
      "Photo              Positive      6\n",
      "Photo quality      Positive      8\n",
      "Picture            Negative      2\n",
      "                   Positive      7\n",
      "Size               Negative      2\n",
      "Software           Negative      3\n",
      "Viewfinder         Negative      6 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Creative Labs dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "Battery            Positive     20\n",
      "Case               Negative     10\n",
      "Headphone          Negative     11\n",
      "Play               Negative     16\n",
      "                   Positive     44\n",
      "Price              Positive     15\n",
      "Scroll             Negative     10\n",
      "Software           Negative     24\n",
      "                   Positive     26\n",
      "Storage            Positive     13 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Nikkon Coolpix 4300 dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "Audio audio        Negative      1\n",
      "Auto mode          Positive      5\n",
      "Battery            Negative      2\n",
      "Camera             Negative      2\n",
      "                   Positive     28\n",
      "Indoor             Negative      2\n",
      "Picture            Negative      3\n",
      "                   Positive      6\n",
      "Picture delay      Positive      9\n",
      "Size               Positive      6 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Nokia 6610 dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "Battery            Positive      9\n",
      "Button             Negative      3\n",
      "Internet           Negative      3\n",
      "Phone              Positive     26\n",
      "Radio              Positive      8\n",
      "Size               Positive     12\n",
      "Speakerphone       Positive      9\n",
      "Vibrate            Negative      3\n",
      "Voice              Negative      5\n",
      "Volume             Negative      4 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Computer dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "Color              Positive      7\n",
      "Color price        Positive      8\n",
      "Display            Positive      7\n",
      "Instructions       Negative      2\n",
      "Monitor            Negative      5\n",
      "                   Positive     16\n",
      "Price              Negative      2\n",
      "Product            Negative      2\n",
      "                   Positive      5\n",
      "The backlight      Negative      2 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Router dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "                   Negative      5\n",
      "                   Positive      8\n",
      "Product            Positive      5\n",
      "Router             Negative     10\n",
      "                   Positive     14\n",
      "Router connection  Negative      5\n",
      "Setup              Positive      5\n",
      "Speed              Negative      4\n",
      "Support            Negative      4\n",
      "                   Positive      8 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Speaker dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "Bit                Negative      2\n",
      "Computer speaker   Positive      7\n",
      "Sound              Negative      4\n",
      "                   Positive     25\n",
      "Sound quality      Negative      2\n",
      "                   Positive      7\n",
      "Speaker            Negative      4\n",
      "                   Positive     37\n",
      "Volum              Negative      3\n",
      "Work               Positive      6 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Canon Powershot SD500 dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "                   Positive      4\n",
      "Camera             Positive     20\n",
      "Lcd                Negative      4\n",
      "Manual control     Negative      2\n",
      "Pics               Positive      3\n",
      "Picture            Positive     10\n",
      "Power              Positive      3\n",
      "Power button       Negative      1\n",
      "Pricey             Negative      1\n",
      "Settings           Negative      1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Canon S100 dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "Battery            Negative      3\n",
      "Camera             Negative      3\n",
      "                   Positive     16\n",
      "Memory             Negative      2\n",
      "Panorama           Positive      3\n",
      "Picture            Negative      4\n",
      "                   Positive      7\n",
      "Size               Positive      9\n",
      "Software           Positive      3\n",
      "Zoom               Negative      4 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Diaper Champ dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "Champ              Positive     10\n",
      "Diaper             Negative      5\n",
      "                   Positive     10\n",
      "Diaper champ       Negative      4\n",
      "                   Positive     18\n",
      "Diaper genie       Negative      2\n",
      "Item               Negative      2\n",
      "Oder               Negative      3\n",
      "Product            Positive      7\n",
      "Work               Positive      9 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Hitachi Router dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "                   Positive      3\n",
      "Dust ment          Negative      8\n",
      "                   Positive      5\n",
      "Money              Positive      4\n",
      "Plunge             Negative      5\n",
      "Price              Positive      6\n",
      "Router             Negative      4\n",
      "                   Positive     17\n",
      "Shaft              Negative      4\n",
      "Spring             Negative      4 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Ipod dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "Any complaints     Negative      2\n",
      "Battery            Negative      7\n",
      "                   Positive      3\n",
      "Design             Positive      3\n",
      "Interface          Negative      2\n",
      "Ipod               Positive      5\n",
      "Scratch            Negative      4\n",
      "Sound              Positive      5\n",
      "Tuner              Negative      3\n",
      "                   Positive      6 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Linksys Router dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "                   Negative      4\n",
      "Configuration      Negative      2\n",
      "Connect            Negative      4\n",
      "Documentation      Negative      2\n",
      "Instruction        Negative      3\n",
      "Product            Positive      5\n",
      "Router             Positive     22\n",
      "Setup              Positive      6\n",
      "Speed              Positive      5\n",
      "Works              Positive      6 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Micro MP3 dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "Battery            Negative      7\n",
      "                   Positive     10\n",
      "Features           Positive     11\n",
      "Headphones         Negative      7\n",
      "Look               Positive     11\n",
      "Play               Negative      9\n",
      "                   Positive     33\n",
      "Reception          Negative      8\n",
      "Sound              Positive     10\n",
      "Touch              Negative      7 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Nokia 6600 dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "                   Positive      5\n",
      "Battery            Negative      9\n",
      "Bluetooth          Positive      5\n",
      "Feature            Positive      7\n",
      "Game               Positive      5\n",
      "Nokia              Negative      3\n",
      "Phone              Negative     13\n",
      "                   Positive     40\n",
      "Screen             Negative      3\n",
      "Voice              Negative      3 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ============================================================================================================\n",
      "                                  Top Features extracted from Norton dataset\n",
      "============================================================================================================ \n",
      "\n",
      "                            Review\n",
      "                             Count\n",
      "Extracted Features Reviews        \n",
      "                   Positive      3\n",
      "Antivirus          Positive      5\n",
      "Corporate edition  Positive      3\n",
      "Install            Negative     14\n",
      "Norton             Negative      7\n",
      "Performance        Positive      3\n",
      "Product            Negative     11\n",
      "                   Positive      3\n",
      "Support            Negative      9\n",
      "Update             Negative      5 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_one = review_similarity_count(evaluate(\n",
    "         models_training(\n",
    "         split_data_apply_tfidf(\n",
    "         feature_extraction(\n",
    "         data_preprocessing(\n",
    "         load_data(your_path_to_data)))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea66edf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67acf52a",
   "metadata": {},
   "source": [
    "# Pipeline 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76155648",
   "metadata": {},
   "source": [
    "The first two steps of pipeline two, data parsing and pre-processing, are shared with pipeline one.This is because the data ingestion and cleaning phase are carried on in the same way. <br>\n",
    "The two pipelines start branching off from step three: features extraction. The following is an holistic view of pipiline's two main features and design characteristics: \n",
    "<br><br>\n",
    "* Spacy `noun_chunks` is used to extract noun phrases. This method is used because our research has shown a richness of quality results in tends to be returned. This approach was taken in the attempt to obtain a larger noun sample size than using NLTK\n",
    "* Dataframes with missing reviews are kept to extract valuable product features\n",
    "* On each dataset five machine learning models are trained both as standard baseline and also through an optimised hyperparameters search.The following are the five models:<br> `Naive Bayes Classifier`,`Support Vector Machine Classifier`,`Random Forest Classifier`,`Logistic Regression` and `Stocastic Gradient Decent Classifier`\n",
    "<br>\n",
    "This results in five + five (baseline + optimised) models being trained on each of the seventeen datasets for a total of one hundred and seventy models\n",
    "* The best out of sample performing model on each dataset is used to predict the sentiment of sentences with missing reviews\n",
    "* Extracted features with reviews are combined with features thathad no reviews and now have predicted reviews. Features evaluation summaries with reviews count split by product are printed out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d174029",
   "metadata": {},
   "source": [
    "# Pipeline 2 step 3: Product feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3128461a",
   "metadata": {},
   "source": [
    "Feature extraction step of pipeline two uses Spacy `noun_chunks` method to extract Noun chunks. These are “base noun phrases” that have a noun as their head and can be thought as a noun plus the words describing the noun. \n",
    "<br>\n",
    "\n",
    "As highlighted in the section above the reason behind using this method is to collect a semantically rich noun set to present a denser feature review output table. \n",
    "<br>\n",
    "\n",
    "The other key difference of this step is that copies of dataframes, before NaN rows get dropped, are stored under `dataframe_full` variable to be consumed by downstream functions. \n",
    "<br>\n",
    "\n",
    "The remaining procedures: stripping out reviews, splitting datasets for test and training as well as vectorisation are carried on in the same way as pipeline 1. Please refer to that section for a more detailed explaination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b86a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_two(all_datasets_pre):    \n",
    "        \n",
    "    all_datasets_extract = {}\n",
    "    \n",
    "    for idx, review_dataset in enumerate(all_datasets_pre):\n",
    "        \n",
    "        dataframe = review_dataset.copy()\n",
    "\n",
    "        # Apply Spacy chunk parser \n",
    "        dataframe['Spacy_Chunk'] = dataframe['Str_PoS_tokens'].apply(lambda x: [chunk for chunk in nlp(x).noun_chunks])    \n",
    "\n",
    "        # Store negative reviews\n",
    "        negative_reviews = [\"-1\",\"-2\",\"-3\"]\n",
    "\n",
    "        # Extract reviews\n",
    "        def split_review(raw_review):\n",
    "\n",
    "            # Regex to get [x] from text\n",
    "            review = re.findall(r\"\\[\\s*\\+?(-?\\d+)\\s*\\]\", raw_review)\n",
    "\n",
    "            if len(review) == 0: # Missing review\n",
    "                return None\n",
    "            elif review[0] in negative_reviews:\n",
    "                return 0\n",
    "            return 1\n",
    "\n",
    "        # Apply review extraction\n",
    "        dataframe['Review'] = dataframe['Raw_text'].apply(lambda x: split_review(x))\n",
    "        \n",
    "        # NOTE: The difference with pipeline 1 at this stage is that a copy of dataframe \n",
    "        # with NaN rows (mostly missing reviews) is kept to extract valuable features\n",
    "        # and forecast their opinion tendency in subsequent steps \n",
    "\n",
    "        # Keep copy of dataframe for predicting missing reviews and retain more features\n",
    "        dataframe_full = dataframe.copy()\n",
    "        \n",
    "        # Drop nan values for training\n",
    "        dataframe = dataframe.dropna() \n",
    "        \n",
    "        # Append dataframes to dict of lists\n",
    "        all_datasets_extract[idx] = []\n",
    "        all_datasets_extract[idx].extend([dataframe,dataframe_full])\n",
    "    \n",
    "    return all_datasets_extract\n",
    "\n",
    "\n",
    "def split_data_apply_tfidf_two(all_datasets_extract):\n",
    "         \n",
    "    # Use also to vectorize nan reviews\n",
    "    global vectorizer_list\n",
    "    vectorizer_list = []\n",
    "    all_datasets_split = {}\n",
    "    \n",
    "    for key, review_dataset in all_datasets_extract.items():\n",
    "        \n",
    "        # Unpack \n",
    "        dataframe = review_dataset[0]\n",
    "        dataframe_full = review_dataset[1]\n",
    "        \n",
    "        # To pandas dataframe\n",
    "        dataframe = pd.DataFrame(dataframe)       \n",
    "        \n",
    "        # Dataset\n",
    "        X = dataframe.iloc[:,:-1]\n",
    "\n",
    "        # Labels\n",
    "        y = dataframe.iloc[:,-1:]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "        \n",
    "        # Apply tfidf vectorizer\n",
    "        vectorizer = TfidfVectorizer(stop_words='english',ngram_range=(1, 2))\n",
    "\n",
    "        \n",
    "        def vectorized_reviews(vectorizer, X_train, X_test):\n",
    "            X_train = vectorizer.fit_transform(X_train[chosen_feature[feature_num]])\n",
    "            \n",
    "            # Append fit transform state for nan review\n",
    "            vectorizer_list.append(vectorizer)\n",
    "            \n",
    "            X_test = vectorizer.transform(X_test[chosen_feature[feature_num]])\n",
    "\n",
    "            return X_train, X_test\n",
    "\n",
    "        X_train, X_test = vectorized_reviews(vectorizer, X_train, X_test)\n",
    "    \n",
    "        all_datasets_split[key] = []\n",
    "        all_datasets_split[key].extend([X_train, X_test, y_train, y_test, dataframe_full])\n",
    "    \n",
    "    return all_datasets_split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1c191",
   "metadata": {},
   "source": [
    "# Pipeline 2 step 4: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58163d",
   "metadata": {},
   "source": [
    "Function `models_training_two` as the name suggets, trains various machine learning models to compare their performance across datasets.\n",
    "<br>\n",
    "\n",
    "The following are the classifers trained: `Naive Bayes Classifier`,`Support Vector Machine Classifier`,`Random Forest Classifier`,`Logistic Regression` and `Stocastic Gradient Decent Classifier`. \n",
    "<br>\n",
    "\n",
    "I've chosen these models because they are known to be good performers and was interested in expanding my knowledge in this area. LSTM + GloVe as well as neural networks could have been another interesting approach but I've researched and applied them on previous assignments.\n",
    "<br>\n",
    "\n",
    "On each of the above models, a base version is fitted to get a minimum threshold measure of performance. From there an hyperparametrised version using sklearn `GridSearchCV` is trained for performance improvements. \n",
    "<br>\n",
    "\n",
    "Note: due to the number of hyperparametrised models trained (170) the search space has been constrained to a reasonable configuration. Commented out parameter settings can be found on each grid for performence boost. This choice was made to let the user experience the output in a reasonable time-manner. Using current settings, the whole pipeline takes about 9 minutes to complete on a 8GB MacBook Pro Apple M1 chip.\n",
    "<br>\n",
    "\n",
    "The following section will go through the innerworkings of each model. \n",
    "<br>\n",
    "<br>\n",
    "`MultinominalNB`:\n",
    "<br>\n",
    "\n",
    "Multimodal Naive Bayes is a specialized version of Naive Bayes designed to handle text documents using word counts as it's underlying method of calculating probability. Below is the Bayes equation: \n",
    "\n",
    "\\begin{equation*}\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\\end{equation*}\n",
    "\n",
    "This is a probabilistic equation that infers a probability from previous data where: \n",
    "\n",
    "\\begin{equation*}\n",
    "P(A|B)\n",
    "\\end{equation*}\n",
    "<br>\n",
    "Is the probability of `A` given `B`, also called Posterior probability.\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "P(B|A)\n",
    "\\end{equation*}\n",
    "<br>\n",
    "\n",
    "Is the probability of `B` given `A`, also called likelihood or Conditional probability.\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "P(A)\n",
    "\\end{equation*}\n",
    "<br>\n",
    "\n",
    "Is the probability of `A`, called Prior or Class probability.\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "P(B)\n",
    "\\end{equation*}\n",
    "<br>\n",
    "\n",
    "Is the probability of `B` or Evidence.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The `MultinominalNB` hyperparameter search space has the following parameters [6]:\n",
    "<br>\n",
    " \n",
    "* `alpha`: float, default=1.0\n",
    "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "* `fit_prior`: bool, default=True\n",
    "Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "`Support Vector Machine Classifier`:\n",
    "<br>\n",
    "\n",
    "The objective of support vector machine is to find a hyperplane in an N-dimensional space that distinctly classifies data points. Several different hyperplanes could be chosen to separate the two classes of data point. The objective is to find a plane that maximises the distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
    "<br>\n",
    "\n",
    "Also, thanks to the so called ‘kernel trick’,  SVMs can efficiently perform a non-linear classification by implicitly mapping their inputs into high-dimensional feature spaces.\n",
    "\n",
    "These are the parameters used in the optimisation phase [7]:\n",
    "<br>\n",
    "\n",
    "* `C`: float, default=1.0\n",
    "Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared `l2` penalty.\n",
    "* `kernel`: {`linear`, `poly`, `rbf`, `sigmoid`, `precomputed`}, default=`rbf`\n",
    "Specifies the kernel type to be used in the algorithm. It must be one of `linear`, `poly`, `rbf`, `sigmoid`, `precomputed` or a callable. If none is given, `rbf` will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples).\n",
    "* `gamma`{`scale`, `auto`} or float, default=`scale`\n",
    "Kernel coefficient for `rbf`, `poly` and `sigmoid`.\n",
    "if gamma=`scale` (default) is passed then it uses `1 / (n_features * X.var())` as value of gamma,\n",
    "if `auto`, uses `1 / n_features`\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "`Logistic Regression`:\n",
    "<br>\n",
    "\n",
    "Contrary to what the name might suggest, logistic regression is a classification method to predict binary outcomes. This method  is used when the dependent variable (target) is categorical.\n",
    "<br>\n",
    "Since the outcome is a probability, the dependent variable is bounded between 0 and 1 and therefore a logit transformation is applied on the odds. In practice this means that the probability of success is divided by the probability of failure, also commonly known as the log odds. The following Is the logistic regression formulas:\n",
    "\n",
    " \n",
    "\\begin{equation*}\n",
    "Logit(pi) = 1/(1+ exp(-pi))\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "ln(pi/(1-pi)) = Beta_0 + Beta_1*X_1 + … + B_k*K_k\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "Where `logit(pi)` is the dependent variable and `x` is the independent variable.\n",
    "<br>\n",
    "\n",
    "Maximum likelihood estimation (MLE) is used to find the beta parameter via multiple iterations to optimize for the best fit of log odds. This produces the log likelihood function and logistic regression seeks to maximize it to find the best parameter estimate\n",
    "<br>\n",
    "\n",
    "The following are the solver parameters used [8]:\n",
    "<br>\n",
    "\n",
    "* `newton-cg`: Uses an Hessian matrix. It's slow for large datasets, because it computes the second derivatives.\n",
    "\n",
    "* `lbfgs`: Approximates the second derivative matrix updates with gradient evaluations. It stores only the last few updates therefore saving memory\n",
    "\n",
    "* `liblinear`: Uses a coordinate descent algorithm which tries to minimise a multivariate function by solving univariate optimization problems in a loop\n",
    "\n",
    "* `sag`: Stochastic Average Gradient descent uses a random sample of previous gradient values and works well on  large datasets\n",
    "\n",
    "* `saga`: Extension of sag that also allows for L1 regularization. Should generally train faster than sag\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "`Stocastic Gradient Decent`:\n",
    "\n",
    "<br>\n",
    "\n",
    "Stocastic Gradient Decent (SGD) Classifier is a linear classifier (SVM, logistic regression, a.o.) optimized by the SGD. Gradient descent is used to minimize a cost function towards reaching global minima.\n",
    "<br>\n",
    "\n",
    "Computes the gradient using a single sample and allows for minibatch learning. Therefore, SGD works well on large scale problems. Another advantage of using SGD is that the cost function in Logistic Regression cannot be calculated directly, so we try to minimize it via Stochastic Gradient Descent, also known as Online Gradient Descent. The goal is to descend along the cost function towards its minimum for each new training observation.\n",
    "\n",
    "These are the parameters used [9]:\n",
    "\n",
    "* `loss`: str, default=`hinge`\n",
    "The loss function to be used. Defaults to `hinge`, which gives a linear SVM.\n",
    "The possible options are `hinge`, `log`, `modified_huber`, `squared_hinge`, `perceptron`, or a regression loss: `squared_error`, `huber`, `epsilon_insensitive`, or `squared_epsilon_insensitive`\n",
    "The `log` loss gives logistic regression, a probabilistic classifier. `modified_huber` is another smooth loss that brings tolerance to outliers as well as probability estimates. `squared_hinge` is like hinge but is quadratically penalized. `perceptron` is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description\n",
    "\n",
    "* `penalty`: {`l2`, `l1`, `elasticnet`}, default=`l2`\n",
    "The penalty (aka regularization term) to be used. Defaults to `l2` which is the standard regularizer for linear SVM models. `l1` and `elasticnet` might bring sparsity to the model (feature selection) not achievable with `l2`\n",
    "\n",
    "* `alpha`: float, default=0.0001\n",
    "Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when set to learning_rate is set to `optimal`\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Please refer to step 4 of pipeline 2 for an explaination of `Random Forest Classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cdb14ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def models_training_two(all_datasets_split):\n",
    "        \n",
    "    # To store models created\n",
    "    best_models = {}\n",
    "    \n",
    "    for key, review_dataset in all_datasets_split.items():\n",
    "                \n",
    "        # Unpack\n",
    "        X_train = review_dataset[0]\n",
    "        X_test = review_dataset[1]\n",
    "        y_train = np.ravel(review_dataset[2])\n",
    "        y_test = np.ravel(review_dataset[3])\n",
    "        dataframe_full = review_dataset[4]\n",
    "        \n",
    "        # Model 1 of 5: Multinomial Naive Bayes     \n",
    "        \n",
    "        bayes_est_base = MultinomialNB()\n",
    "        \n",
    "        # Train MultinomialNB baseline model for comparison\n",
    "        bayes_baseline = bayes_est_base.fit(X_train, y_train)\n",
    "        bayes_baseline_pred = bayes_baseline.predict(X_test)\n",
    "        \n",
    "        bayes_hyperparams = {\n",
    "                'alpha'     : [0.8, 1.0, 1.2, 1.4],\n",
    "                'fit_prior' : [True, False],\n",
    "        }\n",
    "        \n",
    "        bayes_estimator = MultinomialNB()\n",
    "        best_model_multinominal_nb = get_best_model(bayes_estimator, bayes_hyperparams, \n",
    "                                                    X_train, y_train, X_test, y_test, best_models, key,\n",
    "                                                    'MultinominalNB')\n",
    "        \n",
    "        \n",
    "        # Store best model and baseline predictions\n",
    "        best_models[key].extend([best_model_multinominal_nb.best_estimator_,bayes_baseline_pred])\n",
    "        \n",
    "        \n",
    "        # Model 2 of 5: Support Vector Machine\n",
    "        \n",
    "        svc_est_base = SVC()\n",
    "        \n",
    "        # Train SVM Classifier baseline model for comparison\n",
    "        svc_baseline = svc_est_base.fit(X_train, y_train)\n",
    "        svc_baseline_pred = svc_baseline.predict(X_test)\n",
    "                \n",
    "        svc_hyperparams = {\n",
    "                        'C': [0.1, 1, 10, 100],\n",
    "                    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "                   'kernel': ['rbf','linear']\n",
    "        }\n",
    "\n",
    "        svc_estimator = SVC(random_state=1)\n",
    "        best_model_svc = get_best_model(svc_estimator, svc_hyperparams,\n",
    "                                        X_train, y_train, X_test, y_test, best_models, key,\n",
    "                                       'SVMClassifier')\n",
    "        \n",
    "        # Store best model and baseline predictions\n",
    "        best_models[key].extend([best_model_svc.best_estimator_,svc_baseline_pred])\n",
    "                \n",
    "        # Model 3 of 5: Random Forest Classifier\n",
    "        \n",
    "        rfc_est_base = RandomForestClassifier()\n",
    "        \n",
    "        # Train Random Forest Classifier baseline model for comparison\n",
    "        rfc_baseline = rfc_est_base.fit(X_train, y_train)\n",
    "        rfc_baseline_pred = rfc_baseline.predict(X_test)\n",
    "\n",
    "        rfc_hyperparams = {\n",
    "            'bootstrap' : [True, False],\n",
    "            'n_estimators' : list(range(100,200,100)), \n",
    "            #'n_estimators' : list(range(200,2000,100))\n",
    "            'criterion' : ['gini', 'entropy'],\n",
    "            'min_samples_leaf' : list(range(1,5,2)),\n",
    "            'max_features' : ['sqrt', 'log2']\n",
    "        }\n",
    "        \n",
    "        rfc_estimator = RandomForestClassifier(random_state=1)\n",
    "        best_model_rfc = get_best_model(rfc_estimator, rfc_hyperparams,\n",
    "                                                 X_train, y_train, X_test, y_test, best_models, key,\n",
    "                                                 'RandomForestClassifier')\n",
    "        \n",
    "        # Store best model and baseline predictions\n",
    "        best_models[key].extend([best_model_rfc.best_estimator_, rfc_baseline_pred])\n",
    "         \n",
    "            \n",
    "            \n",
    "        # Model 4 of 5: Logistic Regression\n",
    "        \n",
    "        log_est_base = LogisticRegression()\n",
    "        \n",
    "        # Train Logistic Regression baseline model for comparison\n",
    "        log_baseline = log_est_base.fit(X_train, y_train)\n",
    "        log_baseline_pred = log_baseline.predict(X_test)\n",
    "        \n",
    "        log_hyperparams = [{\n",
    "                'penalty' : ['l2'],\n",
    "                #'penalty' : ['l2', 'elasticnet'],\n",
    "                'C' : [0.01, 0.1, 1,10],\n",
    "                'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                #'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "                }\n",
    "            ]\n",
    "                \n",
    "        log_estimator = LogisticRegression(random_state=1)\n",
    "        best_model_log = get_best_model(log_estimator, log_hyperparams,\n",
    "                                                 X_train, y_train, X_test, y_test, best_models, key,\n",
    "                                                 'LogisticRegression')\n",
    "        \n",
    "        # Store best model and baseline predictions\n",
    "        best_models[key].extend([best_model_log.best_estimator_, log_baseline_pred])\n",
    "        \n",
    "        \n",
    "        # Model 5 of 5: SGD Classifier\n",
    "        sgd_est_base = SGDClassifier()\n",
    "        \n",
    "        # Train Stocastic Gradient Decent model for comparison\n",
    "        sgd_baseline = sgd_est_base.fit(X_train, y_train)\n",
    "        sgd_baseline_pred = sgd_baseline.predict(X_test)\n",
    "        \n",
    "        sgd_hyperparams = {\n",
    "                'loss'    : ['hinge', 'log'],\n",
    "                #'loss'    : ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "                'penalty' : ['l2', 'elasticnet'],\n",
    "                #'penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "                'alpha'   : [0.001, 0.01, 0.1]\n",
    "            }\n",
    "        \n",
    "        sgd_estimator = SGDClassifier(random_state=1, early_stopping=True)\n",
    "        best_model_sgd = get_best_model(sgd_estimator, sgd_hyperparams, \n",
    "                                        X_train, y_train, X_test, y_test, best_models, key,\n",
    "                                        'SGDClassifier')\n",
    "\n",
    "        # Store best model and baseline predictions\n",
    "        best_models[key].extend([best_model_sgd.best_estimator_, sgd_baseline_pred, dataframe_full])\n",
    "    \n",
    "    return best_models\n",
    "        \n",
    "\n",
    "\n",
    "def get_best_model(estimator, hyperparams, X_train, y_train, X_test, y_test, \n",
    "                   best_models, key, name, fit_params={}):\n",
    "    \n",
    "    # 10 splits 3 folds\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    \n",
    "    # Go through search space with chosen params\n",
    "    grid_search = GridSearchCV(estimator=estimator, param_grid=hyperparams, n_jobs=-1, cv=cv, scoring=\"accuracy\")\n",
    "    \n",
    "    # Fit train data\n",
    "    best_model = grid_search.fit(X_train, y_train, **fit_params)\n",
    "    \n",
    "    # Get best params\n",
    "    best_params = best_model.best_estimator_.get_params()\n",
    "    \n",
    "    best_predict = best_model.predict(X_test)\n",
    "    \n",
    "    if key not in best_models.keys():\n",
    "        best_models[key] = []\n",
    "        best_models[key].extend([name, best_params, best_predict, y_test])\n",
    "    else:\n",
    "        best_models[key].extend([name, best_params, best_predict, y_test])\n",
    "        \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b617eb6",
   "metadata": {},
   "source": [
    "# Pipeline 2 step 5: Evaluation and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725562f",
   "metadata": {},
   "source": [
    "In this final step, function `evaluate_two` processes each dataset sequentially and for every model trained on that dataset (five baseline and five hyperparametrised models) prints out evaluation reports containing the following statistics: `precision`, `recall`, `f1-score`, `accuracy`, `average` and `weighted average`. \n",
    "<br>\n",
    "\n",
    "Worth noting that some baseline models perform better than the one using `GridSreachCV` and this is because of the constrained parameters set which doesn't allow the cost function to reach local or global minima. Since the scope of this assignement is not to optimise for accuracy but rather to demonstrate an understanding of NLP I've chosen not to deploy substantial computational resourses and training time towrads it in favour of conent quality. Although as explained above, should the user whish to exploit its full potential, uncommenting the parameters on each parameter grid in step 4 will help achieve that. \n",
    "<br>\n",
    "\n",
    "In terms of accuracy, the following is the model rank averaged across datasets where, for each model, the highest score between baseline or parametrised is taken:\n",
    "* Support Vector Machine: 78.91% \n",
    "* Logistic Regression: 78.87% \n",
    "* Multimodal Naive Bayes: 78.66%\n",
    "* Stocastic Gradient Decent: 78.33%\n",
    "* Random Forest Classifier: 77.16%\n",
    "<br>\n",
    "\n",
    "The highest models average score across datasets is registered on Apex AD 2600 with an accuracy of 88.4%, whereas the lowest is on Nokia 6600 dataset with an average accuracy of 68.22%. \n",
    "<br>\n",
    "\n",
    "In terms of single model across datasets, `Stocastic Gradient Decent` gets the highest score with an accuracy of 93.02% on Diaper Champ dataset, whereas the lowest goes to `Random Forest Classifier` with a score of 57.53%. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d98dc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_two(best_models):\n",
    "    \n",
    "    all_datasets_eval = best_models.copy()\n",
    "    model_scores = {}\n",
    "\n",
    "    for idx, (key, review_dataset) in enumerate(best_models.items()):\n",
    "        \n",
    "        print('===='*27)\n",
    "        print(' '*33,f'Evaluation metrics for {tot_items[idx]} dataset')\n",
    "        print('===='*27)\n",
    "\n",
    "        # Each model has 6 variables. [:-1] because contains dataset_full (used later)\n",
    "        for idx, num in enumerate(range(0,len(review_dataset[:-1]),6)):\n",
    "            \n",
    "            name = review_dataset[0+num]\n",
    "            best_params = review_dataset[1+num]\n",
    "            best_predict = review_dataset[2+num]\n",
    "            y_test = review_dataset[3+num]\n",
    "            best_model = review_dataset[4+num]\n",
    "            baseline_pred = review_dataset[5+num]\n",
    "\n",
    "            print('\\n',' '*12,f'MODEL {1+idx}: {name}')\n",
    "            print(f'\\nBest Parameters for {name} classsifier')\n",
    "            print('='*53)\n",
    "            print(best_params)\n",
    "            print('='*53)\n",
    "\n",
    "            report = classification_report(y_test, best_predict)\n",
    "            baseline_report = classification_report(y_test, baseline_pred)\n",
    "            \n",
    "            score = accuracy_score(y_true=y_test, y_pred=best_predict)\n",
    "\n",
    "            # Store score to select best model in evaluate_two\n",
    "            if idx not in model_scores.keys():\n",
    "                model_scores[idx] = []\n",
    "                model_scores[idx].append(score.copy())\n",
    "            else:\n",
    "                model_scores[idx].append(score.copy())\n",
    "            \n",
    "            baseline_score = accuracy_score(y_true=y_test, y_pred=baseline_pred)\n",
    "\n",
    "            print(f'\\n\\nPerformance report of the baseline {name} model')\n",
    "            print('=='*27)\n",
    "            print(baseline_report)\n",
    "            print('=='*27)\n",
    "            print(\"{} {:0.2f}%\".format(\"Accuracy Score: \", baseline_score*100))\n",
    "            print('=='*27)\n",
    "            print(f'\\n\\nPerformance report of the Optimised {name} model')\n",
    "            print('=='*27)\n",
    "            print(report)\n",
    "            print('=='*27)\n",
    "            print(\"{} {:0.2f}%\".format(\"Accuracy Score: \", score*100))\n",
    "            print('=='*27)\n",
    "            print('\\n')\n",
    "    \n",
    "    return all_datasets_eval, model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4294e4",
   "metadata": {},
   "source": [
    "The features summary reports are printed out by  `review_similarity_count_two` which takes in `all_datasets_eval` which is a dictionary of datasets containing also rows with NaN reviews and `model_scores` which are all the `GridSearchCV` models scores from `evaluate_two` function.\n",
    "<br>\n",
    "<br>\n",
    "`review_similarity_count_two` function performs the following steps: \n",
    "\n",
    "* Calculates the best parametrised model among trained models for each product review \n",
    "* Splits each dataset into one with and without reviews\n",
    "* Performs vectorization using the corresponding previoustly trained vectoriser for that dataset and the `transform` method on the newly split missing review dataset\n",
    "* Predicts the semantic orientation of missing product reviews using the best model for that specific dataset\n",
    "* Merges the dataset with original reviews and the one with forecasted reviews\n",
    "* Creates two dictionaries of positive and negative reviews and for each feature performs a review count\n",
    "* Checks for word similarity using Levenshtein distance method seen in pipeline 1, although fine tuned with a different pipeline hyperparameter ratio (0.6 instead of 0.7 due to larger number of features)\n",
    "* Prints out features review table summaries for each dataset as well as details about the model used to forecast missing reviews, its out of sample accuracy and what percentage of the original dataset the forecasted missing reviews reppresented\n",
    "\n",
    "Interestingly, `MultinominalNB` was selected to be the forecasting model on 11 out of 17 datasets. Logistic Regression was selected three times, Random Forest twice and Support Vector Machine once. This model selection is diametrically opposite to what was observed in the step above where SVM was the highest scoring model. The difference comes from the fact that the model chosen here are `GridSearchCV` models whereas before the best between the two was picked. This in turn suggests that `SVM` baseline model on average performs better than the others whereas `MultinominalNB` is on average the best hyperparametrized model.\n",
    "\n",
    "The engineering solution of retaining data with missing reviews and the use Spacy `noun_chunks` method to extract noun phrases has paid its dividends in terms of quality and number of extracted reviews. This aaproach has delivered a much more semantically rich features set as opposed to the solution seen in pipeline 1.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5046d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_similarity_count_two(all_datasets_eval, model_scores):\n",
    "    \n",
    "    \n",
    "    # store models and index values \n",
    "    ml_models = {0:'MultinominalNB',1:'Support Vector Machine Classifier',2:'Random Forest Classifier',\n",
    "                 3:'Logistic Regression',4:'Stocastic Gradient Decent Classifier'}\n",
    "        \n",
    "    # Get best model among trained models for each product review        \n",
    "    highest_score_idx = []\n",
    "    highest_score_model = []\n",
    "    for i in range(len(all_datasets_eval)):\n",
    "        model_idx = 0\n",
    "        model_score = 0\n",
    "        for key, value in model_scores.items():\n",
    "            if value[i] > model_score:\n",
    "                model_score = value[i]\n",
    "                model_idx = key\n",
    "        highest_score_idx.append(model_idx)\n",
    "        highest_score_model.append(model_score)\n",
    "        \n",
    "    for idx, (key, review_dataset) in enumerate(all_datasets_eval.items()):\n",
    "        \n",
    "        print('===='*27)\n",
    "        print(' '*33,f'Top Features extracted from {tot_items[idx]} dataset')\n",
    "        print('===='*27)\n",
    "        \n",
    "        print(f'\\nSemantic orientation of missing reviews is forecasted using {ml_models[highest_score_idx[idx]]}')\n",
    "        print(f' since this is the best performing model on this dataset with an out of sample accuracy of {round((highest_score_model[idx]*100),2)}%\\n')\n",
    "        \n",
    "        # Best trained model for each ML group (MultinominalNB,SVM,RFC,LR,SGD)\n",
    "        # is store from idx 4 in steps of 6: 4;10;16\n",
    "        if highest_score_idx[idx] == 0:\n",
    "            best_model = review_dataset[4]\n",
    "        elif highest_score_idx[idx] == 1:\n",
    "            best_model = review_dataset[10]\n",
    "        elif highest_score_idx[idx] == 2:\n",
    "            best_model = review_dataset[16]\n",
    "        elif highest_score_idx[idx] == 3:\n",
    "            best_model = review_dataset[22]\n",
    "        elif highest_score_idx[idx] == 4:\n",
    "            best_model = review_dataset[28]\n",
    "        \n",
    "        \n",
    "        df_global_features = review_dataset[30] \n",
    "                \n",
    "        \n",
    "        # Dataframe with reviews\n",
    "        complete_reviews = df_global_features[df_global_features['Review'].notna()]\n",
    "        \n",
    "        # Dataframe without reviews\n",
    "        missing_reviews = df_global_features[df_global_features['Review'].isna()]\n",
    "        \n",
    "        tot_reviews = complete_reviews.shape[0]+missing_reviews.shape[0]\n",
    "        \n",
    "        print(f'The number of forecasted (missing) reviews is {missing_reviews.shape[0]} ' \\\n",
    "              f'which equates to {round((missing_reviews.shape[0]/tot_reviews)*100,2)}% of all reviews\\n')\n",
    "\n",
    "        # Apply tfidf vectorizer\n",
    "        X_test = vectorizer_list[idx].transform(missing_reviews[chosen_feature[feature_num]])\n",
    "\n",
    "        # Predict missing reviews\n",
    "        pred_miss_reviews = best_model.predict(X_test)\n",
    "        \n",
    "        # Drop Review col (Nan)\n",
    "        missing_reviews = missing_reviews.drop(['Review'],axis=1)\n",
    "\n",
    "        # Add predicted reviews\n",
    "        missing_reviews['Review'] = pred_miss_reviews\n",
    "\n",
    "        # Concat all reviews\n",
    "        all_reviews = pd.concat([complete_reviews,missing_reviews]).reset_index()\n",
    "                               \n",
    "        # Get spacy noun chunked reviews\n",
    "        spacy_chunk = all_reviews['Spacy_Chunk'].tolist()\n",
    "        \n",
    "        spacy_dict_pos = {}\n",
    "        spacy_dict_neg = {}\n",
    "        spacy_list_reviews = []\n",
    "        \n",
    "        for idx_review, noun_sequence in enumerate(spacy_chunk):\n",
    "            for single_noun in noun_sequence:\n",
    "                container = ''\n",
    "                for idx, letter in enumerate(single_noun):\n",
    "                    if len(letter) > 3 and str(letter) not in stopwords.words():\n",
    "                        container += str(letter) + \" \"\n",
    "                # Add positive review\n",
    "                if all_reviews['Review'][idx_review] == 1:\n",
    "                    if container[:-1] not in spacy_dict_pos.keys() and container != '':\n",
    "                        # Append pos review\n",
    "                        spacy_dict_pos[container[:-1]] = []\n",
    "                        spacy_dict_pos[container[:-1]].append(all_reviews['Review'][idx_review])\n",
    "                    elif container != '':\n",
    "                        # Append positive review if key already present\n",
    "                        spacy_dict_pos[container[:-1]].append(all_reviews['Review'][idx_review])\n",
    "                \n",
    "                # Add negative review \n",
    "                elif all_reviews['Review'][idx_review] == 0:\n",
    "                    if container[:-1] not in spacy_dict_neg.keys() and container != '':\n",
    "                        # Append neg review\n",
    "                        spacy_dict_neg[container[:-1]] = []\n",
    "                        spacy_dict_neg[container[:-1]].append(1)\n",
    "                    elif container != '':\n",
    "                        # Append negative review if key already present\n",
    "                        spacy_dict_neg[container[:-1]].append(1)\n",
    "\n",
    "        # Sum values in dics\n",
    "        spacy_dict_pos = {k: sum(v) for k, v in spacy_dict_pos.items()}\n",
    "        spacy_dict_neg = {k: sum(v) for k, v in spacy_dict_neg.items()}\n",
    "\n",
    "        # Check for word similarity in positive dict\n",
    "        tot = 0\n",
    "        for idx, key in enumerate(spacy_dict_pos.copy()):\n",
    "            for idy, key_two in enumerate(spacy_dict_pos.copy()):\n",
    "                # Check keys for name similarity\n",
    "                ratio = Levenshtein.ratio(key,key_two)\n",
    "                if ratio > levenshtein_ratio_one and idx != idy:\n",
    "                    # Get the shorter and longer keys\n",
    "                    longer = key if len(key)>= len(key_two) else key_two\n",
    "                    shorter = key if key != longer else key_two\n",
    "\n",
    "                    # Delete if key in dic and not the only key left \n",
    "                    if longer in spacy_dict_pos.keys() and list(spacy_dict_pos.keys()).index(longer) != idy:\n",
    "                        # Store values before deleting\n",
    "                        tot = spacy_dict_pos[longer]\n",
    "                        # Delete\n",
    "                        del spacy_dict_pos[longer]\n",
    "\n",
    "                        if shorter in spacy_dict_pos.keys():\n",
    "                            # Add value back to same key\n",
    "                            spacy_dict_pos[shorter] += tot\n",
    "                        else:\n",
    "                            spacy_dict_pos[shorter] = []\n",
    "                            spacy_dict_pos[shorter] = tot\n",
    "\n",
    "        # Check for word similarity in negative dict\n",
    "        tot = 0\n",
    "        for idx, key in enumerate(spacy_dict_neg.copy()):\n",
    "            for idy, key_two in enumerate(spacy_dict_neg.copy()):\n",
    "                # Check keys for name similarity\n",
    "                ratio = Levenshtein.ratio(key,key_two)\n",
    "                if ratio > levenshtein_ratio_one and idx != idy:\n",
    "                    # Get the shorter and longer keys\n",
    "                    longer = key if len(key)>= len(key_two) else key_two\n",
    "                    shorter = key if key != longer else key_two\n",
    "\n",
    "                    # Delete if key in dic and not the only key left \n",
    "                    if longer in spacy_dict_neg.keys() and list(spacy_dict_neg.keys()).index(longer) != idy:\n",
    "                        # Store values before deleting\n",
    "                        tot = spacy_dict_neg[longer]\n",
    "                        # Delete\n",
    "                        del spacy_dict_neg[longer]\n",
    "\n",
    "                        if shorter in spacy_dict_neg.keys():\n",
    "                            # Add value back to same key\n",
    "                            spacy_dict_neg[shorter] += tot\n",
    "                        else:\n",
    "                            spacy_dict_neg[shorter] = []\n",
    "                            spacy_dict_neg[shorter] = tot\n",
    "\n",
    "        # Sort dicts\n",
    "        spacy_dict_pos = dict(sorted(spacy_dict_pos.items(), reverse=True, key=lambda item: item[1]))\n",
    "        spacy_dict_neg = dict(sorted(spacy_dict_neg.items(), reverse=True, key=lambda item: item[1]))\n",
    "\n",
    "        # Dict to dataframe\n",
    "        spacy_pos_df = pd.DataFrame(spacy_dict_pos.items(), columns=['Features', 'Positive Review Count'])\n",
    "        spacy_neg_df = pd.DataFrame(spacy_dict_neg.items(), columns=['Features', 'Negative Review Count'])\n",
    "\n",
    "        # Concat dataframes on common features\n",
    "        spacy_pos_neg_df = pd.concat([spacy_pos_df.set_index('Features'),spacy_neg_df.set_index('Features')],\n",
    "                                     axis=1, join='inner').reset_index()\n",
    "\n",
    "        # Capitalise first letter\n",
    "        spacy_pos_neg_df['Features'] = spacy_pos_neg_df['Features'].apply(lambda x: str.capitalize(x))\n",
    "        \n",
    "        print(spacy_pos_neg_df)\n",
    "        print('\\n\\n\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00a08410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================\n",
      "                                  Evaluation metrics for Apex AD 2600 dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.93        44\n",
      "         1.0       0.95      0.80      0.87        25\n",
      "\n",
      "    accuracy                           0.91        69\n",
      "   macro avg       0.92      0.89      0.90        69\n",
      "weighted avg       0.92      0.91      0.91        69\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  91.30%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.93        44\n",
      "         1.0       0.95      0.80      0.87        25\n",
      "\n",
      "    accuracy                           0.91        69\n",
      "   macro avg       0.92      0.89      0.90        69\n",
      "weighted avg       0.92      0.91      0.91        69\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  91.30%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 1, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 1, 'kernel': 'linear', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.98      0.92        44\n",
      "         1.0       0.95      0.76      0.84        25\n",
      "\n",
      "    accuracy                           0.90        69\n",
      "   macro avg       0.91      0.87      0.88        69\n",
      "weighted avg       0.90      0.90      0.90        69\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  89.86%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.87        44\n",
      "         1.0       0.75      0.84      0.79        25\n",
      "\n",
      "    accuracy                           0.84        69\n",
      "   macro avg       0.83      0.84      0.83        69\n",
      "weighted avg       0.85      0.84      0.84        69\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.06%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      1.00      0.89        44\n",
      "         1.0       1.00      0.56      0.72        25\n",
      "\n",
      "    accuracy                           0.84        69\n",
      "   macro avg       0.90      0.78      0.80        69\n",
      "weighted avg       0.87      0.84      0.83        69\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.06%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.95      0.88        44\n",
      "         1.0       0.89      0.64      0.74        25\n",
      "\n",
      "    accuracy                           0.84        69\n",
      "   macro avg       0.86      0.80      0.81        69\n",
      "weighted avg       0.85      0.84      0.83        69\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.06%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.93        44\n",
      "         1.0       0.95      0.80      0.87        25\n",
      "\n",
      "    accuracy                           0.91        69\n",
      "   macro avg       0.92      0.89      0.90        69\n",
      "weighted avg       0.92      0.91      0.91        69\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  91.30%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.93      0.92        44\n",
      "         1.0       0.88      0.84      0.86        25\n",
      "\n",
      "    accuracy                           0.90        69\n",
      "   macro avg       0.89      0.89      0.89        69\n",
      "weighted avg       0.90      0.90      0.90        69\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  89.86%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.89      0.89        44\n",
      "         1.0       0.80      0.80      0.80        25\n",
      "\n",
      "    accuracy                           0.86        69\n",
      "   macro avg       0.84      0.84      0.84        69\n",
      "weighted avg       0.86      0.86      0.86        69\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  85.51%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.84      0.85        44\n",
      "         1.0       0.73      0.76      0.75        25\n",
      "\n",
      "    accuracy                           0.81        69\n",
      "   macro avg       0.80      0.80      0.80        69\n",
      "weighted avg       0.81      0.81      0.81        69\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.16%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Canon G3 dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.8, 'class_prior': None, 'fit_prior': True}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.08      0.15        12\n",
      "         1.0       0.77      1.00      0.87        36\n",
      "\n",
      "    accuracy                           0.77        48\n",
      "   macro avg       0.88      0.54      0.51        48\n",
      "weighted avg       0.82      0.77      0.69        48\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.08%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.17      0.29        12\n",
      "         1.0       0.78      1.00      0.88        36\n",
      "\n",
      "    accuracy                           0.79        48\n",
      "   macro avg       0.89      0.58      0.58        48\n",
      "weighted avg       0.84      0.79      0.73        48\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  79.17%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 1, 'kernel': 'linear', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.08      0.15        12\n",
      "         1.0       0.77      1.00      0.87        36\n",
      "\n",
      "    accuracy                           0.77        48\n",
      "   macro avg       0.88      0.54      0.51        48\n",
      "weighted avg       0.82      0.77      0.69        48\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.08%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.33      0.42        12\n",
      "         1.0       0.80      0.92      0.86        36\n",
      "\n",
      "    accuracy                           0.77        48\n",
      "   macro avg       0.69      0.62      0.64        48\n",
      "weighted avg       0.75      0.77      0.75        48\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.08%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.08      0.15        12\n",
      "         1.0       0.77      1.00      0.87        36\n",
      "\n",
      "    accuracy                           0.77        48\n",
      "   macro avg       0.88      0.54      0.51        48\n",
      "weighted avg       0.82      0.77      0.69        48\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.08%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.17      0.25        12\n",
      "         1.0       0.77      0.94      0.85        36\n",
      "\n",
      "    accuracy                           0.75        48\n",
      "   macro avg       0.64      0.56      0.55        48\n",
      "weighted avg       0.70      0.75      0.70        48\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  75.00%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.08      0.15        12\n",
      "         1.0       0.77      1.00      0.87        36\n",
      "\n",
      "    accuracy                           0.77        48\n",
      "   macro avg       0.88      0.54      0.51        48\n",
      "weighted avg       0.82      0.77      0.69        48\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.08%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.25      0.40        12\n",
      "         1.0       0.80      1.00      0.89        36\n",
      "\n",
      "    accuracy                           0.81        48\n",
      "   macro avg       0.90      0.62      0.64        48\n",
      "weighted avg       0.85      0.81      0.77        48\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.25%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.33      0.47        12\n",
      "         1.0       0.81      0.97      0.89        36\n",
      "\n",
      "    accuracy                           0.81        48\n",
      "   macro avg       0.81      0.65      0.68        48\n",
      "weighted avg       0.81      0.81      0.78        48\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.25%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.50      0.57        12\n",
      "         1.0       0.85      0.92      0.88        36\n",
      "\n",
      "    accuracy                           0.81        48\n",
      "   macro avg       0.76      0.71      0.73        48\n",
      "weighted avg       0.80      0.81      0.80        48\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.25%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Creative Labs dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 1.0, 'class_prior': None, 'fit_prior': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.38      0.50        50\n",
      "         1.0       0.74      0.93      0.82        94\n",
      "\n",
      "    accuracy                           0.74       144\n",
      "   macro avg       0.73      0.65      0.66       144\n",
      "weighted avg       0.74      0.74      0.71       144\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.61%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.64      0.66        50\n",
      "         1.0       0.81      0.84      0.83        94\n",
      "\n",
      "    accuracy                           0.77       144\n",
      "   macro avg       0.75      0.74      0.74       144\n",
      "weighted avg       0.77      0.77      0.77       144\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.08%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 1, 'kernel': 'linear', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.50      0.60        50\n",
      "         1.0       0.77      0.90      0.83        94\n",
      "\n",
      "    accuracy                           0.76       144\n",
      "   macro avg       0.75      0.70      0.71       144\n",
      "weighted avg       0.76      0.76      0.75       144\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  76.39%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.66      0.66        50\n",
      "         1.0       0.82      0.82      0.82        94\n",
      "\n",
      "    accuracy                           0.76       144\n",
      "   macro avg       0.74      0.74      0.74       144\n",
      "weighted avg       0.76      0.76      0.76       144\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  76.39%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.36      0.47        50\n",
      "         1.0       0.73      0.90      0.81        94\n",
      "\n",
      "    accuracy                           0.72       144\n",
      "   macro avg       0.70      0.63      0.64       144\n",
      "weighted avg       0.71      0.72      0.69       144\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.53%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.54      0.59        50\n",
      "         1.0       0.77      0.84      0.81        94\n",
      "\n",
      "    accuracy                           0.74       144\n",
      "   macro avg       0.71      0.69      0.70       144\n",
      "weighted avg       0.73      0.74      0.73       144\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.61%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.48      0.56        50\n",
      "         1.0       0.76      0.88      0.82        94\n",
      "\n",
      "    accuracy                           0.74       144\n",
      "   macro avg       0.72      0.68      0.69       144\n",
      "weighted avg       0.74      0.74      0.73       144\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  74.31%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.62      0.61        50\n",
      "         1.0       0.80      0.79      0.79        94\n",
      "\n",
      "    accuracy                           0.73       144\n",
      "   macro avg       0.70      0.70      0.70       144\n",
      "weighted avg       0.73      0.73      0.73       144\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  72.92%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.66      0.67        50\n",
      "         1.0       0.82      0.83      0.83        94\n",
      "\n",
      "    accuracy                           0.77       144\n",
      "   macro avg       0.75      0.74      0.75       144\n",
      "weighted avg       0.77      0.77      0.77       144\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.08%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.74      0.69        50\n",
      "         1.0       0.85      0.79      0.82        94\n",
      "\n",
      "    accuracy                           0.77       144\n",
      "   macro avg       0.75      0.76      0.75       144\n",
      "weighted avg       0.78      0.77      0.77       144\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.08%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Nikkon Coolpix 4300 dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.8, 'class_prior': None, 'fit_prior': True}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.84      1.00      0.92        27\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.42      0.50      0.46        32\n",
      "weighted avg       0.71      0.84      0.77        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.38%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.84      1.00      0.92        27\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.42      0.50      0.46        32\n",
      "weighted avg       0.71      0.84      0.77        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.38%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 1, 'kernel': 'linear', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.84      1.00      0.92        27\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.42      0.50      0.46        32\n",
      "weighted avg       0.71      0.84      0.77        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.38%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.20      0.29         5\n",
      "         1.0       0.87      0.96      0.91        27\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.68      0.58      0.60        32\n",
      "weighted avg       0.81      0.84      0.81        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.38%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.84      1.00      0.92        27\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.42      0.50      0.46        32\n",
      "weighted avg       0.71      0.84      0.77        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.38%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.84      1.00      0.92        27\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.42      0.50      0.46        32\n",
      "weighted avg       0.71      0.84      0.77        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.38%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 0.01, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.84      1.00      0.92        27\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.42      0.50      0.46        32\n",
      "weighted avg       0.71      0.84      0.77        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.38%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.84      1.00      0.92        27\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.42      0.50      0.46        32\n",
      "weighted avg       0.71      0.84      0.77        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.38%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'elasticnet', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.20      0.25         5\n",
      "         1.0       0.86      0.93      0.89        27\n",
      "\n",
      "    accuracy                           0.81        32\n",
      "   macro avg       0.60      0.56      0.57        32\n",
      "weighted avg       0.78      0.81      0.79        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.25%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.20      0.29         5\n",
      "         1.0       0.87      0.96      0.91        27\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.68      0.58      0.60        32\n",
      "weighted avg       0.81      0.84      0.81        32\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.38%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Nokia 6610 dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 1.4, 'class_prior': None, 'fit_prior': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.05      0.10        19\n",
      "         1.0       0.65      1.00      0.79        34\n",
      "\n",
      "    accuracy                           0.66        53\n",
      "   macro avg       0.83      0.53      0.45        53\n",
      "weighted avg       0.78      0.66      0.54        53\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  66.04%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.74      0.78        19\n",
      "         1.0       0.86      0.91      0.89        34\n",
      "\n",
      "    accuracy                           0.85        53\n",
      "   macro avg       0.84      0.82      0.83        53\n",
      "weighted avg       0.85      0.85      0.85        53\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.91%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 100, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.01, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.16      0.27        19\n",
      "         1.0       0.68      1.00      0.81        34\n",
      "\n",
      "    accuracy                           0.70        53\n",
      "   macro avg       0.84      0.58      0.54        53\n",
      "weighted avg       0.79      0.70      0.62        53\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  69.81%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.63      0.75        19\n",
      "         1.0       0.82      0.97      0.89        34\n",
      "\n",
      "    accuracy                           0.85        53\n",
      "   macro avg       0.87      0.80      0.82        53\n",
      "weighted avg       0.86      0.85      0.84        53\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.91%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.11      0.19        19\n",
      "         1.0       0.67      1.00      0.80        34\n",
      "\n",
      "    accuracy                           0.68        53\n",
      "   macro avg       0.83      0.55      0.50        53\n",
      "weighted avg       0.79      0.68      0.58        53\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  67.92%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.42      0.59        19\n",
      "         1.0       0.76      1.00      0.86        34\n",
      "\n",
      "    accuracy                           0.79        53\n",
      "   macro avg       0.88      0.71      0.73        53\n",
      "weighted avg       0.84      0.79      0.76        53\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  79.25%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.05      0.10        19\n",
      "         1.0       0.65      1.00      0.79        34\n",
      "\n",
      "    accuracy                           0.66        53\n",
      "   macro avg       0.83      0.53      0.45        53\n",
      "weighted avg       0.78      0.66      0.54        53\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  66.04%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.37      0.54        19\n",
      "         1.0       0.74      1.00      0.85        34\n",
      "\n",
      "    accuracy                           0.77        53\n",
      "   macro avg       0.87      0.68      0.69        53\n",
      "weighted avg       0.83      0.77      0.74        53\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.36%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'elasticnet', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.63      0.69        19\n",
      "         1.0       0.81      0.88      0.85        34\n",
      "\n",
      "    accuracy                           0.79        53\n",
      "   macro avg       0.78      0.76      0.77        53\n",
      "weighted avg       0.79      0.79      0.79        53\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  79.25%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.58      0.69        19\n",
      "         1.0       0.80      0.94      0.86        34\n",
      "\n",
      "    accuracy                           0.81        53\n",
      "   macro avg       0.82      0.76      0.78        53\n",
      "weighted avg       0.82      0.81      0.80        53\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.13%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Computer dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.8, 'class_prior': None, 'fit_prior': True}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         7\n",
      "         1.0       0.85      1.00      0.92        39\n",
      "\n",
      "    accuracy                           0.85        46\n",
      "   macro avg       0.42      0.50      0.46        46\n",
      "weighted avg       0.72      0.85      0.78        46\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.78%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         7\n",
      "         1.0       0.85      1.00      0.92        39\n",
      "\n",
      "    accuracy                           0.85        46\n",
      "   macro avg       0.42      0.50      0.46        46\n",
      "weighted avg       0.72      0.85      0.78        46\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.78%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.1, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         7\n",
      "         1.0       0.85      1.00      0.92        39\n",
      "\n",
      "    accuracy                           0.85        46\n",
      "   macro avg       0.42      0.50      0.46        46\n",
      "weighted avg       0.72      0.85      0.78        46\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.78%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.43      0.40         7\n",
      "         1.0       0.89      0.87      0.88        39\n",
      "\n",
      "    accuracy                           0.80        46\n",
      "   macro avg       0.63      0.65      0.64        46\n",
      "weighted avg       0.82      0.80      0.81        46\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.43%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         7\n",
      "         1.0       0.85      1.00      0.92        39\n",
      "\n",
      "    accuracy                           0.85        46\n",
      "   macro avg       0.42      0.50      0.46        46\n",
      "weighted avg       0.72      0.85      0.78        46\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.78%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         7\n",
      "         1.0       0.84      0.92      0.88        39\n",
      "\n",
      "    accuracy                           0.78        46\n",
      "   macro avg       0.42      0.46      0.44        46\n",
      "weighted avg       0.71      0.78      0.74        46\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.26%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         7\n",
      "         1.0       0.85      1.00      0.92        39\n",
      "\n",
      "    accuracy                           0.85        46\n",
      "   macro avg       0.42      0.50      0.46        46\n",
      "weighted avg       0.72      0.85      0.78        46\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.78%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.29      0.40         7\n",
      "         1.0       0.88      0.97      0.93        39\n",
      "\n",
      "    accuracy                           0.87        46\n",
      "   macro avg       0.78      0.63      0.66        46\n",
      "weighted avg       0.85      0.87      0.85        46\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  86.96%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.43      0.40         7\n",
      "         1.0       0.89      0.87      0.88        39\n",
      "\n",
      "    accuracy                           0.80        46\n",
      "   macro avg       0.63      0.65      0.64        46\n",
      "weighted avg       0.82      0.80      0.81        46\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.43%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.23      0.43      0.30         7\n",
      "         1.0       0.88      0.74      0.81        39\n",
      "\n",
      "    accuracy                           0.70        46\n",
      "   macro avg       0.55      0.59      0.55        46\n",
      "weighted avg       0.78      0.70      0.73        46\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  69.57%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Router dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 1.2, 'class_prior': None, 'fit_prior': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.47      0.51        19\n",
      "         1.0       0.70      0.77      0.73        30\n",
      "\n",
      "    accuracy                           0.65        49\n",
      "   macro avg       0.63      0.62      0.62        49\n",
      "weighted avg       0.64      0.65      0.65        49\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  65.31%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.74      0.64        19\n",
      "         1.0       0.79      0.63      0.70        30\n",
      "\n",
      "    accuracy                           0.67        49\n",
      "   macro avg       0.68      0.69      0.67        49\n",
      "weighted avg       0.70      0.67      0.68        49\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  67.35%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 100, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.1, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.32      0.41        19\n",
      "         1.0       0.67      0.87      0.75        30\n",
      "\n",
      "    accuracy                           0.65        49\n",
      "   macro avg       0.63      0.59      0.58        49\n",
      "weighted avg       0.64      0.65      0.62        49\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  65.31%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.68      0.60        19\n",
      "         1.0       0.76      0.63      0.69        30\n",
      "\n",
      "    accuracy                           0.65        49\n",
      "   macro avg       0.65      0.66      0.65        49\n",
      "weighted avg       0.68      0.65      0.66        49\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  65.31%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.47      0.58        19\n",
      "         1.0       0.73      0.90      0.81        30\n",
      "\n",
      "    accuracy                           0.73        49\n",
      "   macro avg       0.74      0.69      0.69        49\n",
      "weighted avg       0.74      0.73      0.72        49\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.47%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.79      0.73        19\n",
      "         1.0       0.85      0.77      0.81        30\n",
      "\n",
      "    accuracy                           0.78        49\n",
      "   macro avg       0.77      0.78      0.77        49\n",
      "weighted avg       0.79      0.78      0.78        49\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.55%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.53      0.56        19\n",
      "         1.0       0.72      0.77      0.74        30\n",
      "\n",
      "    accuracy                           0.67        49\n",
      "   macro avg       0.65      0.65      0.65        49\n",
      "weighted avg       0.67      0.67      0.67        49\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  67.35%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.68      0.62        19\n",
      "         1.0       0.77      0.67      0.71        30\n",
      "\n",
      "    accuracy                           0.67        49\n",
      "   macro avg       0.67      0.68      0.67        49\n",
      "weighted avg       0.69      0.67      0.68        49\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  67.35%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.74      0.65        19\n",
      "         1.0       0.80      0.67      0.73        30\n",
      "\n",
      "    accuracy                           0.69        49\n",
      "   macro avg       0.69      0.70      0.69        49\n",
      "weighted avg       0.72      0.69      0.70        49\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  69.39%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.79      0.67        19\n",
      "         1.0       0.83      0.63      0.72        30\n",
      "\n",
      "    accuracy                           0.69        49\n",
      "   macro avg       0.70      0.71      0.69        49\n",
      "weighted avg       0.73      0.69      0.70        49\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  69.39%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Speaker dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.8, 'class_prior': None, 'fit_prior': True}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        17\n",
      "         1.0       0.72      1.00      0.83        43\n",
      "\n",
      "    accuracy                           0.72        60\n",
      "   macro avg       0.36      0.50      0.42        60\n",
      "weighted avg       0.51      0.72      0.60        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.67%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        17\n",
      "         1.0       0.72      1.00      0.83        43\n",
      "\n",
      "    accuracy                           0.72        60\n",
      "   macro avg       0.36      0.50      0.42        60\n",
      "weighted avg       0.51      0.72      0.60        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.67%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 1, 'kernel': 'linear', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        17\n",
      "         1.0       0.72      1.00      0.83        43\n",
      "\n",
      "    accuracy                           0.72        60\n",
      "   macro avg       0.36      0.50      0.42        60\n",
      "weighted avg       0.51      0.72      0.60        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.67%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.12      0.18        17\n",
      "         1.0       0.73      0.93      0.82        43\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.56      0.52      0.50        60\n",
      "weighted avg       0.63      0.70      0.64        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  70.00%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        17\n",
      "         1.0       0.72      1.00      0.83        43\n",
      "\n",
      "    accuracy                           0.72        60\n",
      "   macro avg       0.36      0.50      0.42        60\n",
      "weighted avg       0.51      0.72      0.60        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.67%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        17\n",
      "         1.0       0.72      1.00      0.83        43\n",
      "\n",
      "    accuracy                           0.72        60\n",
      "   macro avg       0.36      0.50      0.42        60\n",
      "weighted avg       0.51      0.72      0.60        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.67%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 0.01, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        17\n",
      "         1.0       0.72      1.00      0.83        43\n",
      "\n",
      "    accuracy                           0.72        60\n",
      "   macro avg       0.36      0.50      0.42        60\n",
      "weighted avg       0.51      0.72      0.60        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.67%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        17\n",
      "         1.0       0.72      1.00      0.83        43\n",
      "\n",
      "    accuracy                           0.72        60\n",
      "   macro avg       0.36      0.50      0.42        60\n",
      "weighted avg       0.51      0.72      0.60        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.67%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.06      0.11        17\n",
      "         1.0       0.73      1.00      0.84        43\n",
      "\n",
      "    accuracy                           0.73        60\n",
      "   macro avg       0.86      0.53      0.48        60\n",
      "weighted avg       0.81      0.73      0.64        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.33%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.06      0.10        17\n",
      "         1.0       0.72      0.95      0.82        43\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.53      0.51      0.46        60\n",
      "weighted avg       0.61      0.70      0.62        60\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  70.00%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Canon Powershot SD500 dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.81      1.00      0.89        21\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.40      0.50      0.45        26\n",
      "weighted avg       0.65      0.81      0.72        26\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.77%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.81      1.00      0.89        21\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.40      0.50      0.45        26\n",
      "weighted avg       0.65      0.81      0.72        26\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.77%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.1, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.81      1.00      0.89        21\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.40      0.50      0.45        26\n",
      "weighted avg       0.65      0.81      0.72        26\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.77%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.20      0.29         5\n",
      "         1.0       0.83      0.95      0.89        21\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.67      0.58      0.59        26\n",
      "weighted avg       0.77      0.81      0.77        26\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.77%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.81      1.00      0.89        21\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.40      0.50      0.45        26\n",
      "weighted avg       0.65      0.81      0.72        26\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.77%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.20      0.33         5\n",
      "         1.0       0.84      1.00      0.91        21\n",
      "\n",
      "    accuracy                           0.85        26\n",
      "   macro avg       0.92      0.60      0.62        26\n",
      "weighted avg       0.87      0.85      0.80        26\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  84.62%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 0.01, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.81      1.00      0.89        21\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.40      0.50      0.45        26\n",
      "weighted avg       0.65      0.81      0.72        26\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.77%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         5\n",
      "         1.0       0.81      1.00      0.89        21\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.40      0.50      0.45        26\n",
      "weighted avg       0.65      0.81      0.72        26\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.77%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.20      0.22         5\n",
      "         1.0       0.82      0.86      0.84        21\n",
      "\n",
      "    accuracy                           0.73        26\n",
      "   macro avg       0.53      0.53      0.53        26\n",
      "weighted avg       0.71      0.73      0.72        26\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.08%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.20      0.29         5\n",
      "         1.0       0.83      0.95      0.89        21\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.67      0.58      0.59        26\n",
      "weighted avg       0.77      0.81      0.77        26\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.77%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Canon S100 dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.8, 'class_prior': None, 'fit_prior': True}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.71      1.00      0.83        25\n",
      "\n",
      "    accuracy                           0.71        35\n",
      "   macro avg       0.36      0.50      0.42        35\n",
      "weighted avg       0.51      0.71      0.60        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.43%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.71      1.00      0.83        25\n",
      "\n",
      "    accuracy                           0.71        35\n",
      "   macro avg       0.36      0.50      0.42        35\n",
      "weighted avg       0.51      0.71      0.60        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.43%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 1, 'kernel': 'linear', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.71      1.00      0.83        25\n",
      "\n",
      "    accuracy                           0.71        35\n",
      "   macro avg       0.36      0.50      0.42        35\n",
      "weighted avg       0.51      0.71      0.60        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.43%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.40      0.53        10\n",
      "         1.0       0.80      0.96      0.87        25\n",
      "\n",
      "    accuracy                           0.80        35\n",
      "   macro avg       0.80      0.68      0.70        35\n",
      "weighted avg       0.80      0.80      0.78        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.00%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.71      1.00      0.83        25\n",
      "\n",
      "    accuracy                           0.71        35\n",
      "   macro avg       0.36      0.50      0.42        35\n",
      "weighted avg       0.51      0.71      0.60        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.43%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.20      0.33        10\n",
      "         1.0       0.76      1.00      0.86        25\n",
      "\n",
      "    accuracy                           0.77        35\n",
      "   macro avg       0.88      0.60      0.60        35\n",
      "weighted avg       0.83      0.77      0.71        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.14%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.71      1.00      0.83        25\n",
      "\n",
      "    accuracy                           0.71        35\n",
      "   macro avg       0.36      0.50      0.42        35\n",
      "weighted avg       0.51      0.71      0.60        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.43%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.40      0.57        10\n",
      "         1.0       0.81      1.00      0.89        25\n",
      "\n",
      "    accuracy                           0.83        35\n",
      "   macro avg       0.90      0.70      0.73        35\n",
      "weighted avg       0.86      0.83      0.80        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  82.86%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.40      0.53        10\n",
      "         1.0       0.80      0.96      0.87        25\n",
      "\n",
      "    accuracy                           0.80        35\n",
      "   macro avg       0.80      0.68      0.70        35\n",
      "weighted avg       0.80      0.80      0.78        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  80.00%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.40      0.47        10\n",
      "         1.0       0.79      0.88      0.83        25\n",
      "\n",
      "    accuracy                           0.74        35\n",
      "   macro avg       0.68      0.64      0.65        35\n",
      "weighted avg       0.72      0.74      0.73        35\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  74.29%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Diaper Champ dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 1.4, 'class_prior': None, 'fit_prior': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        11\n",
      "         1.0       0.74      1.00      0.85        32\n",
      "\n",
      "    accuracy                           0.74        43\n",
      "   macro avg       0.37      0.50      0.43        43\n",
      "weighted avg       0.55      0.74      0.64        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  74.42%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.55      0.71        11\n",
      "         1.0       0.86      1.00      0.93        32\n",
      "\n",
      "    accuracy                           0.88        43\n",
      "   macro avg       0.93      0.77      0.82        43\n",
      "weighted avg       0.90      0.88      0.87        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  88.37%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 1, 'kernel': 'linear', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        11\n",
      "         1.0       0.74      1.00      0.85        32\n",
      "\n",
      "    accuracy                           0.74        43\n",
      "   macro avg       0.37      0.50      0.43        43\n",
      "weighted avg       0.55      0.74      0.64        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  74.42%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.64      0.74        11\n",
      "         1.0       0.89      0.97      0.93        32\n",
      "\n",
      "    accuracy                           0.88        43\n",
      "   macro avg       0.88      0.80      0.83        43\n",
      "weighted avg       0.88      0.88      0.88        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  88.37%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.09      0.17        11\n",
      "         1.0       0.76      1.00      0.86        32\n",
      "\n",
      "    accuracy                           0.77        43\n",
      "   macro avg       0.88      0.55      0.52        43\n",
      "weighted avg       0.82      0.77      0.69        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  76.74%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        11\n",
      "         1.0       0.74      1.00      0.85        32\n",
      "\n",
      "    accuracy                           0.74        43\n",
      "   macro avg       0.37      0.50      0.43        43\n",
      "weighted avg       0.55      0.74      0.64        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  74.42%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        11\n",
      "         1.0       0.74      1.00      0.85        32\n",
      "\n",
      "    accuracy                           0.74        43\n",
      "   macro avg       0.37      0.50      0.43        43\n",
      "weighted avg       0.55      0.74      0.64        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  74.42%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.36      0.53        11\n",
      "         1.0       0.82      1.00      0.90        32\n",
      "\n",
      "    accuracy                           0.84        43\n",
      "   macro avg       0.91      0.68      0.72        43\n",
      "weighted avg       0.87      0.84      0.81        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  83.72%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.73      0.84        11\n",
      "         1.0       0.91      1.00      0.96        32\n",
      "\n",
      "    accuracy                           0.93        43\n",
      "   macro avg       0.96      0.86      0.90        43\n",
      "weighted avg       0.94      0.93      0.93        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  93.02%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.36      0.53        11\n",
      "         1.0       0.82      1.00      0.90        32\n",
      "\n",
      "    accuracy                           0.84        43\n",
      "   macro avg       0.91      0.68      0.72        43\n",
      "weighted avg       0.87      0.84      0.81        43\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  83.72%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Hitachi Router dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 1.2, 'class_prior': None, 'fit_prior': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.75      1.00      0.86        30\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.38      0.50      0.43        40\n",
      "weighted avg       0.56      0.75      0.64        40\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  75.00%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.20      0.27        10\n",
      "         1.0       0.77      0.90      0.83        30\n",
      "\n",
      "    accuracy                           0.73        40\n",
      "   macro avg       0.59      0.55      0.55        40\n",
      "weighted avg       0.68      0.72      0.69        40\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  72.50%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 100, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.1, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.75      1.00      0.86        30\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.38      0.50      0.43        40\n",
      "weighted avg       0.56      0.75      0.64        40\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  75.00%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      0.10      0.13        10\n",
      "         1.0       0.74      0.87      0.80        30\n",
      "\n",
      "    accuracy                           0.68        40\n",
      "   macro avg       0.47      0.48      0.47        40\n",
      "weighted avg       0.61      0.68      0.63        40\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  67.50%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.75      1.00      0.86        30\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.38      0.50      0.43        40\n",
      "weighted avg       0.56      0.75      0.64        40\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  75.00%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.10      0.14        10\n",
      "         1.0       0.75      0.90      0.82        30\n",
      "\n",
      "    accuracy                           0.70        40\n",
      "   macro avg       0.50      0.50      0.48        40\n",
      "weighted avg       0.62      0.70      0.65        40\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  70.00%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.75      1.00      0.86        30\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.38      0.50      0.43        40\n",
      "weighted avg       0.56      0.75      0.64        40\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  75.00%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.10      0.15        10\n",
      "         1.0       0.76      0.93      0.84        30\n",
      "\n",
      "    accuracy                           0.73        40\n",
      "   macro avg       0.55      0.52      0.49        40\n",
      "weighted avg       0.65      0.72      0.67        40\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  72.50%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.17      0.10      0.12        10\n",
      "         1.0       0.74      0.83      0.78        30\n",
      "\n",
      "    accuracy                           0.65        40\n",
      "   macro avg       0.45      0.47      0.45        40\n",
      "weighted avg       0.59      0.65      0.62        40\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  65.00%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.10      0.15        10\n",
      "         1.0       0.76      0.93      0.84        30\n",
      "\n",
      "    accuracy                           0.73        40\n",
      "   macro avg       0.55      0.52      0.49        40\n",
      "weighted avg       0.65      0.72      0.67        40\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  72.50%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Ipod dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 1.4, 'class_prior': None, 'fit_prior': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.40      0.57        10\n",
      "         1.0       0.79      1.00      0.88        23\n",
      "\n",
      "    accuracy                           0.82        33\n",
      "   macro avg       0.90      0.70      0.73        33\n",
      "weighted avg       0.86      0.82      0.79        33\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.82%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.60      0.67        10\n",
      "         1.0       0.84      0.91      0.87        23\n",
      "\n",
      "    accuracy                           0.82        33\n",
      "   macro avg       0.79      0.76      0.77        33\n",
      "weighted avg       0.81      0.82      0.81        33\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.82%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 100, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.01, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.30      0.46        10\n",
      "         1.0       0.77      1.00      0.87        23\n",
      "\n",
      "    accuracy                           0.79        33\n",
      "   macro avg       0.88      0.65      0.66        33\n",
      "weighted avg       0.84      0.79      0.74        33\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.79%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.60      0.67        10\n",
      "         1.0       0.84      0.91      0.87        23\n",
      "\n",
      "    accuracy                           0.82        33\n",
      "   macro avg       0.79      0.76      0.77        33\n",
      "weighted avg       0.81      0.82      0.81        33\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.82%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.10      0.18        10\n",
      "         1.0       0.72      1.00      0.84        23\n",
      "\n",
      "    accuracy                           0.73        33\n",
      "   macro avg       0.86      0.55      0.51        33\n",
      "weighted avg       0.80      0.73      0.64        33\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  72.73%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.30      0.46        10\n",
      "         1.0       0.77      1.00      0.87        23\n",
      "\n",
      "    accuracy                           0.79        33\n",
      "   macro avg       0.88      0.65      0.66        33\n",
      "weighted avg       0.84      0.79      0.74        33\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.79%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.30      0.46        10\n",
      "         1.0       0.77      1.00      0.87        23\n",
      "\n",
      "    accuracy                           0.79        33\n",
      "   macro avg       0.88      0.65      0.66        33\n",
      "weighted avg       0.84      0.79      0.74        33\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.79%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.60      0.67        10\n",
      "         1.0       0.84      0.91      0.87        23\n",
      "\n",
      "    accuracy                           0.82        33\n",
      "   macro avg       0.79      0.76      0.77        33\n",
      "weighted avg       0.81      0.82      0.81        33\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.82%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'elasticnet', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.60      0.67        10\n",
      "         1.0       0.84      0.91      0.87        23\n",
      "\n",
      "    accuracy                           0.82        33\n",
      "   macro avg       0.79      0.76      0.77        33\n",
      "weighted avg       0.81      0.82      0.81        33\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.82%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.60      0.67        10\n",
      "         1.0       0.84      0.91      0.87        23\n",
      "\n",
      "    accuracy                           0.82        33\n",
      "   macro avg       0.79      0.76      0.77        33\n",
      "weighted avg       0.81      0.82      0.81        33\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  81.82%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Linksys Router dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.8, 'class_prior': None, 'fit_prior': True}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.74      1.00      0.85        28\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.37      0.50      0.42        38\n",
      "weighted avg       0.54      0.74      0.63        38\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.68%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.74      1.00      0.85        28\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.37      0.50      0.42        38\n",
      "weighted avg       0.54      0.74      0.63        38\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.68%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.1, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.74      1.00      0.85        28\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.37      0.50      0.42        38\n",
      "weighted avg       0.54      0.74      0.63        38\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.68%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.20      0.29        10\n",
      "         1.0       0.76      0.93      0.84        28\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.63      0.56      0.56        38\n",
      "weighted avg       0.70      0.74      0.69        38\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.68%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.74      1.00      0.85        28\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.37      0.50      0.42        38\n",
      "weighted avg       0.54      0.74      0.63        38\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.68%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.74      1.00      0.85        28\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.37      0.50      0.42        38\n",
      "weighted avg       0.54      0.74      0.63        38\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.68%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        10\n",
      "         1.0       0.74      1.00      0.85        28\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.37      0.50      0.42        38\n",
      "weighted avg       0.54      0.74      0.63        38\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.68%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.10      0.15        10\n",
      "         1.0       0.74      0.93      0.83        28\n",
      "\n",
      "    accuracy                           0.71        38\n",
      "   macro avg       0.54      0.51      0.49        38\n",
      "weighted avg       0.64      0.71      0.65        38\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.05%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'elasticnet', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.20      0.27        10\n",
      "         1.0       0.76      0.89      0.82        28\n",
      "\n",
      "    accuracy                           0.71        38\n",
      "   macro avg       0.58      0.55      0.54        38\n",
      "weighted avg       0.66      0.71      0.67        38\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.05%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.20      0.27        10\n",
      "         1.0       0.76      0.89      0.82        28\n",
      "\n",
      "    accuracy                           0.71        38\n",
      "   macro avg       0.58      0.55      0.54        38\n",
      "weighted avg       0.66      0.71      0.67        38\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.05%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Micro MP3 dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 1.0, 'class_prior': None, 'fit_prior': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.07      0.13        28\n",
      "         1.0       0.71      1.00      0.83        65\n",
      "\n",
      "    accuracy                           0.72        93\n",
      "   macro avg       0.86      0.54      0.48        93\n",
      "weighted avg       0.80      0.72      0.62        93\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  72.04%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.43      0.55        28\n",
      "         1.0       0.79      0.94      0.86        65\n",
      "\n",
      "    accuracy                           0.78        93\n",
      "   macro avg       0.77      0.68      0.70        93\n",
      "weighted avg       0.78      0.78      0.76        93\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.49%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 100, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.01, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.11      0.19        28\n",
      "         1.0       0.72      1.00      0.84        65\n",
      "\n",
      "    accuracy                           0.73        93\n",
      "   macro avg       0.86      0.55      0.52        93\n",
      "weighted avg       0.81      0.73      0.64        93\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.12%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.50      0.56        28\n",
      "         1.0       0.80      0.88      0.84        65\n",
      "\n",
      "    accuracy                           0.76        93\n",
      "   macro avg       0.72      0.69      0.70        93\n",
      "weighted avg       0.75      0.76      0.75        93\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  76.34%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.18      0.29        28\n",
      "         1.0       0.73      0.97      0.83        65\n",
      "\n",
      "    accuracy                           0.73        93\n",
      "   macro avg       0.72      0.57      0.56        93\n",
      "weighted avg       0.73      0.73      0.67        93\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.12%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.36      0.49        28\n",
      "         1.0       0.78      0.95      0.86        65\n",
      "\n",
      "    accuracy                           0.77        93\n",
      "   macro avg       0.77      0.66      0.67        93\n",
      "weighted avg       0.77      0.77      0.74        93\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.42%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.07      0.13        28\n",
      "         1.0       0.71      1.00      0.83        65\n",
      "\n",
      "    accuracy                           0.72        93\n",
      "   macro avg       0.86      0.54      0.48        93\n",
      "weighted avg       0.80      0.72      0.62        93\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  72.04%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.43      0.55        28\n",
      "         1.0       0.79      0.94      0.86        65\n",
      "\n",
      "    accuracy                           0.78        93\n",
      "   macro avg       0.77      0.68      0.70        93\n",
      "weighted avg       0.78      0.78      0.76        93\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.49%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.50      0.57        28\n",
      "         1.0       0.81      0.89      0.85        65\n",
      "\n",
      "    accuracy                           0.77        93\n",
      "   macro avg       0.74      0.70      0.71        93\n",
      "weighted avg       0.76      0.77      0.76        93\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  77.42%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.46      0.53        28\n",
      "         1.0       0.79      0.88      0.83        65\n",
      "\n",
      "    accuracy                           0.75        93\n",
      "   macro avg       0.71      0.67      0.68        93\n",
      "weighted avg       0.74      0.75      0.74        93\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  75.27%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Nokia 6600 dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 1.4, 'class_prior': None, 'fit_prior': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.09      0.17        32\n",
      "         1.0       0.59      1.00      0.74        41\n",
      "\n",
      "    accuracy                           0.60        73\n",
      "   macro avg       0.79      0.55      0.46        73\n",
      "weighted avg       0.77      0.60      0.49        73\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  60.27%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.44      0.55        32\n",
      "         1.0       0.67      0.88      0.76        41\n",
      "\n",
      "    accuracy                           0.68        73\n",
      "   macro avg       0.70      0.66      0.65        73\n",
      "weighted avg       0.70      0.68      0.67        73\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  68.49%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 100, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.1, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.06      0.12        32\n",
      "         1.0       0.58      1.00      0.73        41\n",
      "\n",
      "    accuracy                           0.59        73\n",
      "   macro avg       0.79      0.53      0.42        73\n",
      "weighted avg       0.76      0.59      0.46        73\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  58.90%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.53      0.63        32\n",
      "         1.0       0.71      0.88      0.78        41\n",
      "\n",
      "    accuracy                           0.73        73\n",
      "   macro avg       0.74      0.70      0.71        73\n",
      "weighted avg       0.74      0.73      0.72        73\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  72.60%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.03      0.06        32\n",
      "         1.0       0.56      0.98      0.71        41\n",
      "\n",
      "    accuracy                           0.56        73\n",
      "   macro avg       0.53      0.50      0.39        73\n",
      "weighted avg       0.54      0.56      0.43        73\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  56.16%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.06      0.11        32\n",
      "         1.0       0.57      0.98      0.72        41\n",
      "\n",
      "    accuracy                           0.58        73\n",
      "   macro avg       0.62      0.52      0.42        73\n",
      "weighted avg       0.61      0.58      0.45        73\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  57.53%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.03      0.06        32\n",
      "         1.0       0.57      1.00      0.73        41\n",
      "\n",
      "    accuracy                           0.58        73\n",
      "   macro avg       0.78      0.52      0.39        73\n",
      "weighted avg       0.76      0.58      0.43        73\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  57.53%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.41      0.55        32\n",
      "         1.0       0.67      0.95      0.79        41\n",
      "\n",
      "    accuracy                           0.71        73\n",
      "   macro avg       0.77      0.68      0.67        73\n",
      "weighted avg       0.76      0.71      0.69        73\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.23%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'elasticnet', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.53      0.62        32\n",
      "         1.0       0.70      0.85      0.77        41\n",
      "\n",
      "    accuracy                           0.71        73\n",
      "   macro avg       0.72      0.69      0.69        73\n",
      "weighted avg       0.72      0.71      0.70        73\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.23%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.50      0.59        32\n",
      "         1.0       0.69      0.85      0.76        41\n",
      "\n",
      "    accuracy                           0.70        73\n",
      "   macro avg       0.71      0.68      0.68        73\n",
      "weighted avg       0.70      0.70      0.69        73\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  69.86%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Evaluation metrics for Norton dataset\n",
      "============================================================================================================\n",
      "\n",
      "              MODEL 1: MultinominalNB\n",
      "\n",
      "Best Parameters for MultinominalNB classsifier\n",
      "=====================================================\n",
      "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      1.00      0.88        32\n",
      "         1.0       1.00      0.10      0.18        10\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.89      0.55      0.53        42\n",
      "weighted avg       0.83      0.79      0.71        42\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.57%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised MultinominalNB model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      1.00      0.88        32\n",
      "         1.0       1.00      0.10      0.18        10\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.89      0.55      0.53        42\n",
      "weighted avg       0.83      0.79      0.71        42\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.57%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 2: SVMClassifier\n",
      "\n",
      "Best Parameters for SVMClassifier classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.1, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      1.00      0.88        32\n",
      "         1.0       1.00      0.10      0.18        10\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.89      0.55      0.53        42\n",
      "weighted avg       0.83      0.79      0.71        42\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.57%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SVMClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.84      0.82        32\n",
      "         1.0       0.38      0.30      0.33        10\n",
      "\n",
      "    accuracy                           0.71        42\n",
      "   macro avg       0.58      0.57      0.58        42\n",
      "weighted avg       0.69      0.71      0.70        42\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.43%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 3: RandomForestClassifier\n",
      "\n",
      "Best Parameters for RandomForestClassifier classsifier\n",
      "=====================================================\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      1.00      0.88        32\n",
      "         1.0       1.00      0.10      0.18        10\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.89      0.55      0.53        42\n",
      "weighted avg       0.83      0.79      0.71        42\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.57%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised RandomForestClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.88      0.81        32\n",
      "         1.0       0.20      0.10      0.13        10\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.48      0.49      0.47        42\n",
      "weighted avg       0.62      0.69      0.65        42\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  69.05%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 4: LogisticRegression\n",
      "\n",
      "Best Parameters for LogisticRegression classsifier\n",
      "=====================================================\n",
      "{'C': 10, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      1.00      0.88        32\n",
      "         1.0       1.00      0.10      0.18        10\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.89      0.55      0.53        42\n",
      "weighted avg       0.83      0.79      0.71        42\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  78.57%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised LogisticRegression model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.88      0.84        32\n",
      "         1.0       0.43      0.30      0.35        10\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.61      0.59      0.59        42\n",
      "weighted avg       0.71      0.74      0.72        42\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  73.81%\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "              MODEL 5: SGDClassifier\n",
      "\n",
      "Best Parameters for SGDClassifier classsifier\n",
      "=====================================================\n",
      "{'alpha': 0.001, 'average': False, 'class_weight': None, 'early_stopping': True, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'elasticnet', 'power_t': 0.5, 'random_state': 1, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "=====================================================\n",
      "\n",
      "\n",
      "Performance report of the baseline SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.75      0.76        32\n",
      "         1.0       0.27      0.30      0.29        10\n",
      "\n",
      "    accuracy                           0.64        42\n",
      "   macro avg       0.52      0.53      0.52        42\n",
      "weighted avg       0.65      0.64      0.65        42\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  64.29%\n",
      "======================================================\n",
      "\n",
      "\n",
      "Performance report of the Optimised SGDClassifier model\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.84      0.82        32\n",
      "         1.0       0.38      0.30      0.33        10\n",
      "\n",
      "    accuracy                           0.71        42\n",
      "   macro avg       0.58      0.57      0.58        42\n",
      "weighted avg       0.69      0.71      0.70        42\n",
      "\n",
      "======================================================\n",
      "Accuracy Score:  71.43%\n",
      "======================================================\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Apex AD 2600 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using MultinominalNB\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 91.3%\n",
      "\n",
      "The number of forecasted (missing) reviews is 495 which equates to 59.0% of all reviews\n",
      "\n",
      "              Features  Positive Review Count  Negative Review Count\n",
      "0                 Apex                   25.0                     32\n",
      "1               Amazon                   18.0                      4\n",
      "2               Review                   15.0                      4\n",
      "3              Problem                   13.0                     38\n",
      "4                Money                   10.0                     10\n",
      "5              Picture                   10.0                     28\n",
      "6               Format                    9.0                      2\n",
      "7            Christmas                    7.0                     16\n",
      "8              Product                    7.0                     12\n",
      "9              Machine                    7.0                      5\n",
      "10                Unit                    6.0                     19\n",
      "11               Sound                    6.0                      4\n",
      "12              Button                    5.0                     12\n",
      "13               Model                    5.0                      3\n",
      "14              Movies                    5.0                      1\n",
      "15            Instance                    5.0                      2\n",
      "16               Brand                    5.0                      2\n",
      "17                Sony                    4.0                      1\n",
      "18                Week                    4.0                     10\n",
      "19             Display                    4.0                      1\n",
      "20         Performance                    3.0                      1\n",
      "21               Video                    3.0                     10\n",
      "22                Buck                    3.0                      3\n",
      "23               Files                    2.0                      1\n",
      "24                Time                    2.0                      9\n",
      "25              Screen                    2.0                      7\n",
      "26               Media                    2.0                      1\n",
      "27              Remote                    2.0                     12\n",
      "28        Silver plate                    2.0                      3\n",
      "29  Inexpensive player                    2.0                      1\n",
      "30               Audio                    2.0                      2\n",
      "31             Company                    2.0                      9\n",
      "32                Long                    2.0                      1\n",
      "33           Excellent                    2.0                      1\n",
      "34         Replacement                    2.0                      4\n",
      "35          Experience                    2.0                      2\n",
      "36        Apex product                    2.0                      1\n",
      "37               Motor                    1.0                      2\n",
      "38                Fact                    1.0                      3\n",
      "39              Drawer                    1.0                      1\n",
      "40      Remote control                    1.0                     11\n",
      "41              Rental                    1.0                      1\n",
      "42       Output output                    1.0                      1\n",
      "43               Lemon                    1.0                      1\n",
      "44                Lots                    1.0                      3\n",
      "45              Layout                    1.0                      1\n",
      "46           Apex apex                    1.0                      1\n",
      "47       Build quality                    1.0                      2\n",
      "48               Color                    1.0                      1\n",
      "49           Customers                    1.0                     12\n",
      "50               Favor                    1.0                      1\n",
      "51             Toshiba                    1.0                      1\n",
      "52                Gift                    1.0                      3\n",
      "53                Rest                    1.0                      1\n",
      "54              Burner                    1.0                      1\n",
      "55              Family                    1.0                      2\n",
      "56               Month                    1.0                     17\n",
      "57            Research                    1.0                      1\n",
      "58              Season                    1.0                      5\n",
      "59             Finding                    1.0                      1\n",
      "60                Crap                    1.0                      3\n",
      "61            Exchange                    1.0                      1\n",
      "62                Bang                    1.0                      2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Canon G3 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using Logistic Regression\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 81.25%\n",
      "\n",
      "The number of forecasted (missing) reviews is 403 which equates to 62.77% of all reviews\n",
      "\n",
      "          Features  Positive Review Count  Negative Review Count\n",
      "0           Camera                  178.0                     13\n",
      "1          Picture                   41.0                      6\n",
      "2     Great camera                   24.0                      2\n",
      "3             Time                   21.0                      2\n",
      "4            Point                   14.0                      1\n",
      "5             Hand                   12.0                      1\n",
      "6            Flash                   11.0                      3\n",
      "7            Image                   10.0                      8\n",
      "8             Dial                    9.0                      1\n",
      "9          Shutter                    9.0                      3\n",
      "10            Lens                    9.0                     14\n",
      "11           Nikon                    8.0                      1\n",
      "12        Software                    7.0                      5\n",
      "13           Light                    6.0                      1\n",
      "14            Fact                    5.0                      1\n",
      "15          Button                    5.0                      1\n",
      "16            View                    5.0                      2\n",
      "17           Focus                    4.0                      2\n",
      "18           Noise                    4.0                      1\n",
      "19      Viewfinder                    3.0                     15\n",
      "20       Long time                    3.0                      1\n",
      "21          Months                    3.0                      1\n",
      "22        Drawback                    2.0                      1\n",
      "23          Action                    2.0                      1\n",
      "24           Grain                    1.0                      1\n",
      "25           Depth                    1.0                      1\n",
      "26           Catch                    1.0                      1\n",
      "27           Delay                    1.0                      1\n",
      "28  Shutter button                    1.0                      1\n",
      "29            Task                    1.0                      1\n",
      "30            Lack                    1.0                      1\n",
      "31           Strap                    1.0                      1\n",
      "32     Limitations                    1.0                      1\n",
      "33  Online reviews                    1.0                      1\n",
      "34       Complaint                    1.0                      1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Creative Labs dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using MultinominalNB\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 77.08%\n",
      "\n",
      "The number of forecasted (missing) reviews is 1071 which equates to 59.8% of all reviews\n",
      "\n",
      "                  Features  Positive Review Count  Negative Review Count\n",
      "0                     Ipod                  112.0                     17\n",
      "1                 Software                   88.0                     63\n",
      "2                    Music                   47.0                     10\n",
      "3                    Nomad                   37.0                     19\n",
      "4                  Product                   36.0                     16\n",
      "5                     Xtra                   35.0                      6\n",
      "6                 Computer                   35.0                     17\n",
      "7                     Play                   28.0                     13\n",
      "8                     Unit                   24.0                      9\n",
      "9                    Sound                   20.0                      3\n",
      "10                Playlist                   19.0                     13\n",
      "11                   Store                   18.0                      3\n",
      "12                   Month                   16.0                     11\n",
      "13                    Time                   15.0                     21\n",
      "14                  Screen                   14.0                      6\n",
      "15                  Things                   14.0                      7\n",
      "16                Creative                   14.0                      7\n",
      "17                  Option                   13.0                     12\n",
      "18                 Company                   12.0                      7\n",
      "19                   Apple                   12.0                      3\n",
      "20                   Track                   12.0                     19\n",
      "21                  Future                   12.0                      4\n",
      "22                   Money                   11.0                      1\n",
      "23                    Week                   11.0                      4\n",
      "24                  Button                   11.0                     27\n",
      "25                  Window                   10.0                      7\n",
      "26                   Tunes                   10.0                      1\n",
      "27                    Cons                   10.0                     22\n",
      "28                 Battery                   10.0                     16\n",
      "29                    Fact                    9.0                      6\n",
      "30                    Pair                    9.0                      7\n",
      "31                  Answer                    9.0                      4\n",
      "32            Media source                    8.0                     15\n",
      "33                   Album                    8.0                      6\n",
      "34                    Zens                    8.0                      2\n",
      "35                  Design                    7.0                      3\n",
      "36                   Order                    7.0                     14\n",
      "37                    Kind                    7.0                      5\n",
      "38                    Pros                    7.0                     14\n",
      "39                 Feature                    7.0                      1\n",
      "40                 Jukebox                    6.0                      3\n",
      "41             Music match                    6.0                      1\n",
      "42               Hard disk                    6.0                     25\n",
      "43                   Front                    5.0                      2\n",
      "44                  Scroll                    5.0                      7\n",
      "45                    Room                    5.0                      2\n",
      "46                   Issue                    5.0                      7\n",
      "47                   Model                    5.0                      4\n",
      "48                   Karma                    5.0                      2\n",
      "49                    Dent                    5.0                      1\n",
      "50                 Trouble                    5.0                     20\n",
      "51                    Home                    4.0                      1\n",
      "52            Scroll wheel                    4.0                     11\n",
      "53                  Archos                    4.0                      2\n",
      "54                    Item                    4.0                     14\n",
      "55                  Hassle                    4.0                      2\n",
      "56                  Pocket                    4.0                      1\n",
      "57                    Menu                    4.0                      2\n",
      "58                    List                    4.0                      1\n",
      "59                 Dollars                    4.0                      2\n",
      "60                Explorer                    4.0                      1\n",
      "61                Warranty                    4.0                     15\n",
      "62                Internet                    4.0                      8\n",
      "63                  Number                    3.0                      5\n",
      "64               Equalizer                    3.0                      1\n",
      "65                    Nice                    3.0                      1\n",
      "66                  Access                    3.0                      7\n",
      "67                    User                    3.0                      4\n",
      "68                  Format                    3.0                      1\n",
      "69                   Thumb                    2.0                      1\n",
      "70   Navigation navigation                    2.0                      3\n",
      "71         Previous player                    2.0                      1\n",
      "72                    Gigs                    2.0                      1\n",
      "73                   Class                    2.0                      2\n",
      "74              Protection                    2.0                      1\n",
      "75                  Plenty                    2.0                      1\n",
      "76                  Manual                    2.0                      4\n",
      "77                   Cable                    2.0                      2\n",
      "78                   Title                    2.0                     11\n",
      "79                 Website                    2.0                      1\n",
      "80            Organization                    2.0                      2\n",
      "81                  Couple                    2.0                      2\n",
      "82              Data files                    2.0                      2\n",
      "83              Short time                    2.0                      1\n",
      "84           Mobile device                    2.0                      1\n",
      "85              Experience                    2.0                      4\n",
      "86                    Crap                    2.0                      3\n",
      "87                  Folder                    2.0                      2\n",
      "88           Carrying case                    1.0                      3\n",
      "89                   Setup                    1.0                      1\n",
      "90                   Wheel                    1.0                      2\n",
      "91                   Train                    1.0                      1\n",
      "92                    Feet                    1.0                      1\n",
      "93                  System                    1.0                      4\n",
      "94                 Program                    1.0                      5\n",
      "95                   Doubt                    1.0                      2\n",
      "96                  Remote                    1.0                      2\n",
      "97            Couple weeks                    1.0                      1\n",
      "98                Supplied                    1.0                      1\n",
      "99                  Dennis                    1.0                      1\n",
      "100             Cell phone                    1.0                      1\n",
      "101              Beginning                    1.0                      1\n",
      "102             Apple ipod                    1.0                      2\n",
      "103                 Laptop                    1.0                      1\n",
      "104                   Mins                    1.0                      9\n",
      "105               Feedback                    1.0                      3\n",
      "106             Hard drive                    1.0                      2\n",
      "107                 Reason                    1.0                      3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Nikkon Coolpix 4300 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using MultinominalNB\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 84.38%\n",
      "\n",
      "The number of forecasted (missing) reviews is 220 which equates to 57.89% of all reviews\n",
      "\n",
      "        Features  Positive Review Count  Negative Review Count\n",
      "0         Camera                   99.0                     14\n",
      "1        Picture                   31.0                      3\n",
      "2          Nikon                   15.0                      1\n",
      "3        Battery                   13.0                      3\n",
      "4           Card                   10.0                      1\n",
      "5           Time                    7.0                      1\n",
      "6           Pics                    7.0                      1\n",
      "7         Things                    7.0                      1\n",
      "8   Optical zoom                    6.0                      1\n",
      "9          Image                    6.0                      2\n",
      "10          Lens                    5.0                      4\n",
      "11       Problem                    5.0                      2\n",
      "12      Pictures                    5.0                      2\n",
      "13     Autofocus                    4.0                      2\n",
      "14    Situations                    4.0                      1\n",
      "15         Shots                    4.0                      1\n",
      "16         Light                    4.0                      2\n",
      "17          Auto                    3.0                      2\n",
      "18         Flash                    3.0                      4\n",
      "19     Drawbacks                    3.0                      1\n",
      "20          Feet                    1.0                      1\n",
      "21  System error                    1.0                      2\n",
      "22        Button                    1.0                      1\n",
      "23         Color                    1.0                      1\n",
      "24      Downside                    1.0                      1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Nokia 6610 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using MultinominalNB\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 84.91%\n",
      "\n",
      "The number of forecasted (missing) reviews is 322 which equates to 54.86% of all reviews\n",
      "\n",
      "           Features  Positive Review Count  Negative Review Count\n",
      "0             Phone                  170.0                     30\n",
      "1             Nokia                   24.0                      3\n",
      "2            Screen                    9.0                      6\n",
      "3              Game                    6.0                      1\n",
      "4              Menu                    5.0                      2\n",
      "5             Hours                    5.0                      5\n",
      "6            Amazon                    4.0                      1\n",
      "7              Call                    4.0                      2\n",
      "8              Keys                    4.0                      2\n",
      "9           Picture                    4.0                      1\n",
      "10             Plan                    4.0                      3\n",
      "11          Account                    4.0                      5\n",
      "12           Review                    4.0                      2\n",
      "13          Carrier                    4.0                      1\n",
      "14           Person                    3.0                      3\n",
      "15             Fact                    3.0                      1\n",
      "16        Bluetooth                    3.0                      1\n",
      "17           Design                    3.0                      1\n",
      "18             Ring                    3.0                      1\n",
      "19             Trib                    3.0                      1\n",
      "20         Warranty                    2.0                      1\n",
      "21  Corporate email                    2.0                      1\n",
      "22           Colors                    2.0                      1\n",
      "23     Tech support                    2.0                      1\n",
      "24           Europe                    2.0                      1\n",
      "25             Cons                    2.0                      4\n",
      "26         Internet                    2.0                      2\n",
      "27           Number                    2.0                      4\n",
      "28           Camera                    2.0                      4\n",
      "29           Option                    2.0                     11\n",
      "30          Contact                    2.0                      1\n",
      "31           Things                    2.0                      2\n",
      "32         Features                    2.0                      4\n",
      "33         Earpiece                    1.0                      1\n",
      "34         Hardware                    1.0                      2\n",
      "35      Application                    1.0                      2\n",
      "36         Downside                    1.0                      2\n",
      "37        Half hour                    1.0                      1\n",
      "38             Pros                    1.0                      5\n",
      "39          Minutes                    1.0                      2\n",
      "40           Laptop                    1.0                      2\n",
      "41            Email                    1.0                      1\n",
      "42     Manufacturer                    1.0                      1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Computer dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using Logistic Regression\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 86.96%\n",
      "\n",
      "The number of forecasted (missing) reviews is 285 which equates to 55.34% of all reviews\n",
      "\n",
      "       Features  Positive Review Count  Negative Review Count\n",
      "0       Monitor                   61.0                     19\n",
      "1         Price                   30.0                      2\n",
      "2      Computer                   23.0                      2\n",
      "3       Product                   17.0                      4\n",
      "4          Time                   12.0                      4\n",
      "5        Screen                   12.0                      2\n",
      "6       Problem                   11.0                      8\n",
      "7          Work                   10.0                      1\n",
      "8          Acer                    9.0                     10\n",
      "9       Display                    8.0                      2\n",
      "10       Review                    7.0                      2\n",
      "11         Desk                    6.0                      1\n",
      "12      Picture                    5.0                      3\n",
      "13        Mouse                    5.0                      1\n",
      "14        Money                    5.0                      1\n",
      "15      Quality                    4.0                      2\n",
      "16      Machine                    4.0                      2\n",
      "17     Keyboard                    4.0                      2\n",
      "18        Angle                    4.0                      2\n",
      "19         Unit                    4.0                      1\n",
      "20     Internet                    4.0                      1\n",
      "21       Memory                    3.0                      1\n",
      "22        Stars                    3.0                      2\n",
      "23     Speakers                    2.0                      2\n",
      "24     Programs                    2.0                      1\n",
      "25  Information                    2.0                      2\n",
      "26      Feature                    1.0                      1\n",
      "27    Viewsonic                    1.0                      1\n",
      "28         Area                    1.0                      1\n",
      "29         Fact                    1.0                      2\n",
      "30        Model                    1.0                      1\n",
      "31        Years                    1.0                      1\n",
      "32        Batch                    1.0                      1\n",
      "33     Comments                    1.0                      1\n",
      "34      Adapter                    1.0                      1\n",
      "35       System                    1.0                      3\n",
      "36      Package                    1.0                      1\n",
      "37       Tablet                    1.0                      1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Router dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using Random Forest Classifier\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 77.55%\n",
      "\n",
      "The number of forecasted (missing) reviews is 617 which equates to 71.83% of all reviews\n",
      "\n",
      "                Features  Positive Review Count  Negative Review Count\n",
      "0                   Hour                   20.0                      7\n",
      "1                Problem                   19.0                     16\n",
      "2               Wireless                   19.0                      3\n",
      "3                 Laptop                   18.0                      5\n",
      "4                Product                   16.0                     12\n",
      "5                   Port                   16.0                     35\n",
      "6               Firmware                   13.0                     13\n",
      "7                   Link                   12.0                     24\n",
      "8                  Setup                   11.0                      2\n",
      "9                  Range                   11.0                      2\n",
      "10                Future                   11.0                      3\n",
      "11                  Time                   10.0                      3\n",
      "12              Software                    9.0                     10\n",
      "13            Connection                    9.0                     19\n",
      "14              Password                    9.0                      1\n",
      "15                 Point                    9.0                      3\n",
      "16               Minutes                    8.0                      8\n",
      "17                  Site                    8.0                      5\n",
      "18                  Year                    8.0                      6\n",
      "19                 Model                    6.0                     14\n",
      "20                 Speed                    5.0                      8\n",
      "21               Windows                    5.0                      3\n",
      "22                  Case                    5.0                      3\n",
      "23                Number                    5.0                      4\n",
      "24          Access point                    5.0                      2\n",
      "25       Signal strength                    5.0                      2\n",
      "26                Router                    5.0                     86\n",
      "27              Internet                    4.0                      2\n",
      "28               Desktop                    4.0                      4\n",
      "29               Netgear                    4.0                     20\n",
      "30                 Power                    4.0                      1\n",
      "31                  Wifi                    4.0                      1\n",
      "32                 Order                    4.0                      2\n",
      "33                 Cisco                    4.0                      2\n",
      "34                  Days                    4.0                      2\n",
      "35       Latest firmware                    4.0                      4\n",
      "36                System                    3.0                      3\n",
      "37           Replacement                    3.0                      8\n",
      "38                 Video                    3.0                      2\n",
      "39                  Mbps                    3.0                     12\n",
      "40                Refund                    3.0                      1\n",
      "41                 Money                    2.0                      1\n",
      "42            Phone call                    2.0                      2\n",
      "43               Process                    2.0                      2\n",
      "44           Frustration                    2.0                      1\n",
      "45                  Bars                    2.0                      1\n",
      "46                 Issue                    2.0                      4\n",
      "47                Manual                    2.0                      5\n",
      "48                 Ddwrt                    2.0                      1\n",
      "49                  Luck                    2.0                      2\n",
      "50              Document                    2.0                      1\n",
      "51              Comments                    2.0                      1\n",
      "52                  Roku                    2.0                      1\n",
      "53               Seconds                    2.0                      5\n",
      "54                  Unit                    2.0                      6\n",
      "55                Person                    2.0                      3\n",
      "56               Default                    2.0                      2\n",
      "57               Upgrade                    2.0                      2\n",
      "58               Address                    2.0                      7\n",
      "59                Device                    2.0                     15\n",
      "60       Firmware update                    2.0                      2\n",
      "61            Difference                    2.0                      1\n",
      "62                  Xbox                    1.0                      1\n",
      "63              Repeater                    1.0                      1\n",
      "64            House hold                    1.0                      1\n",
      "65          Setup wizard                    1.0                      1\n",
      "66              Visitors                    1.0                      1\n",
      "67          Expectations                    1.0                      1\n",
      "68                 Store                    1.0                      2\n",
      "69                 Vista                    1.0                      3\n",
      "70     Internet provider                    1.0                      2\n",
      "71         Power adapter                    1.0                      3\n",
      "72           Experiences                    1.0                      1\n",
      "73                  Hold                    1.0                      1\n",
      "74               Youtube                    1.0                      1\n",
      "75                  Home                    1.0                      2\n",
      "76                 Bonus                    1.0                      2\n",
      "77         Guest network                    1.0                      7\n",
      "78            Difficulty                    1.0                      1\n",
      "79                  Auto                    1.0                      1\n",
      "80                  Type                    1.0                      3\n",
      "81          Reset button                    1.0                      1\n",
      "82  Buffalo technologies                    1.0                      1\n",
      "83              Problems                    1.0                      1\n",
      "84              Settings                    1.0                      2\n",
      "85          Installation                    1.0                      5\n",
      "86               Company                    1.0                      2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Speaker dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using MultinominalNB\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 71.67%\n",
      "\n",
      "The number of forecasted (missing) reviews is 383 which equates to 56.41% of all reviews\n",
      "\n",
      "         Features  Positive Review Count  Negative Review Count\n",
      "0         Speaker                  113.0                     13\n",
      "1           Sound                   46.0                      9\n",
      "2           Music                   21.0                      1\n",
      "3            Bose                   19.0                      4\n",
      "4        Speakers                   19.0                      1\n",
      "5            Ipad                   13.0                      3\n",
      "6          System                   12.0                      2\n",
      "7            Back                   12.0                      1\n",
      "8          Laptop                   11.0                      2\n",
      "9           Audio                    9.0                      2\n",
      "10        Quality                    7.0                      2\n",
      "11          Range                    7.0                      2\n",
      "12          Tower                    6.0                      1\n",
      "13         Woofer                    5.0                      4\n",
      "14          Issue                    5.0                      1\n",
      "15        Company                    5.0                      3\n",
      "16           Klip                    5.0                      1\n",
      "17        Netbook                    5.0                      1\n",
      "18      Bluetooth                    5.0                      1\n",
      "19     Satellites                    4.0                      2\n",
      "20         Design                    4.0                      1\n",
      "21         Office                    4.0                      1\n",
      "22           Type                    4.0                      1\n",
      "23         Iphone                    4.0                      1\n",
      "24         Volume                    4.0                      3\n",
      "25  Sound quality                    3.0                      4\n",
      "26        Buttons                    2.0                      2\n",
      "27        Reviews                    2.0                      2\n",
      "28   Expectations                    1.0                      1\n",
      "29     Conclusion                    1.0                      1\n",
      "30         Stands                    1.0                      2\n",
      "31       Computer                    1.0                      1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Canon Powershot SD500 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using Random Forest Classifier\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 84.62%\n",
      "\n",
      "The number of forecasted (missing) reviews is 103 which equates to 44.98% of all reviews\n",
      "\n",
      "             Features  Positive Review Count  Negative Review Count\n",
      "0              Camera                   74.0                      9\n",
      "1                Card                    6.0                      1\n",
      "2               Video                    5.0                      1\n",
      "3               Flash                    5.0                      2\n",
      "4                Cnet                    4.0                      1\n",
      "5               Sites                    4.0                      2\n",
      "6              Screen                    3.0                      2\n",
      "7               Sound                    3.0                      1\n",
      "8              Review                    3.0                      3\n",
      "9            Shooting                    1.0                      1\n",
      "10  Advanced controls                    1.0                      2\n",
      "11              Photo                    1.0                      1\n",
      "12          Long time                    1.0                      1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Canon S100 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using Logistic Regression\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 82.86%\n",
      "\n",
      "The number of forecasted (missing) reviews is 124 which equates to 41.61% of all reviews\n",
      "\n",
      "        Features  Positive Review Count  Negative Review Count\n",
      "0           Size                   11.0                      1\n",
      "1        Cameras                    7.0                      1\n",
      "2     Resolution                    6.0                      1\n",
      "3           Lens                    5.0                      1\n",
      "4           Pics                    5.0                      3\n",
      "5         Manual                    4.0                      2\n",
      "6           Elph                    4.0                      1\n",
      "7       Computer                    3.0                      1\n",
      "8          Money                    3.0                      1\n",
      "9       Software                    3.0                      3\n",
      "10  Optical zoom                    3.0                      1\n",
      "11          Life                    3.0                      3\n",
      "12          Time                    3.0                      4\n",
      "13        Reason                    2.0                      1\n",
      "14          Auto                    2.0                      2\n",
      "15          Zoom                    2.0                      4\n",
      "16          Card                    2.0                      4\n",
      "17        Wallet                    1.0                      1\n",
      "18         Kodak                    1.0                      1\n",
      "19         Stars                    1.0                      2\n",
      "20       Windows                    1.0                      2\n",
      "21        Screen                    1.0                      4\n",
      "22        Things                    1.0                      1\n",
      "23    Viewfinder                    1.0                      1\n",
      "24          Film                    1.0                      1\n",
      "25         Place                    1.0                      1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Diaper Champ dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using MultinominalNB\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 88.37%\n",
      "\n",
      "The number of forecasted (missing) reviews is 163 which equates to 43.47% of all reviews\n",
      "\n",
      "        Features  Positive Review Count  Negative Review Count\n",
      "0         Diaper                   50.0                     25\n",
      "1   Diaper champ                   47.0                     10\n",
      "2          Champ                   32.0                      4\n",
      "3           Odor                   23.0                      7\n",
      "4   Diaper genie                   19.0                      8\n",
      "5           Pain                   15.0                      2\n",
      "6   Dirty diaper                   14.0                      4\n",
      "7           Bags                   13.0                      2\n",
      "8        Refills                   12.0                      3\n",
      "9        Problem                   11.0                      4\n",
      "10         Smell                    7.0                      6\n",
      "11         Trash                    6.0                      1\n",
      "12          Time                    5.0                      2\n",
      "13         Genie                    5.0                      1\n",
      "14        Months                    5.0                      3\n",
      "15         Child                    5.0                      1\n",
      "16   Diaper pail                    5.0                      1\n",
      "17       Diapers                    4.0                      3\n",
      "18          Item                    3.0                      3\n",
      "19       Plastic                    3.0                      1\n",
      "20       Nursery                    3.0                      2\n",
      "21          Room                    3.0                      2\n",
      "22         Money                    3.0                      1\n",
      "23      Daughter                    3.0                      1\n",
      "24         Lysol                    2.0                      2\n",
      "25         Floor                    2.0                      1\n",
      "26          Mind                    2.0                      1\n",
      "27       Reviews                    2.0                      5\n",
      "28       Product                    2.0                      1\n",
      "29         Weeks                    1.0                      1\n",
      "30       Opening                    1.0                      1\n",
      "31          Unit                    1.0                      1\n",
      "32          Poop                    1.0                      1\n",
      "33       Parents                    1.0                      2\n",
      "34        Solids                    1.0                      3\n",
      "35          Plug                    1.0                      4\n",
      "36        Babies                    1.0                      2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Hitachi Router dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using MultinominalNB\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 72.5%\n",
      "\n",
      "The number of forecasted (missing) reviews is 114 which equates to 36.54% of all reviews\n",
      "\n",
      "         Features  Positive Review Count  Negative Review Count\n",
      "0          Router                   53.0                     22\n",
      "1         Hitachi                   20.0                      2\n",
      "2    Porter cable                   16.0                      4\n",
      "3           Price                   14.0                      2\n",
      "4           Table                   14.0                      8\n",
      "5            Tool                   10.0                      1\n",
      "6   Speed control                   10.0                      4\n",
      "7            Hand                    8.0                      7\n",
      "8         Machine                    8.0                      3\n",
      "9            Wood                    7.0                      2\n",
      "10   Great router                    7.0                      2\n",
      "11     Soft start                    5.0                      1\n",
      "12         Collet                    4.0                      5\n",
      "13        Problem                    4.0                      4\n",
      "14        Reviews                    4.0                      1\n",
      "15          Speed                    3.0                      3\n",
      "16           Shop                    3.0                      2\n",
      "17           Bits                    2.0                     10\n",
      "18         Makita                    2.0                      1\n",
      "19          Motor                    2.0                      1\n",
      "20           Type                    2.0                      1\n",
      "21       Freehand                    2.0                      2\n",
      "22        Control                    2.0                      3\n",
      "23    Performance                    2.0                      1\n",
      "24         Design                    2.0                      1\n",
      "25          Hands                    2.0                      1\n",
      "26          Stars                    1.0                      1\n",
      "27         Weight                    1.0                      2\n",
      "28          Setup                    1.0                      1\n",
      "29    Combination                    1.0                      1\n",
      "30          Worth                    1.0                      2\n",
      "31         Muscle                    1.0                      1\n",
      "32          Place                    1.0                      2\n",
      "33        Package                    1.0                      1\n",
      "34         Couple                    1.0                      1\n",
      "35         Points                    1.0                      1\n",
      "36    Accessories                    1.0                      1\n",
      "37      Inch bits                    1.0                      4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Ipod dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using MultinominalNB\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 81.82%\n",
      "\n",
      "The number of forecasted (missing) reviews is 368 which equates to 69.57% of all reviews\n",
      "\n",
      "           Features  Positive Review Count  Negative Review Count\n",
      "0              Ipod                  115.0                     37\n",
      "1             Music                   25.0                      2\n",
      "2             Apple                   18.0                     11\n",
      "3              Time                   14.0                      2\n",
      "4           Quality                   11.0                      3\n",
      "5           Battery                    9.0                      7\n",
      "6              Cons                    8.0                      4\n",
      "7              Case                    7.0                      7\n",
      "8            Charge                    6.0                      3\n",
      "9            Market                    6.0                      3\n",
      "10             Fact                    6.0                      1\n",
      "11         Software                    5.0                      1\n",
      "12          Problem                    5.0                      4\n",
      "13         Creative                    5.0                      4\n",
      "14       Generation                    5.0                      2\n",
      "15           Design                    4.0                      2\n",
      "16             Pros                    4.0                      2\n",
      "17          Company                    3.0                      5\n",
      "18             Hand                    3.0                      1\n",
      "19           Owners                    3.0                      2\n",
      "20            Month                    3.0                      7\n",
      "21             Game                    3.0                      5\n",
      "22           Screen                    2.0                      2\n",
      "23          Windows                    2.0                      3\n",
      "24      Apple apple                    2.0                      1\n",
      "25      Music lover                    2.0                      2\n",
      "26            Issue                    2.0                      2\n",
      "27     Circuit city                    2.0                      1\n",
      "28           Middle                    2.0                      1\n",
      "29  Unlimited music                    2.0                      1\n",
      "30           Volume                    2.0                      2\n",
      "31             Dock                    1.0                      1\n",
      "32           Pocket                    1.0                      1\n",
      "33        Equalizer                    1.0                      1\n",
      "34            Doubt                    1.0                      1\n",
      "35        Christmas                    1.0                      2\n",
      "36       Hard drive                    1.0                      2\n",
      "37       Audiophile                    1.0                      1\n",
      "38             List                    1.0                      1\n",
      "39           Minute                    1.0                      3\n",
      "40           Hassle                    1.0                      3\n",
      "41           Videos                    1.0                      1\n",
      "42             Luck                    1.0                      1\n",
      "43            Weeks                    1.0                      1\n",
      "44        Backlight                    1.0                      1\n",
      "45           Google                    1.0                      1\n",
      "46            Files                    1.0                      2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Linksys Router dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using MultinominalNB\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 73.68%\n",
      "\n",
      "The number of forecasted (missing) reviews is 377 which equates to 66.49% of all reviews\n",
      "\n",
      "         Features  Positive Review Count  Negative Review Count\n",
      "0            Link                   46.0                      4\n",
      "1        Security                   16.0                      2\n",
      "2      Connection                   15.0                      2\n",
      "3           Setup                   12.0                      2\n",
      "4        Firmware                   11.0                      1\n",
      "5            Time                   10.0                      3\n",
      "6          Laptop                    9.0                      1\n",
      "7         Version                    9.0                      1\n",
      "8   Configuration                    8.0                      5\n",
      "9          Manual                    8.0                      4\n",
      "10     Encryption                    8.0                      2\n",
      "11         Couple                    6.0                      2\n",
      "12           Case                    6.0                      2\n",
      "13           Star                    5.0                      2\n",
      "14         System                    5.0                      2\n",
      "15        Program                    5.0                      1\n",
      "16          Linux                    5.0                      1\n",
      "17          Order                    4.0                      1\n",
      "18   Access point                    4.0                      1\n",
      "19         Design                    3.0                      2\n",
      "20          Cisco                    3.0                      1\n",
      "21       Software                    3.0                      4\n",
      "22        Install                    3.0                      5\n",
      "23           Part                    2.0                      1\n",
      "24        Airport                    2.0                      1\n",
      "25           Idea                    2.0                      1\n",
      "26        Windows                    2.0                      1\n",
      "27    Large files                    2.0                      2\n",
      "28          Walls                    2.0                      1\n",
      "29   Tech support                    2.0                      4\n",
      "30         Speeds                    2.0                      1\n",
      "31          Phone                    1.0                      1\n",
      "32         Wizard                    1.0                      6\n",
      "33         Amazon                    1.0                      1\n",
      "34          Music                    1.0                      1\n",
      "35  Setup program                    1.0                      1\n",
      "36         Memory                    1.0                      1\n",
      "37         Access                    1.0                      2\n",
      "38       Research                    1.0                      1\n",
      "39           Tivo                    1.0                      1\n",
      "40  Wired network                    1.0                      4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Micro MP3 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using MultinominalNB\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 78.49%\n",
      "\n",
      "The number of forecasted (missing) reviews is 547 which equates to 54.16% of all reviews\n",
      "\n",
      "          Features  Positive Review Count  Negative Review Count\n",
      "0            Micro                   79.0                     17\n",
      "1             Ipod                   48.0                      8\n",
      "2          Battery                   33.0                     12\n",
      "3         Creative                   30.0                      4\n",
      "4           Nature                   26.0                      3\n",
      "5            Piece                   19.0                      3\n",
      "6            Music                   18.0                      7\n",
      "7            Touch                   17.0                     12\n",
      "8            Apple                   16.0                      1\n",
      "9          Product                   15.0                      4\n",
      "10           Sound                   14.0                      1\n",
      "11          Review                   14.0                      9\n",
      "12            Case                   14.0                      6\n",
      "13           Month                   12.0                      3\n",
      "14            List                   11.0                      1\n",
      "15        Computer                   10.0                      7\n",
      "16        Software                   10.0                     17\n",
      "17          Design                   10.0                     10\n",
      "18   Compatibility                    9.0                      1\n",
      "19        Firmware                    9.0                     12\n",
      "20         Company                    9.0                      5\n",
      "21            Time                    8.0                      7\n",
      "22        Transfer                    8.0                      2\n",
      "23          Reason                    8.0                      3\n",
      "24           Store                    8.0                      1\n",
      "25          Amazon                    7.0                      1\n",
      "26         Napster                    7.0                     10\n",
      "27         Feature                    7.0                      1\n",
      "28       Batteries                    7.0                      1\n",
      "29           Point                    6.0                      2\n",
      "30            Year                    6.0                      1\n",
      "31            Deal                    6.0                      1\n",
      "32           Cable                    5.0                      1\n",
      "33         Buttons                    5.0                      1\n",
      "34          Format                    5.0                      3\n",
      "35    Installation                    5.0                      2\n",
      "36           Stars                    5.0                      2\n",
      "37      Collection                    5.0                      1\n",
      "38           Great                    5.0                      1\n",
      "39           Value                    5.0                      3\n",
      "40            Cons                    5.0                      4\n",
      "41            Mind                    5.0                      4\n",
      "42      Audiophile                    4.0                      1\n",
      "43      Microphone                    4.0                      1\n",
      "44       Belt clip                    4.0                      4\n",
      "45            Week                    4.0                      4\n",
      "46           Karma                    4.0                      2\n",
      "47            Pair                    4.0                      3\n",
      "48            User                    3.0                      3\n",
      "49           Order                    3.0                      3\n",
      "50         Machine                    2.0                      2\n",
      "51   Touch buttons                    2.0                      1\n",
      "52          Finger                    2.0                      1\n",
      "53           Speed                    2.0                      3\n",
      "54            Unit                    2.0                      3\n",
      "55           Style                    2.0                      1\n",
      "56            Clip                    2.0                      1\n",
      "57          Outlet                    2.0                      2\n",
      "58         Bargain                    2.0                      1\n",
      "59            Area                    2.0                      2\n",
      "60   Great product                    2.0                      2\n",
      "61    Scroll wheel                    2.0                      3\n",
      "62       Interface                    2.0                      6\n",
      "63       Hard case                    1.0                      1\n",
      "64            News                    1.0                      1\n",
      "65      Department                    1.0                      1\n",
      "66            Gigs                    1.0                      1\n",
      "67           Usage                    1.0                      1\n",
      "68     Sensitivity                    1.0                      7\n",
      "69            Info                    1.0                      2\n",
      "70    Organization                    1.0                      2\n",
      "71     Yahoo music                    1.0                      2\n",
      "72     Wall socket                    1.0                      1\n",
      "73           Front                    1.0                      2\n",
      "74  Message boards                    1.0                      1\n",
      "75            Days                    1.0                      1\n",
      "76            Zens                    1.0                      2\n",
      "77         Library                    1.0                      1\n",
      "78            Work                    1.0                      2\n",
      "79         Outlook                    1.0                      3\n",
      "80            Joke                    1.0                      1\n",
      "81  Favorite color                    1.0                      1\n",
      "82           Track                    1.0                      4\n",
      "83   Family member                    1.0                      1\n",
      "84          Pocket                    1.0                      3\n",
      "85     Accessories                    1.0                      2\n",
      "86        Touchpad                    1.0                      1\n",
      "87          Course                    1.0                      4\n",
      "88          Colors                    1.0                      1\n",
      "89          Things                    1.0                      5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Nokia 6600 dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using Support Vector Machine Classifier\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 72.6%\n",
      "\n",
      "The number of forecasted (missing) reviews is 189 which equates to 34.36% of all reviews\n",
      "\n",
      "            Features  Positive Review Count  Negative Review Count\n",
      "0              Phone                  160.0                     42\n",
      "1          Bluetooth                   27.0                      8\n",
      "2            Feature                   21.0                      6\n",
      "3              Nokia                   19.0                     23\n",
      "4             Camera                   12.0                      5\n",
      "5           Computer                   10.0                      4\n",
      "6            Speaker                   10.0                      1\n",
      "7            Battery                    9.0                      1\n",
      "8             Screen                    8.0                      7\n",
      "9            Picture                    7.0                      2\n",
      "10     Sony ericsson                    7.0                      1\n",
      "11              Idea                    7.0                      2\n",
      "12              Hour                    6.0                      4\n",
      "13           Contact                    6.0                      3\n",
      "14              Hand                    6.0                      1\n",
      "15         Interface                    5.0                      2\n",
      "16            Images                    5.0                      1\n",
      "17            Number                    4.0                      5\n",
      "18              Data                    4.0                      1\n",
      "19         Reception                    4.0                      3\n",
      "20             Video                    3.0                      1\n",
      "21      Remote areas                    2.0                      1\n",
      "22             Terms                    2.0                      1\n",
      "23           Options                    2.0                      1\n",
      "24       Application                    2.0                      4\n",
      "25              Palm                    2.0                      2\n",
      "26        Technician                    2.0                      2\n",
      "27            Repair                    2.0                      3\n",
      "28          Function                    2.0                      1\n",
      "29              Secs                    2.0                      6\n",
      "30            Movies                    1.0                      2\n",
      "31             Globe                    1.0                      1\n",
      "32  Service provider                    1.0                      1\n",
      "33           Samsung                    1.0                      2\n",
      "34             Issue                    1.0                      2\n",
      "35            Pocket                    1.0                      1\n",
      "36           Carrier                    1.0                      1\n",
      "37           Browser                    1.0                      1\n",
      "38              Case                    1.0                      1\n",
      "39              Sony                    1.0                      3\n",
      "40       Combination                    1.0                      1\n",
      "41        Touch tone                    1.0                      1\n",
      "42          Shipment                    1.0                      1\n",
      "43             Point                    1.0                      1\n",
      "44            Device                    1.0                      3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================\n",
      "                                  Top Features extracted from Norton dataset\n",
      "============================================================================================================\n",
      "\n",
      "Semantic orientation of missing reviews is forecasted using MultinominalNB\n",
      " since this is the best performing model on this dataset with an out of sample accuracy of 78.57%\n",
      "\n",
      "The number of forecasted (missing) reviews is 171 which equates to 45.0% of all reviews\n",
      "\n",
      "              Features  Positive Review Count  Negative Review Count\n",
      "0            Antivirus                   11.0                      7\n",
      "1              Product                    7.0                     37\n",
      "2              Version                    6.0                     26\n",
      "3              Trouble                    6.0                     27\n",
      "4             Computer                    5.0                      4\n",
      "5             Firewall                    5.0                      6\n",
      "6               System                    4.0                      9\n",
      "7    Corporate edition                    4.0                      1\n",
      "8             Antispam                    3.0                      2\n",
      "9             Software                    3.0                     11\n",
      "10                Time                    3.0                      9\n",
      "11            Security                    3.0                     13\n",
      "12             Reviews                    2.0                      2\n",
      "13            Features                    2.0                      1\n",
      "14          Zone alarm                    2.0                      2\n",
      "15             Program                    2.0                     26\n",
      "16               Email                    2.0                      2\n",
      "17               Speed                    1.0                      1\n",
      "18  Antivirus software                    1.0                      1\n",
      "19            Download                    1.0                      1\n",
      "20              Norton                    1.0                     26\n",
      "21                Fact                    1.0                      2\n",
      "22               Proof                    1.0                      4\n",
      "23        Subscription                    1.0                      4\n",
      "24              Memory                    1.0                      3\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = review_similarity_count_two(\n",
    "    *evaluate_two(models_training_two(split_data_apply_tfidf_two(\n",
    "         feature_extraction_two(\n",
    "         data_preprocessing(\n",
    "         load_data(your_path_to_data)))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8cb93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24805eff",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb66a3",
   "metadata": {},
   "source": [
    "\n",
    "* [1] https://en.wikipedia.org/wiki/Viterbi_algorithm\n",
    "* [2] https://stackoverflow.com/questions/195010/how-can-i-split-multiple-joined-words\n",
    "* [3] https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "* [4] https://www.nltk.org/book/ch07.html\n",
    "* [5] https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "* [6] https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "* [7] https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=support+vector+machine\n",
    "* [8] https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "* https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/\n",
    "* https://www.cs.uic.edu/~liub/publications/aaai04-featureExtract.pdf\n",
    "* https://www.cs.uic.edu/~liub/publications/kdd04-revSummary.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423b994",
   "metadata": {},
   "source": [
    "Thanks for your attention, we hope you have enjoyed reading through this aasignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
